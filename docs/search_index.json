[
["Chapter-Modeling.html", "Chapter 3 Predicting tuber yield category 3.1 Objective 3.2 Useful libraries 3.3 Machine learning data set 3.4 Processing Machine learning 3.5 Yield Class Prediction with rf algorithm on test set 3.6 Comparison with non-informative classification 3.7 Train data set backup for true negatives specimens extraction", " Chapter 3 Predicting tuber yield category 3.1 Objective The objective of this chapter is to develop, evaluate and compare the performance of some machine learning algorithms (k-nearest neighbors, random forest and support vector machine - package caret) for the prediction of a potato yield category using both its leaf ionome and its ionomics group. I use the previous chapter (chapter 2) handeled data file dfml.csv which contains clr coordinates, maturity classes, ionomics groups and the yield two categorical variable created using the 65^th percentile for each cultivar. I use accuracy as models quality metric. I run the Chi-square homogenity test to compare prediction with a non-informative classification consisting of an equal distribution of 50% successful and 50% unsuccessful cases, using the best model. Then, I filter only true negative specimens i.e., correctely predicted high yielders of training data set in a new data frame for leaf health index purpose (Chapter 4). 3.2 Useful libraries The tidyverse package is always needed for data handling and visualization, and then extrafont to make changes in graphs as demanded for the article. The particularly useful packages are caret and kknn needed for machine leraning functions. library(&quot;tidyverse&quot;) library(&#39;extrafont&#39;) library(&#39;caret&#39;) library(&#39;kknn&#39;) 3.3 Machine learning data set I load the dfml.csv data set and named it df. The clr coordinates are scaled to zero mean and unity variance. df = read_csv(&#39;output/dfml.csv&#39;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Mg = col_double(), ## clr_Ca = col_double(), ## clr_Fv = col_double(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## RendVendable = col_double(), ## yieldCutoff = col_double(), ## yieldClass = col_character(), ## ionomicGroup = col_double() ## ) colnames(df)[colnames(df)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; # makes it simplier ! df$group_i = factor(df$group_i) df$Maturity5 = factor(df$Maturity5) df$yieldClass = factor(df$yieldClass) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrMg&quot;, &quot;clrCa&quot;, &quot;clrFv&quot;) colnames(df)[which(names(df) %in% clr_no)] &lt;- clrNo df.sc = df # copy df.sc[, clrNo] &lt;- apply(df.sc[, clrNo], 2, scale) # scale clr coordinates I check the data frame to see cultivars abundance. Cultivar Goldrush overcomes the others. pc &lt;- round(with(df.sc, prop.table(table(Cultivar)) * 100), 2) dist &lt;- with(df.sc, cbind(freq = table(Cultivar), percentage = pc)) dist &lt;- data.frame(cbind(dist, rownames(dist))) colnames(dist)[3] &lt;- &quot;Cultivar&quot; dist$freq &lt;- as.numeric(as.character(dist$freq)) dist %&gt;% arrange(desc(freq)) %&gt;% head(10) # or discard the last pipe to see all. ## freq percentage Cultivar ## 1 560 16.56 Goldrush ## 2 367 10.85 Superior ## 3 346 10.23 FL 1207 ## 4 258 7.63 Chieftain ## 5 189 5.59 Kennebec ## 6 188 5.56 Snowden ## 7 184 5.44 Atlantic ## 8 183 5.41 FL 1533 ## 9 165 4.88 Coastal Russet ## 10 112 3.31 Shepody 3.4 Processing Machine learning 3.4.1 Train and Test splits I randomly split the data into a training set (75 % of the data) used to fit the models, and a testing set (remaining 25 %) I use for models’ evaluation. The next chunk splits data and groups predictors using three schemes. set.seed(853739) split_index &lt;- createDataPartition(df.sc$yieldClass, group = &quot;Cultivar&quot;, p = 0.75, list = FALSE, times = 1) train &lt;- df.sc[split_index, ] test &lt;- df.sc[-split_index, ] ## With only clr coordinates as predictors ml_clr &lt;- c(clrNo, &#39;yieldClass&#39;) train_clr = train[, ml_clr] test_clr = test[, ml_clr] ## With clr and maturity classes as predictors ml_mc &lt;- c(clrNo, &#39;Maturity5&#39;, &#39;yieldClass&#39;) train_mc = train[, ml_mc] test_mc = test[, ml_mc] ## With clr and ionomic groups as predictors ml_grp &lt;- c(clrNo, &#39;group_i&#39;, &#39;yieldClass&#39;) train_grp = train[, ml_grp] test_grp = test[, ml_grp] With the kknn package, we must specify three parameters: kmax which is the number of neighbors to consider, distance a distance parameter to specify (1 for the Mahattan distance and 2 for the Euclidean distance), and a kernel which is a function to measure the distance. A best method currently used to choose the right parameters consists in creating a parameter grid. grid &lt;- expand.grid(kmax = c(7,9,12,15), distance = 1:2, kernel = c(&quot;rectangular&quot;, &quot;gaussian&quot;, &quot;optimal&quot;)) grid ## kmax distance kernel ## 1 7 1 rectangular ## 2 9 1 rectangular ## 3 12 1 rectangular ## 4 15 1 rectangular ## 5 7 2 rectangular ## 6 9 2 rectangular ## 7 12 2 rectangular ## 8 15 2 rectangular ## 9 7 1 gaussian ## 10 9 1 gaussian ## 11 12 1 gaussian ## 12 15 1 gaussian ## 13 7 2 gaussian ## 14 9 2 gaussian ## 15 12 2 gaussian ## 16 15 2 gaussian ## 17 7 1 optimal ## 18 9 1 optimal ## 19 12 1 optimal ## 20 15 1 optimal ## 21 7 2 optimal ## 22 9 2 optimal ## 23 12 2 optimal ## 24 15 2 optimal I use the metric of “Accuracy” to evaluate models. This is a ratio of the number of correctly predicted instances divided by the total number of instances in the dataset (e.g. 95% accurate). The accuracy of the models will be estimated using a 10-fold cross-validation (cv) scheme. This will split the data set into 10 subsets of equal size. The models are built 10 times, each time leaving out one of the subsets from training and use it as the test set. control &lt;- trainControl(method = &quot;cv&quot;, number = 10) metric &lt;- &quot;Accuracy&quot; 3.4.2 Building the Models I reset the random number seed before reach run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable as explained by Jason Brownlee. # a) Non-linear algorithm ## kNN set.seed(7) kknn_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) # b) Advanced algorithms ## SVM set.seed(7) svm_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;svmRadial&quot;, metric = metric, trControl = control) ## Random Forest set.seed(7) rf_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;rf&quot;, metric = metric, trControl = control) rf_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;rf&quot;, metric = metric, trControl = control) rf_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;rf&quot;, metric = metric, trControl = control) 3.4.3 Goodness of fit on training set I assess the accuracy metric also when building the models (training set) but the target metric is for the evaluation set. This chart sorts models. # Summary results results &lt;- resamples(list(kknn_clr_solely = kknn_clr, kknn_clr_and_ionomicgroup = kknn_grp, kknn_clr_and_maturityclass = kknn_mc, svm_clr_solely = svm_clr, svm_clr_and_ionomicgroup = svm_grp, svm_clr_and_maturityclass = svm_mc, rf_clr_solely = rf_clr, rf_clr_and_ionomicgroup = rf_grp, rf_clr_and_maturityclass = rf_mc)) #summary(results) with dotplot() options(repr.plot.width = 5, repr.plot.height = 4) dotplot(results) Figure 3.1: Comparison of models accuracies at training. This chunk also sorts models in a descending order using accuracies only, in a table. models_acc &lt;- data.frame(Model = summary(results)$models, Accuracy = c(confusionMatrix(train_clr$yieldClass, predict(kknn_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(kknn_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(kknn_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(svm_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(svm_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(svm_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(rf_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(rf_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(rf_mc))$overall[1])) models_acc[order(models_acc[,&quot;Accuracy&quot;], decreasing = TRUE), ] ## Model Accuracy ## 7 rf_clr_solely 0.9795034 ## 8 rf_clr_and_ionomicgroup 0.9795034 ## 9 rf_clr_and_maturityclass 0.9444225 ## 2 kknn_clr_and_ionomicgroup 0.8600709 ## 1 kknn_clr_solely 0.8588885 ## 3 kknn_clr_and_maturityclass 0.8454868 ## 6 svm_clr_and_maturityclass 0.6929444 ## 5 svm_clr_and_ionomicgroup 0.6901853 ## 4 svm_clr_solely 0.6618053 3.4.4 Models evaluation (Test set) Model Evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. Th e next chunk performs this computations and gives the sorted accuracy metrics. predicted_kknn_clr &lt;- predict(kknn_clr, test_clr) predicted_kknn_mc &lt;- predict(kknn_mc, test_mc) predicted_kknn_grp &lt;- predict(kknn_grp, test_grp) predicted_svm_clr &lt;- predict(svm_clr, test_clr) predicted_svm_mc &lt;- predict(svm_mc, test_mc) predicted_svm_grp &lt;- predict(svm_grp, test_grp) predicted_rf_clr &lt;- predict(rf_clr, test_clr) predicted_rf_mc &lt;- predict(rf_mc, test_mc) predicted_rf_grp &lt;- predict(rf_grp, test_grp) #The best model tests_acc &lt;- data.frame(Model = summary(results)$models, Accuracy_on_test = c( confusionMatrix(test_clr$yieldClass, predicted_kknn_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_kknn_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_kknn_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_svm_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_svm_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_svm_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_rf_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_rf_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_rf_mc)$overall[1])) tests_acc[order(tests_acc[,&quot;Accuracy_on_test&quot;], decreasing = TRUE), ] ## Model Accuracy_on_test ## 8 rf_clr_and_ionomicgroup 0.7242604 ## 9 rf_clr_and_maturityclass 0.7242604 ## 7 rf_clr_solely 0.7088757 ## 3 kknn_clr_and_maturityclass 0.7065089 ## 2 kknn_clr_and_ionomicgroup 0.6863905 ## 6 svm_clr_and_maturityclass 0.6828402 ## 1 kknn_clr_solely 0.6769231 ## 5 svm_clr_and_ionomicgroup 0.6674556 ## 4 svm_clr_solely 0.6579882 The k-nearest neighbours, the random forest and the support vector machine models returned similar predictive accuracies although slightly higher for the random forest (73%) and the k-nearest neighbours (71%) algorithms. Using maturity classes or ionomics groups also returned similar predictive accuracies for all the models on test set. 3.5 Yield Class Prediction with rf algorithm on test set I sort the predictive quality metrics by cultivar with random forest algorithm combining clr coordinates and new clusters variable: model rf_clr_and_ionomicgroup. test$ypred = predicted_rf_grp # adds predictions to test set cultivar_acc &lt;- test %&gt;% group_by(Cultivar) %&gt;% do(Accuracy = as.numeric(confusionMatrix(.$yieldClass, .$ypred)$overall[1]), numb_samples = as.numeric(nrow(.))) cultivar_acc$Accuracy &lt;- unlist(cultivar_acc$Accuracy) cultivar_acc$numb_samples &lt;- unlist(cultivar_acc$numb_samples) data = data.frame(subset(cultivar_acc, Accuracy&gt;0)) data[order(data[,&quot;Accuracy&quot;], decreasing=T), ] ## Cultivar Accuracy numb_samples ## 2 AC Chaleur 1.0000000 4 ## 12 Dark Red Chieftain 1.0000000 2 ## 18 Harmony 1.0000000 1 ## 19 Kanona 1.0000000 1 ## 21 Keuka Gold 1.0000000 1 ## 22 Krantz 1.0000000 1 ## 23 Lamoka 1.0000000 2 ## 26 Norland 1.0000000 12 ## 27 Peribonka 1.0000000 2 ## 30 Red Cloud 1.0000000 4 ## 41 W 1386 1.0000000 4 ## 42 Waneta 1.0000000 3 ## 10 Chieftain 0.8695652 69 ## 20 Kennebec 0.8181818 44 ## 25 Mystere 0.8125000 16 ## 17 Goldrush 0.8028169 142 ## 6 Argos 0.8000000 5 ## 14 FL 1207 0.7978723 94 ## 4 Andover 0.7500000 8 ## 16 Frontier Russet 0.7500000 4 ## 32 Roko 0.7500000 4 ## 33 Russet Burbank 0.7500000 8 ## 35 Shepody 0.7500000 28 ## 28 Pommerelle 0.7142857 7 ## 11 Coastal Russet 0.7058824 34 ## 9 Carolina 0.6666667 3 ## 31 Red Maria 0.6666667 3 ## 38 Superior 0.6494845 97 ## 15 FL 1533 0.6428571 56 ## 37 Snowden 0.6410256 39 ## 34 Russet Norkota 0.6000000 5 ## 40 Vivaldi 0.6000000 5 ## 5 Aquilon 0.5789474 19 ## 7 Atlantic 0.5576923 52 ## 43 Yukon Gold 0.5555556 18 ## 1 AC Belmont 0.5000000 16 ## 3 Ambra 0.5000000 2 ## 8 Bijou Rouge 0.5000000 2 ## 13 Estima 0.5000000 12 ## 29 Reba 0.5000000 4 ## 36 Sifra 0.5000000 2 ## 24 Lanorma 0.3333333 3 ## 39 Viking 0.3333333 3 The predictive accuracy is very high for some cultivars, but this result must be taken with care due to small size of samples often available. The next chunk plots accuracies for cultivars using ggplot2 functions and categorising low, high and very high predictive accuracies. options(repr.plot.width = 8, repr.plot.height = 4) ggplot(data, aes(reorder(Cultivar, Accuracy), Accuracy)) + geom_point(aes(color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1)))) + geom_segment(aes(x=Cultivar, xend=Cultivar, y=0, yend=Accuracy, color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1))), size=1) + xlab(&quot;Cultivar&quot;) + theme_bw() + theme(legend.title=element_blank(), axis.text.x=element_text(angle=90, hjust=1))+ theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) (#fig:accuracy_cultivar)Predictive accuracy for cultivars. #ggsave(&quot;images/cultivAcc.tiff&quot;, width=8, height=3) 3.6 Comparison with non-informative classification The non-informative classification consists of an equal distribution of 50% successful and 50% unsuccessful classification cases (Swets J. A., 1988). I run the Chi-square homogenity test to compare predictive accuracy of the random frest model with this non-informative classification model. cm &lt;- confusionMatrix(predicted_rf_grp, test_grp$yieldClass) cm$table # confusion matrix ## Reference ## Prediction HY LY ## HY 197 97 ## LY 136 415 # rf_clr_and_ionomicgroup model&#39;s classification good_class &lt;- cm$table[1,1]+cm$table[2,2] # HY or LY and correctly predicted misclass &lt;- cm$table[1,2]+cm$table[2,1] # wrong classification ml_class &lt;- c(good_class, misclass) # Non-informative model (nim) total &lt;- sum(cm$table) # total number of samples good_nim &lt;- 0.50 * total misclass_nim &lt;- 0.50 * total non_inf_model &lt;- c(good_nim, misclass_nim) # Matrix for chisquare test m &lt;- rbind(ml_class, non_inf_model) m ## [,1] [,2] ## ml_class 612.0 233.0 ## non_inf_model 422.5 422.5 # chisq.test khi2_test &lt;- chisq.test(m) khi2_test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: m ## X-squared = 88.554, df = 1, p-value &lt; 2.2e-16 The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test, the p-value is too low (&lt;&lt; 0.05). 3.7 Train data set backup for true negatives specimens extraction The train data set with predicted yield classes train_df backup will be used to test perturbation vector theory in the next file. I use the rf model. pred_yield &lt;- predict(rf_grp, train_grp) train_df = data.frame(cbind(df[split_index, ], pred_yield)) write_csv(train_df, &quot;output/train_df.csv&quot;) nrow(train_df) ## [1] 2537 I consider as True Negatives (TN) specimens for this study, observations of the training data set having a high yield (HY) and correctly predicted with the rf model. Then, I compute clr centroids for cultivars using True Negatives original (not scaled) clr values. Thes centroids could be used as clr norms for ionomics groups. TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; pred_yield == &#39;HY&#39;, ] TNmedianNorms &lt;- TNs %&gt;% group_by(group_i) %&gt;% select(clrNo) %&gt;% summarise_all(list(median)) ## Adding missing grouping variables: `group_i` TNmedianNorms ## # A tibble: 6 x 7 ## group_i clrN clrP clrK clrMg clrCa clrFv ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.731 -1.98 0.512 -1.83 -0.978 3.56 ## 2 2 0.910 -1.93 0.532 -1.92 -1.14 3.57 ## 3 3 0.894 -2.22 0.359 -1.60 -1.20 3.83 ## 4 4 0.747 -1.97 0.219 -1.48 -1.04 3.50 ## 5 5 0.601 -2.20 0.507 -1.60 -0.823 3.40 ## 6 6 0.700 -2.04 0.640 -2.12 -0.827 3.61 "]
]
