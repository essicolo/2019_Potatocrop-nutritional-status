[
["Chapter-Clustering.html", "Chapter 2 Ionome analysis 2.1 Objective 2.2 Useful libraries and custom functions 2.3 Leaves processed compositions data set 2.4 The yield cut-off, low and high yielders delimiter 2.5 Centered log-ratio (clr) centroids computation 2.6 Patterns between potato cultivars 2.7 Axis reduction 2.8 Do clrs affect potato tuber yield? 2.9 Arranging data for Machine Learning", " Chapter 2 Ionome analysis 2.1 Objective This chapter has two objectives. Firstly, I try to assign cultivars to groups based on diagnostic leaves ionomes. Although plant health is a continuous domain rather than a categorical status, yield thresholds are useful for decision-making. Because yield potential varies widely among cultivars, I split experimental data into low- and high-productivity categories using a marketable yield delimiter at the 65^th percentile for each cultivar. Hence, I use high yielders subpopulation (samples which marketable yield is larger than the yield cut-off) to look for patterns (if there are) discriminating groups of similar multivariate compositions (ionomics groups). Then, I check for macroelements that best discriminate cultivars centroids. The discriminant scores are appended to the data frame. In the second stage, I check the effect of clr coordinates on tubers yield in a bayesian linear mixed effect model. The third sudsection of the chapter maps the experimental sites locations. At the end, the output data file is called dfml_df.csv i.e., the data frame for machine learning chapter (Chapter 3). 2.2 Useful libraries and custom functions A set of packages is needed for data manipulation and visualization like tidyverse presented in previous chapter (Chapter 1), mvoutlier for multivariate outliers detection, ade4 gathers tools for multivariate data analysis (I use it for discriminant analysis), dbscan a density-based clustering algorithm, which can be used to identify clusters of any shape in data set containing noise and outliers, factoextra needed with dbscan for clustering and visualization, ggmap makes it easy to retrieve raster map tiles from online mapping services like Google Maps and Stamen Maps and plot them using the ggplot2 framework, ggridges for geoms to make ridgeline plots using ggplot2, cowplot which can combine multiple ggplot plots to make publication-ready plots, extrafont for custom fonts for graphs with ggplot2, ggrepel provides text and label geoms for ‘ggplot2’ that help to avoid overlapping text labels, and brms for bayesian regression modeling using Stan. library(&quot;tidyverse&quot;) library(&quot;mvoutlier&quot;) library(&quot;ade4&quot;) library(&quot;dbscan&quot;) library(&quot;factoextra&quot;) library(&quot;ggrepel&quot;) library(&quot;ggmap&quot;) library(&quot;ggridges&quot;) library(&quot;cowplot&quot;) library(&quot;extrafont&quot;) library(&quot;brms&quot;) 2.3 Leaves processed compositions data set For this chapter, the initial data set is the outcome of the previous chapter (Chapter 1) leaf_clust_df.csv. Let’s load the data frame. leaf_clust_df &lt;- read_csv(&quot;output/leaf_clust_df.csv&quot;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## Annee = col_double(), ## LatDD = col_double(), ## LonDD = col_double(), ## AnalyseFoliaireStade = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## RendVendable = col_double(), ## leaf_countNA = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double() ## ) The next cell maps experimental sites locations. qc_leaf &lt;- get_stamenmap(bbox = c(left = -76, right = -68, bottom = 45, top = 50), zoom = 7, maptype = &#39;toner-lite&#39;) ggmap(qc_leaf) + geom_point(data = leaf_clust_df %&gt;% select(LonDD, LatDD) %&gt;% unique(), aes(x = LonDD, y = LatDD), size = 2, shape = 1) + coord_map(&quot;mercator&quot;) + labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;) + theme_bw() + theme(text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) Figure 2.1: Location of experimental sites in the Québec potato data set. ggsave(&quot;images/leaf-df-sites-locations.png&quot;, width = 10, height = 8) 2.4 The yield cut-off, low and high yielders delimiter For cluster analysis, I keep only high yielders which I fixed as yield 65% quantile cutter for each cultivar. The cutQ table is used to add the variable yieldClass categorising yield potential to leaf_clust_df. HY and LY stand for high yield and low yield respectively. cutQ &lt;- leaf_clust_df %&gt;% group_by(Cultivar) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, quantile, probs = 0.65, na.rm = TRUE) %&gt;% rename(rv_cut = RendVendable) ## Adding missing grouping variables: `Cultivar` leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(cutQ, by = &quot;Cultivar&quot;) %&gt;% mutate(yieldClass = forcats::fct_relevel(ifelse(RendVendable &gt;= rv_cut, &quot;HY&quot;, &quot;LY&quot;), &quot;LY&quot;)) For sake of verification, I compute average yield per yieldClass. mean_yield &lt;- leaf_clust_df %&gt;% group_by(yieldClass) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## Adding missing grouping variables: `yieldClass` mean_yield ## # A tibble: 3 x 2 ## yieldClass RendVendable ## &lt;fct&gt; &lt;dbl&gt; ## 1 LY 25.8 ## 2 HY 40.5 ## 3 &lt;NA&gt; NaN So, the average marketable yield is 40.48 Mg \\(ha^-1\\) for high yielders and 24.78 Mg \\(ha^-1\\) for low yielders. In comparison, average potato tuber yields in 2017 in Canada and in Québec were respectively 31.21 Mg \\(ha^-1\\) and 28.75 Mg \\(ha^-1\\). 2.5 Centered log-ratio (clr) centroids computation Compositional data transformation is done in the loaded file. I keep only clr-transformed coordinates for high yielders, at 10 % blossom (AnalyseFoliaireStade = 10% fleur) in the next chunk. hy_df &lt;- leaf_clust_df %&gt;% mutate(isNA = apply(select(., starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable), 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; yieldClass == &quot;HY&quot; &amp; NoEssai != &quot;2&quot;) %&gt;% select(NoEssai, NoBloc, NoTraitement, starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable) %&gt;% droplevels() 1334 lines of observations (samples) will be used to find patterns in potato cultivars. I check the number of samples per cultivar in this high yielders data set. Some cultivars have been discarded from the table after the previous filter. percentage &lt;- round(with(hy_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(hy_df, cbind(numHY = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numHY &lt;- as.numeric(as.character(distribution$numHY)) # numHY is the number of samples distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) distribution &lt;- distribution %&gt;% arrange(desc(numHY)) # arrange in descending order Figure 2.2: High yielders cultivars abundance. Some cultivars are well represented, like Superior and Goldrush. Let’s compute number of cultivars and trials in the data frame. data.frame(numb_cultivars = n_distinct(hy_df$Cultivar, na.rm = TRUE), numb_trials = n_distinct(hy_df$NoEssai, na.rm = TRUE)) ## numb_cultivars numb_trials ## 1 47 151 I create a table with cultivars, maturity classes and compute median clr values i.e., clr centroids. hy_clr &lt;- hy_df %&gt;% group_by(Cultivar, Maturity5) %&gt;% select(Cultivar, Maturity5, starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) hy_clr ## # A tibble: 47 x 8 ## # Groups: Cultivar [47] ## Cultivar Maturity5 clr_N clr_P clr_K clr_Ca clr_Mg clr_Fv ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont early 0.701 -2.02 0.664 -1.11 -1.64 3.47 ## 2 AC Chaleur early 0.696 -1.97 0.349 -1.04 -1.54 3.51 ## 3 Amandine mid-season 0.519 -2.29 0.557 -0.696 -1.62 3.53 ## 4 Ambra mid-season 0.713 -1.98 0.275 -1.08 -1.39 3.46 ## 5 Andover early mid-season 0.872 -2.17 0.738 -0.808 -2.28 3.57 ## 6 Aquilon mid-season 0.783 -1.86 0.387 -0.901 -1.83 3.48 ## 7 Argos late 0.767 -1.77 0.570 -1.28 -1.70 3.43 ## 8 Atlantic mid-season 0.784 -1.87 0.155 -1.09 -1.50 3.56 ## 9 Bijou Rouge early 0.942 -1.93 0.563 -1.12 -1.93 3.61 ## 10 Carolina early 0.598 -2.19 0.131 -0.837 -1.34 3.63 ## # ... with 37 more rows I use multivariate outliers detection technique to identify outliers with a quantitle critical value of qcrit = 0.975 by cultivar only if cultivars contain at leat 20 rows. If less than 20 rows, all rows are kept. The new data frame hy_df_in will be used for patterns recognition and discriminant analysis. hy_df_IO &lt;- hy_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% do({ if (nrow(.) &lt; 20) { IO = rep(1, nrow(.)) } else { IO = sign1(.[,-1], qcrit=0.975)$wfinal01 } cbind(.,IO) }) %&gt;% ungroup() hy_df_in &lt;- hy_df_IO %&gt;% filter(IO == 1) %&gt;% select(-IO) %&gt;% droplevels() 144 outliers have been discarded. 2.6 Patterns between potato cultivars Patterns recognition is done with dbscan algorithm which can identify dense regions measured by the number of objects close to a given point. As explained by the author, the key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points. I use the high yielders clr centroids of cultivars in a new data frame which is the same as hy_df_in without maturity classes. hy_centroids &lt;- hy_df_in %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) %&gt;% ungroup() ## Adding missing grouping variables: `Cultivar` Two important parameters are required for dbscan: epsilon (“eps”) and minimum points (“MinPts”). The parameter eps defines the radius of neighborhood around a point x. It’s called the \\(\\epsilon\\)-neighborhood of x. The parameter MinPts is the minimum number of neighbors within “eps” radius. The optimal value of “eps” parameter can be determined as follow: set.seed(5773) hy_centroids %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% as.matrix() %&gt;% kNNdistplot(., k = 5) abline(h = 0.3, lty = 2) Figure 2.3: The optimal value of “eps” parameter. The chunk below makes the prdiction of clusters delineated by the dbscan algorithm. Zeros are not a cluster or designates the cluster of outliers. res &lt;- hy_centroids %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% as.matrix() %&gt;% dbscan(., eps = .3, minPts = 5) predict(res) ## [1] 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 ## [36] 0 1 1 1 0 1 1 1 0 1 1 1 The result can also be visualized graphically as follow: fviz_cluster(res, hy_centroids %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% as.matrix(), geom = &quot;point&quot;, ggtheme = theme_bw()) Figure 2.4: Clusters delineated with dbscan. Black points are outliers (zeros). As shown on the plot, one cluster means there is no detectable shape between cultivars ionomes as dots are scattered differently. It may not be useful to think of possible strcutures between potato cultivars based on ionome. Nethertheless, one could extract scores of the first two discriminant axes as numerical variables for cultivars in models, and then check for elements that best discriminate cultivars. 2.7 Axis reduction Leaves composition data are compositionnal data (non-negative and summing to unity) so are multivariate. As argued by Legendre et Legendre, 2012, it is not possible to draw such a diagram on paper with more than two or eventually three dimensions, however, even though it is a perfectly valid mathematical construct. For the purpose of analysis, I will project the multidimensional scatter diagram onto bivariate graph which axes are known to be of particular interest. The axes of this graph are chosen to represent a large fraction of the variability of the multidimensional N-P-K-Mg-Ca-Fv data matrix, in a space with reduced (i.e. lower) dimensionality relative to the original data set. With the next chunks, I perform a duality diagram (or dudi) PCA (priciple component analysis) and store scores and loadings. pca_leaf &lt;- dudi.pca(hy_df_in %&gt;% select(starts_with(&quot;clr&quot;)), scannf = FALSE, scale = FALSE) lda_leaf &lt;- discrimin(dudi = pca_leaf, fac = factor(hy_df_in$Cultivar), scannf = FALSE) if (! &quot;DS1&quot; %in% names(hy_df_in)) hy_df_in &lt;- bind_cols(hy_df_in, lda_leaf$li) lda_loading &lt;- lda_leaf$fa %&gt;% rownames_to_column(&quot;clr&quot;) This chunk computes discriminant scores centroids for cultivars. lda_centroids &lt;- hy_df_in %&gt;% select(Cultivar, DS1, DS2) %&gt;% group_by(Cultivar) %&gt;% summarise_all(list(mean)) write_csv(lda_centroids, &quot;output/lda_centroids&quot;) I plot the distance biplot as result of discriminant analysis in separate plots for easy reading. g1 &lt;- lda_centroids %&gt;% ggplot(aes(DS1, DS2, label = Cultivar)) + geom_hline(yintercept=0, color=&quot;black&quot;, linetype=2) + geom_vline(xintercept=0, color=&quot;black&quot;, linetype=2) + geom_label_repel() + geom_point(alpha = 0.5) + xlim(c(-4, 4)) + ylim(c(-6, 5)) + guides(colour=FALSE) + theme_bw(base_size = 12) + theme(axis.text=element_text(size=12), text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) g2 &lt;- ggplot(data=lda_loading) + geom_hline(yintercept=0, color=&quot;black&quot;, linetype=2) + geom_vline(xintercept=0, color=&quot;black&quot;, linetype=2) + geom_segment(aes(xend=DS1, yend=DS2), x=0, y=0, size = 1, color=&quot;grey50&quot;) + geom_label(aes(x=DS1, y=DS2, label=clr), size=5, color=&quot;white&quot;, bg = &quot;grey50&quot;)+ xlim(c(-4, 4)) + ylim(c(-6, 5)) + labs(x = &quot;DS1&quot;, y = &quot;DS2&quot;) + theme_bw(base_size = 12) + theme(axis.text=element_text(size = 12), text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) plot_grid(g1, g2) Figure 2.5: Grid plot of discriminant and cluster analyses of potato cultivars. ggsave(&quot;images/discriminant_plots.png&quot;, width=10, height=8, dpi = 300) The first discriminant axis (DS1) is formed mainly by Mg, K and N. The second discriminant axis (DS2) is driven mainly by Fv, K and weakely by Mg and Ca. I append discriminant scores to leaves composition data frame. leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(lda_centroids, by = &quot;Cultivar&quot;) 2.8 Do clrs affect potato tuber yield? I use a bayesian linear mixed effect model to assess the effect of clr coordinates on tubers marketable yield. mm_df &lt;- leaf_clust_df %&gt;% select(RendVendable, AnalyseFoliaireStade, starts_with(&quot;clr&quot;), NoEssai) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% filter(RendVendable &gt;= 28) %&gt;% select(-c(AnalyseFoliaireStade, is10pcf)) I scale clr coordinates before fitting the linear mixed model. This allow ….? mm_df_sc &lt;- mm_df %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% apply(., 2, scale) %&gt;% as_tibble() %&gt;% mutate(RendVendable = mm_df$RendVendable, NoEssai = mm_df$NoEssai) The STAN model takes some time to compile and sample (~ 20 minutes on my 4-cores computer), so I created a switch to avoid running it every time the code is run. Feel free to switch fit_brmmodel to TRUE. For the model, I discard the filling value to deal with singularity problem. fit_brmmodel &lt;- FALSE # TRUE if(fit_brmmodel) { lmm_b &lt;- brm(RendVendable ~ clr_N + clr_P + clr_K + clr_Ca + clr_Mg + (1|NoEssai), data = mm_df_sc %&gt;% select(RendVendable, starts_with(&quot;clr&quot;), NoEssai), prior = prior(normal(0, 10), class = b) + prior(cauchy(0, 2), class = sd), iter = 4000, warmup = 1000, thin = 1, family = gaussian(), algorithm = &quot;sampling&quot;, chains = 4, cores = 4, # adjust the number of cores seed = 216102) # random.org save(lmm_b, file = &quot;output/lmm_b.RData&quot;) } else { load(&quot;output/lmm_b.RData&quot;) } summary(lmm_b) ## Family: gaussian ## Links: mu = identity; sigma = identity ## Formula: RendVendable ~ clr_N + clr_P + clr_K + clr_Ca + clr_Mg + (1 | NoEssai) ## Data: mm_df_sc %&gt;% select(RendVendable, starts_with(&quot;clr (Number of observations: 2278) ## Samples: 4 chains, each with iter = 4000; warmup = 1000; thin = 1; ## total post-warmup samples = 12000 ## ## Group-Level Effects: ## ~NoEssai (Number of levels: 178) ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sd(Intercept) 7.27 0.41 6.49 8.13 1551 1.00 ## ## Population-Level Effects: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## Intercept 37.75 0.55 36.67 38.82 704 1.01 ## clr_N 2.34 0.32 1.70 2.98 4416 1.00 ## clr_P 1.68 0.40 0.91 2.47 4228 1.00 ## clr_K 1.78 0.50 0.80 2.76 3608 1.00 ## clr_Ca 1.32 0.37 0.61 2.06 4124 1.00 ## clr_Mg 2.33 0.47 1.41 3.27 3571 1.00 ## ## Family Specific Parameters: ## Estimate Est.Error l-95% CI u-95% CI Eff.Sample Rhat ## sigma 4.60 0.07 4.47 4.75 21554 1.00 ## ## Samples were drawn using sampling(NUTS). For each parameter, Eff.Sample ## is a crude measure of effective sample size, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). I assess the goodness of fit by computing the coefficient of determination R-squared using a custom function. I create an intermediate table for this computation. ## R-squared custom function rsq &lt;- function(y, y_pred) { sum((y_pred - mean(y))^2) / sum((y - mean(y))^2) } pred_obs &lt;- predict(lmm_b) %&gt;% as_tibble() %&gt;% select(Estimate) %&gt;% bind_cols(., RendVendable = mm_df_sc %&gt;% na.omit() %&gt;% select(RendVendable)) rsq(pred_obs$RendVendable, pred_obs$Estimate) ## [1] 0.676743 Fixed effects posterior distributions are extracted and tidied. samples &lt;- posterior_samples(lmm_b, &quot;^b&quot;) samples_tidy &lt;- samples %&gt;% gather() %&gt;% separate(col = key, into = c(&quot;col1&quot;, &quot;col2&quot;), sep = &quot;b_&quot;) %&gt;% select(-col1) samples_tidy$clr &lt;- NA samples_tidy$clr[grep(&quot;clr&quot;, samples_tidy$col2)] &lt;- samples_tidy$col2[grep(&quot;clr&quot;, samples_tidy$col2)] samples_tidy &lt;- samples_tidy %&gt;% select(-col2) Then, I plot clrs effects on tubers marketable yield i.e., the posterior distributions of corresponding regression coefficients. samples_tidy %&gt;% filter(!is.na(clr)) %&gt;% ggplot(aes(x = value, y = clr, fill=factor(..quantile..))) + geom_vline(xintercept = 0) + stat_density_ridges(rel_min_height = 0.01, geom = &quot;density_ridges_gradient&quot;, calc_ecdf = TRUE, quantiles = c(0.025, 0.975), scale = 1) + scale_fill_manual( name = &quot;Probability&quot;, values = c(&quot;#FF0000&quot;, &quot;#A0A0A0&quot;, &quot;#FF0000&quot;), labels = c(&quot;(0, 0.025]&quot;, &quot;(0.025, 0.975]&quot;, &quot;(0.975, 1]&quot;)) + theme(axis.text=element_text(size = 12), text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) + theme_bw() Figure 2.6: Effects of clr nutrients on tubers yield. ggsave(&quot;images/bayes-mixed-clr.png&quot;, width=10, height=8, dpi = 300) Each regression coefficient refers to the effect of explanatory variable i.e., one clr variable, on the expectation of the response variable i.e., marketable yield, adjusted for the rest of covariates. Posterior distributions far away from the zero value indicate an important contribution of clrs on the prediction of tubers yield. Moreover, the relationship is positive for all the clrs. Positive association means that changes of the explanatory variable cause changes of the same direction for response variable. 2.9 Arranging data for Machine Learning I select useful columns for the next chapter (Chapter 3) and save the new table as dfml.csv. I conserved only complete cases. dfml &lt;- leaf_clust_df %&gt;% select(NoEssai, NoBloc, NoTraitement, starts_with(&quot;clr&quot;), RendVendable, rv_cut, yieldClass, AnalyseFoliaireStade, Cultivar, Maturity5) %&gt;% mutate(isNA = apply(select(., starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable), 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% select(-c(AnalyseFoliaireStade, isNA, is10pcf)) %&gt;% droplevels() %&gt;% filter(complete.cases(.)) write_csv(dfml, &quot;output/dfml.csv&quot;) The Machine learning data table contains 3382 samples. "]
]
