[
["Chapter-Clustering.html", "Chapter 2 Cluster analysis of potato cultivars 2.1 Objective 2.2 Useful libraries and custom functions 2.3 Leaves processed compositions data set 2.4 High yielders delimiter 2.5 clr centroids computation 2.6 Axis reduction 2.7 Cascade K Means clustering 2.8 Arranging data for Machine Learning 2.9 clr X ionomics groups interactions effects", " Chapter 2 Cluster analysis of potato cultivars 2.1 Objective Although plant health is a continuous domain rather than a categorical status, yield thresholds are useful for decision-making. Because yield potential varies widely among cultivars, I use this chapter to split experimental data into low- and high-productivity categories using a marketable yield delimiter at the 65th percentile for each cultivar. Hence, I use high yielders subpopulation (samples which marketable yield is larger than the yield cut-off) to perform a cascade k-means clustering aiming to discriminate Groups of similar multivariate compositions (ionomics groups). I combine a discriminant analysis to check the macroelements that best discriminate cultivars. I also check the performence of Groups discrimination by computing the Groups X clr-coordinates interaction effect coefficients in a linear mixed effect model. I finally append the new variable ionomicGroupto the leaves clr data frame and discard all the rows with any missing data. I map the experimental sites locations in the third sudsection of this chapter. The output data file is called leaf_ml_df.csv i.e., the data frame for machine learning chapter 3. *** 2.2 Useful libraries and custom functions We need a set of packages for data handling and visualization like the tidyverse package presented in previous chapter 1, ellipse offers functions for drawing ellipses and ellipse-Like confidence regions (maybe I will set this option to FLASE for graph simplicity), mvoutlier for multivariate outliers detection, ade4 gathers tools for multivariate data analysis (I use it for discriminant analysis), vegan performs cluster analysis by k-means or cascade k-means clustering, ggmap makes it easy to retrieve raster map tiles from online mapping services like Google Maps and Stamen Maps and plot them using the ggplot2 framework, extrafont for custom fonts for graphs with ggplot2, ggrepel provides text and label geoms for ‘ggplot2’ that help to avoid overlapping text labels, plotly …, nlme for linear and non-linear mixed effect modeling. Some custom functions are also loaded mainly for Pseudo r-square caculation with linear mixed effect model and discriminant biplot. library(&quot;tidyverse&quot;) library(&#39;ellipse&#39;) library(&quot;mvoutlier&quot;) library(&quot;ade4&quot;) library(&quot;vegan&quot;) library(&quot;extrafont&quot;) library(&quot;ggrepel&quot;) library(&quot;ggmap&quot;) #library(&quot;plotly&quot;) library(&quot;nlme&quot;) source(&quot;data/functions.R&quot;) source(&#39;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_trad.R&#39;) 2.3 Leaves processed compositions data set For this chapter, the initial data set is the outcome of the prvious chapter (1) leaf_clust_df. I load the data frame and create vectors of columns I will use in this chapter. leaf_clust_df &lt;- read_csv(&quot;output/leaf_clust_df.csv&quot;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## Annee = col_double(), ## LatDD = col_double(), ## LonDD = col_double(), ## AnalyseFoliaireStade = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## RendVendable = col_double(), ## leaf_countNA = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double() ## ) keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) cultivarAndyield &lt;- c(&#39;Cultivar&#39;, &#39;Maturity5&#39;, &#39;RendVendable&#39;) extra_col &lt;- c(&#39;LatDD&#39;, &#39;LonDD&#39;, &#39;AnalyseFoliaireStade&#39;) The next cell maps experimental sites locations. #library(&quot;ggmap&quot;) qc_leaf &lt;- get_stamenmap(bbox = c(left=-76, right=-68, bottom=45, top=50), zoom=7, maptype = &#39;toner-lite&#39;) ggmap(qc_leaf) + geom_point(data = leaf_clust_df %&gt;% select(LonDD, LatDD) %&gt;% unique(), aes(x = LonDD, y = LatDD), size = 2, shape = 1) + coord_map(&quot;mercator&quot;) + theme_bw() + theme(text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) (#fig:leaf_df-sites-locations)Location of experimental sites (green dots) in the Québec potato data set. #ggsave(&quot;images/leaf_clust_df-sites-locations.png&quot;, width=10, height=8) 2.4 High yielders delimiter For cluster analysis, I keep only high yielders i.e. yield 65% quantile cutter for each cultivar. The cutQ table contains the yield delimiter for each cultivar. Then, this table is used to add the variable yieldClass to leaf_clust_df. HY and LY stand for high yield and low yield respectively. cutQ &lt;- leaf_clust_df %&gt;% group_by(Cultivar) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, quantile, probs=0.65, na.rm = TRUE) %&gt;% rename(rv_cut = RendVendable) ## Adding missing grouping variables: `Cultivar` leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(cutQ, by = &quot;Cultivar&quot;) %&gt;% mutate(yieldClass = forcats::fct_relevel(ifelse(RendVendable &gt;= rv_cut, &quot;HY&quot;, &quot;LY&quot;), &quot;LY&quot;)) For sake of verification, I compute average yield per yieldClass. meanYield = leaf_clust_df %&gt;% group_by(yieldClass) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## Adding missing grouping variables: `yieldClass` meanYield ## # A tibble: 3 x 2 ## yieldClass RendVendable ## &lt;fct&gt; &lt;dbl&gt; ## 1 LY 25.8 ## 2 HY 40.5 ## 3 &lt;NA&gt; NaN Average marketable yield is 40.48 Mg ha-1 for high yielders and 24.78 Mg ha^-1 for low yielders. In comparison, average potato tuber yields in 2017 in Canada and in Québec were respectively 40 Mg ha^-1 and 38.4 Mg ha^-1. 2.5 clr centroids computation Compositional data transformation is done in the loaded file. I keep only clr-transformed coordinates for high yielders, at 10 % blossom (AnalyseFoliaireStade = 10% fleur) in the next chunk. highYielders_df &lt;- leaf_clust_df %&gt;% mutate(isNA = apply(.[c(clr_no, cultivarAndyield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; yieldClass == &quot;HY&quot; &amp; NoEssai != &quot;2&quot;) %&gt;% select(one_of(keys_col, clr_no, cultivarAndyield)) %&gt;% droplevels() nrow(highYielders_df) ## [1] 1334 So, 1334 lines of observations (samples) will be used for potato cultivars clustering. I Check how many rows of data are they for each cultivar. percentage &lt;- round(with(highYielders_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(highYielders_df, cbind(numHY = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numHY &lt;- as.numeric(as.character(distribution$numHY)) # numHY = number of samples distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) distribution %&gt;% arrange(desc(numHY)) %&gt;% head(5) # arrange in descending order ## numHY percentage Cultivar ## 1 199 14.92 Superior ## 2 197 14.77 Goldrush ## 3 127 9.52 FL 1207 ## 4 119 8.92 Chieftain ## 5 88 6.60 Atlantic Some cultivars are well represented, like Superior and Goldrush. Let’s compute number of cultivars and trials in the data frame. data.frame(numb_cultivars = n_distinct(highYielders_df$Cultivar, na.rm = TRUE), numb_trials = n_distinct(highYielders_df$NoEssai, na.rm = TRUE)) ## numb_cultivars numb_trials ## 1 47 151 I create a table with cultivars, maturity classes and computed median clr values i.e., clr centroids. highYielders_clr &lt;- highYielders_df %&gt;% group_by(Cultivar, Maturity5) %&gt;% select(Cultivar, Maturity5, starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) highYielders_clr ## # A tibble: 47 x 8 ## # Groups: Cultivar [47] ## Cultivar Maturity5 clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont early 0.701 -2.02 0.664 -1.64 -1.11 3.47 ## 2 AC Chaleur early 0.696 -1.97 0.349 -1.54 -1.04 3.51 ## 3 Amandine mid-season 0.519 -2.29 0.557 -1.62 -0.696 3.53 ## 4 Ambra mid-season 0.713 -1.98 0.275 -1.39 -1.08 3.46 ## 5 Andover early mid-season 0.872 -2.17 0.738 -2.28 -0.808 3.57 ## 6 Aquilon mid-season 0.783 -1.86 0.387 -1.83 -0.901 3.48 ## 7 Argos late 0.767 -1.77 0.570 -1.70 -1.28 3.43 ## 8 Atlantic mid-season 0.784 -1.87 0.155 -1.50 -1.09 3.56 ## 9 Bijou Rouge early 0.942 -1.93 0.563 -1.93 -1.12 3.61 ## 10 Carolina early 0.598 -2.19 0.131 -1.34 -0.837 3.63 ## # ... with 37 more rows I use multivariate outliers detection technic to identify outliers with a criterion of detection limit of 0.975 by cultivar only if cultivars contain at leat 20 rows. If less than 20 rows, all rows are kept. The new data frame will be used for discriminant analysis. I call it lda_df. highYielders_df_IO &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% do({ if (nrow(.) &lt; 20) { IO = rep(1, nrow(.)) } else { IO = sign1(.[,-1], qcrit=0.975)$wfinal01 } cbind(.,IO) }) lda_df &lt;- highYielders_df_IO %&gt;% filter(IO == 1) %&gt;% droplevels() nrow(lda_df) ## [1] 1196 Hence, \\r nrow(highYielders_df)-nrow(lda_df) outliers have been discarded from the analysis. The remaining data frame contains \\r n_distinct(lda_df$Cultivar, na.rm = TRUE) cultivars from \\r n_distinct(lda_df$NoEssai, na.rm = TRUE) trials. 2.6 Axis reduction Leaves composition data are compositionnal data (non-negative and summing to unity) so are multivariate. As argued by Legendre et Legendre, 2012 It is not possible to draw such a diagram on paper with more than two or eventually three dimensions, however, even though it is a perfectly valid mathematical construct. For the purpose of analysis, I will project the multidimensional scatter diagram onto bivariate graphs whose axes are known to be of particular interest. The axes of these graphs are chosen to represent a large fraction of the variability of the multidimensional N-P-K-Mg-Ca-Fv data matrix, in a space with reduced (i.e. lower) dimensionality relative to the original data set. With the next chunks, I perform a duality diagram (or dudi) PCA (priciple component analysis) and store scores and loadings. I also filter the data frame so that cultivars with less than 5 occurences will be masked on the resulting biplot. pca_leaf &lt;- dudi.pca(lda_df[clr_no], scannf = FALSE, scale = FALSE) lda_leaf &lt;- discrimin(dudi = pca_leaf, fac = factor(lda_df$Cultivar), scannf = FALSE) lda_leaf_score = lda_leaf$li lda_leaf_loading = lda_leaf$fa lda_df$Cultivar &lt;- factor(lda_df$Cultivar) lda_leaf_group &lt;- lda_df$Cultivar n_cultivar &lt;- table(lda_df$Cultivar) # Do not schow Cultivars whose number of occurrences is &lt; 5 n_filter &lt;- lda_leaf_group %in% names(n_cultivar[n_cultivar &gt;= 5]) filter_cultivars &lt;- names(n_cultivar[n_cultivar &gt;= 5]) lda_df_filter &lt;- lda_df[n_filter, ] I can plot the tThe distance biplot as result of discriminant analysis. I use a custom function. options(repr.plot.width = 6, repr.plot.height = 6) plot_lda(score = lda_leaf_score[n_filter, ], loading = lda_leaf_loading[n_filter, ], group = lda_leaf_group[n_filter], ell_dev = FALSE, ell_err = FALSE, #TRUE, scale_load = 0.5, level = 0.95, legend = FALSE, label = TRUE, transparency = 0.3, xlim = c(-2, 2), ylim = c(-3.5, 3), points = FALSE) Figure 2.1: Discriminant distance biplot of potato cultivars. The first discriminant axis (DS1) is formed mainly by Mg, K and N. The second discriminant axis (DS2) is driven mainly by the Fv, K and weakely by Ca. I perform cluster analysis in next subsection. 2.7 Cascade K Means clustering What is cascade K Means Clustering? It is an an unsupervised learning algorithm inspired by its similar K Means clustering both trying to cluster data based on their similarity. There is no outcome to be predicted. The algorithm just tries to find patterns in the data. In k means clustering, we have to specify the number of clusters we want the data to be grouped into. In cascade K means method, we give a minimum and a maximum number of cluster wanted. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps: reassign data points to the cluster whose centroid is closest, Calculate new centroid of each cluster. These two steps are repeated till the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids. I use the high yielders clr centroids (medians) for cultivars in a next data frame which is the same as highYielders_clr without maturity classes, and Calinski-Harabasz (1974) criterion (package vegan) for clustering.The chunk also plots the process results. highYieldersCentroids &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) set.seed(5773) highYieldersKmeans &lt;- cascadeKM(highYieldersCentroids[, -1], inf.gr = 3, sup.gr = 8, criterion = &quot;ssi&quot;) options(repr.plot.width = 6, repr.plot.height = 4) plot(highYieldersKmeans) Figure 2.2: K-means partitions comparison (calinski criterion). The red dot of the right hand side graph shows 3 optimal clustering partitions which is the inferior limit I gave to the process. I check the partitions data frame. highYieldersKmeans$partition %&gt;% head() ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups ## 1 1 3 1 1 4 3 ## 2 3 2 3 4 6 4 ## 3 2 1 5 5 7 5 ## 4 3 2 3 4 6 8 ## 5 1 4 4 6 5 2 ## 6 1 4 1 1 4 3 I consider 6 groups established from clustering (...) corresponding to the second column. I can use the reference of the column or its name 6 groups directly. I add it up to the centroids data frame. highYieldersCentroids$kgroup &lt;- highYieldersKmeans$partition[, &quot;6 groups&quot;] I compute discriminant scores centroïdes for cultivars. lda_centroids &lt;- lda_leaf_score %&gt;% mutate(group = lda_leaf_group) %&gt;% group_by(group) %&gt;% summarise_all(list(mean)) Plot Cultivar groups in LDA with a custom function. options(repr.plot.width = 6, repr.plot.height = 6) plot_lda(score=lda_leaf_score[n_filter, ], loading=lda_leaf_loading[n_filter, ], group = lda_leaf_group[n_filter], ell_dev=FALSE, ell_err= FALSE, #TRUE, scale_load = 0.4, level=0.95, legend=FALSE, label=TRUE, transparency=0.4, xlim = c(-2.5, 2), ylim = c(-3, 4), points=F) # colour dots for groups (or clusters) col = factor(highYieldersCentroids[highYieldersCentroids$Cultivar %in% filter_cultivars, &#39;kgroup&#39;][[1]]) # remove the cultivar column points(lda_centroids[lda_centroids$group %in% filter_cultivars, c(&#39;DS1&#39;, &#39;DS2&#39;)], pch = 19, col = col, cex = 0.9) legend(1.3, 4, legend = paste(rep(&#39;cluster&#39;, nlevels(col)), as.numeric(levels(col))), pch = 19, col = unique(col), cex = 0.9) Figure 2.3: Discriminant distance biplot of potato cultivars showing ionomics groups. It’s a bit difficult to colour cultivar name in the plot with the custom function. I use functions from packages ggplot2, ggrepel and plotly instead. New intermediate data frames are created for this purpose. cultivars_filtre &lt;- data.frame(Cultivar = filter_cultivars, i_group=col) df &lt;- data.frame( score = lda_leaf_score[n_filter, ], loadings = lda_leaf_loading[n_filter,], Cultivar = lda_leaf_group[n_filter] ) df &lt;- df %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) ## Warning: Column `Cultivar` joining factors with different levels, coercing ## to character vector df %&gt;% head() ## score.DS1 score.DS2 loadings.DS1 loadings.DS2 Cultivar i_group ## 1 1.0366780 1.0148643 1.9989657 -0.1477988 AC Belmont 1 ## 2 0.5973192 1.5106837 -0.1794520 0.7415188 AC Belmont 1 ## 3 0.7195584 1.3363729 2.0112615 2.4201235 AC Belmont 1 ## 4 0.5343385 0.7486986 -2.3193130 1.3696071 AC Belmont 1 ## 5 0.4140698 1.1521065 0.2748907 1.3266327 AC Belmont 1 ## 6 0.3382326 1.1899525 -1.7863528 -5.7100833 AC Belmont 1 centroids = lda_centroids[lda_centroids$group %in% filter_cultivars, ] names(centroids)[match(&quot;group&quot;, names(centroids))] &lt;- &quot;Cultivar&quot; centroids &lt;- centroids %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) centroids %&gt;% head() ## # A tibble: 6 x 4 ## Cultivar DS1 DS2 i_group ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 AC Belmont 0.122 1.02 1 ## 2 AC Chaleur -0.543 0.124 4 ## 3 Andover 1.93 -0.0478 6 ## 4 Aquilon 0.238 0.240 1 ## 5 Argos 0.157 0.859 2 ## 6 Atlantic -1.22 -0.413 4 The chunk below plots discriminant biplot with colours corresponding to ionomics groups. options(repr.plot.width = 9, repr.plot.height = 9) g &lt;- ggplot(centroids, aes(DS1, DS2, label = Cultivar, col=i_group)) + geom_text_repel() + geom_point(alpha = 0.5) + theme_classic(base_size = 12) + #scale_color_manual(values=c(&quot;red&quot;, &quot;magenta&quot;, &quot;blue&quot;, &quot;black&quot;)) + theme(axis.text=element_text(size=12)) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) # Add discriminant loadings using geom_segment() and arrow() x=0; y=0; labels = c(clr_no, rep(NA, nrow(df)-length(clr_no))) g + geom_segment(data=df, mapping=aes(x=x, y=y, xend=x+loadings.DS1, yend=y+loadings.DS2), arrow=arrow(), size = 1, color=&quot;grey80&quot;) + geom_text(data=df, mapping=aes(x=loadings.DS1, y=loadings.DS2, label=labels), size=5, color=&quot;black&quot;) + geom_hline(yintercept=0, color=&quot;black&quot;, linetype=2) + geom_vline(mapping=aes(xintercept=0), color=&quot;black&quot;, linetype=2) + theme(axis.line=element_blank()) Figure 2.4: Discriminant biplot and cluster analysis result of potato cultivars. #ggsave(&quot;images/cultivar_clust.png&quot;, width=10, height=8, dpi = 300) I push cultivars yield cut-off and ionomics groups in the initial data frame. ionomicGroup &lt;- data.frame(lda_centroids[, 1], ionomicGroup = factor(highYieldersKmeans$partition[, &quot;6 groups&quot;])) colnames(ionomicGroup)[colnames(ionomicGroup)==&quot;group&quot;] &lt;- &quot;Cultivar&quot; cutQ &lt;- cutQ[-1, ] # to discard missing cultivar names colnames(cutQ)[which(names(cutQ) == &quot;rv_cut&quot;)] &lt;- &quot;yieldCutoff&quot; cutQ_ig &lt;- cutQ %&gt;% left_join(ionomicGroup, by = &quot;Cultivar&quot;) leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(y = cutQ_ig, by = &#39;Cultivar&#39;) %&gt;% select(-rv_cut) 2.8 Arranging data for Machine Learning I select useful columns for the next chapter (3) and save the new table as dfml.csv. I conserve only complete cases. new_col &lt;- c(&#39;yieldCutoff&#39;, &#39;yieldClass&#39;, &#39;ionomicGroup&#39;) dfml &lt;- leaf_clust_df %&gt;% select(one_of(c(keys_col, clr_no, cultivarAndyield, new_col, &#39;AnalyseFoliaireStade&#39;))) %&gt;% mutate(isNA = apply(.[c(clr_no, cultivarAndyield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% select(-c(AnalyseFoliaireStade, isNA, is10pcf)) %&gt;% droplevels() %&gt;% filter(complete.cases(.)) nrow(dfml) ## [1] 3382 write_csv(dfml, &quot;output/dfml.csv&quot;) Hence, the data table contains 3382 for the next chapter (Machine learning). 2.9 clr X ionomics groups interactions effects I use a linear mixed effect model to assess the effect of clr coordinates on tubers marketable yield between ionomics groups. This last subsection perfom this analysis by extracting and ploting the interaction coefficients of the model. dfml$Cultivar &lt;- factor(dfml$Cultivar) #dfml$Maturity5 &lt;- relevel(dfml$Maturity5, ref = &quot;late&quot;) dfml$NoEssai &lt;- factor(dfml$NoEssai) colnames(dfml)[colnames(dfml)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; dfml$group_i &lt;- factor(dfml$group_i) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Ca&quot;, &quot;clr_Mg&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;, &quot;clrFv&quot;) # for plot colnames(dfml)[which(names(dfml) %in% clr_no)] &lt;- clrNo I scale clr coordinates before ajusting linear mixed model. Discard the filling value to deal with singularity problem. dfml.sc &lt;- dfml # copy dfml.sc[, clrNo] &lt;- apply(dfml.sc[, clrNo], 2, scale) used_clr = c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;) # without &quot;clr_Fv&quot; lmm &lt;- lme(RendVendable ~ (clrN + clrP + clrK + clrCa + clrMg):group_i, data=dfml.sc, random= ~1|NoEssai) pseudoR2 = rsq(dfml.sc$RendVendable, predict(lmm)) pseudoR2 ## [1] 0.7369405 The next code extracts the interactions coefficients of the model and their p-values (pv) matrix. pv &lt;- summary(lmm)$tTable[-1,] pv ## Value Std.Error DF t-value p-value ## clrN:group_i1 5.60218758 0.5496386 3151 10.19249231 5.031902e-24 ## clrN:group_i2 4.30397894 0.7336432 3151 5.86658313 4.909999e-09 ## clrN:group_i3 1.21367104 5.7005996 3151 0.21290235 8.314169e-01 ## clrN:group_i4 8.54331361 0.6871694 3151 12.43261589 1.116381e-34 ## clrN:group_i5 6.34147974 2.2943571 3151 2.76394630 5.743895e-03 ## clrN:group_i6 0.29174441 1.0533480 3151 0.27696868 7.818223e-01 ## clrP:group_i1 4.28649269 0.5866128 3151 7.30719260 3.440135e-13 ## clrP:group_i2 4.51000950 1.1678025 3151 3.86196270 1.147409e-04 ## clrP:group_i3 1.16001050 8.6354916 3151 0.13433057 8.931498e-01 ## clrP:group_i4 8.64313859 0.9100128 3151 9.49782120 4.095742e-21 ## clrP:group_i5 7.23955939 3.0563011 3151 2.36873239 1.790915e-02 ## clrP:group_i6 -0.24376218 1.5811871 3151 -0.15416404 8.774903e-01 ## clrK:group_i1 4.75027835 0.7993461 3151 5.94270558 3.109982e-09 ## clrK:group_i2 3.20803689 1.2493573 3151 2.56774983 1.028182e-02 ## clrK:group_i3 5.08994127 8.2298083 3151 0.61847629 5.363061e-01 ## clrK:group_i4 8.79256304 0.9535030 3151 9.22132688 5.224314e-20 ## clrK:group_i5 8.01431075 3.3899685 3151 2.36412542 1.813302e-02 ## clrK:group_i6 -1.27526027 2.1219199 3151 -0.60099360 5.478875e-01 ## clrCa:group_i1 6.39980521 0.7295692 3151 8.77203264 2.819484e-18 ## clrCa:group_i2 4.01681887 1.1949631 3151 3.36145847 7.845625e-04 ## clrCa:group_i3 4.19209068 14.6386227 3151 0.28637193 7.746121e-01 ## clrCa:group_i4 9.84671173 0.9540141 3151 10.32134813 1.383904e-24 ## clrCa:group_i5 9.26754687 2.7848346 3151 3.32786259 8.851996e-04 ## clrCa:group_i6 0.06304187 1.8499322 3151 0.03407794 9.728172e-01 ## clrMg:group_i1 3.89799879 0.5854246 3151 6.65841358 3.252874e-11 ## clrMg:group_i2 1.70676455 1.0999128 3151 1.55172717 1.208279e-01 ## clrMg:group_i3 -1.70032991 7.2872569 3151 -0.23332921 8.155209e-01 ## clrMg:group_i4 7.53469298 0.8977348 3151 8.39300553 7.061592e-17 ## clrMg:group_i5 7.96978108 3.0177230 3151 2.64099158 8.307192e-03 ## clrMg:group_i6 -0.41568533 1.7171650 3151 -0.24207652 8.087366e-01 Then, I extract their confident intervals, and process data for the plot. interval &lt;- tibble(Estimate = intervals(lmm)$fixed[-1, 2], LL = intervals(lmm)$fixed[-1, 1], UL = intervals(lmm)$fixed[-1, 3]) interval$variable &lt;- rep(&#39;NA&#39;, nrow(interval)) interval$variable &lt;- rownames(intervals(lmm)$fixed)[-1] interval$ionomic_group &lt;- rep(paste(&quot;group&quot;, 1:nlevels(dfml$group_i)), length(clrNo)-1) interval$used_clr &lt;- rep(used_clr, each = nlevels(dfml$group_i)) interval$pvalue &lt;- pv[,&quot;p-value&quot;] interval$is_significant = ifelse(interval$pvalue &lt;= 0.05, &#39;P &lt; 0.05&#39;, &#39;P &gt; 0.05&#39;) interval ## # A tibble: 30 x 8 ## Estimate LL UL variable ionomic_group used_clr pvalue ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5.60 4.52 6.68 clrN:gr~ group 1 clrN 5.03e-24 ## 2 4.30 2.87 5.74 clrN:gr~ group 2 clrN 4.91e- 9 ## 3 1.21 -9.96 12.4 clrN:gr~ group 3 clrN 8.31e- 1 ## 4 8.54 7.20 9.89 clrN:gr~ group 4 clrN 1.12e-34 ## 5 6.34 1.84 10.8 clrN:gr~ group 5 clrN 5.74e- 3 ## 6 0.292 -1.77 2.36 clrN:gr~ group 6 clrN 7.82e- 1 ## 7 4.29 3.14 5.44 clrP:gr~ group 1 clrP 3.44e-13 ## 8 4.51 2.22 6.80 clrP:gr~ group 2 clrP 1.15e- 4 ## 9 1.16 -15.8 18.1 clrP:gr~ group 3 clrP 8.93e- 1 ## 10 8.64 6.86 10.4 clrP:gr~ group 4 clrP 4.10e-21 ## # ... with 20 more rows, and 1 more variable: is_significant &lt;chr&gt; This cell plots the interaction coefficients and their confident intervals using ggplot2. options(repr.plot.width = 4, repr.plot.height = 8) gg &lt;- ggplot(data = interval, mapping = aes(x = Estimate, y = used_clr, color=is_significant)) + facet_grid(ionomic_group ~ .) + #, scales = &#39;free&#39;, space = &#39;free&#39;) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = used_clr, yend = used_clr)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) + theme_bw() + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) gg + theme(legend.title = element_blank())#, legend.position = &#39;bottom&#39;) Figure 2.5: Effect of ionome perturbation on marketable yield as illustrated by a linear mixed effect model. #ggsave(&quot;images/coef_lmm.tiff&quot;, width = 5, height = 5, dpi = 300) "]
]
