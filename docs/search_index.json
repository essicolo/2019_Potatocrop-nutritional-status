[
["index.html", "Balancing the nutritional status of potato crops. Chapter 1 Data processing 1.1 Objective 1.2 Useful libraries for data handling 1.3 Québec potato data set 1.4 Selection of useful variables 1.5 Arranging the data frame 1.6 Cultivar classes correction 1.7 Summarise and backup", " Balancing the nutritional status of potato crops. Zonlehoua Coulibali and Serge-Étienne Parent 2019-11-18 Chapter 1 Data processing 1.1 Objective This chapter is the first of a series of R markdown codes aiming to describe computations methodology used to derive the results and conclusions of potato nutrient diagnosis article. The data set is a collection of potato surveys and N, P and K fertilizer trials conducted in Quebec from 1970 to 2017 between the US border at the 45^th parallel and near the Northern limit of cultivation at the 49^th parallel. The useful variables are the first mature leaf (4^th from top, collected at the beginning of blossom stage) N, P, K, Ca and Mg compositions, cultivars used in experiments and tuber marketable yield. These variables are selected from the Québec potato raw data table (raw_leaf_df.csv) and processed to give useful variables for cultivars clustering (Chapter 2), tuber yield prediction (Chapter 3) and assessment of perturbation vector concept (Chapter 4). A previous exploration showed that oligoelements contained too many missing values, for this reason these elements were excluded from analysis. The chapter ends with the backup of a processed data frame useful for next chapters. 1.2 Useful libraries for data handling We need package tidyverse which loads a set of packages for easy data manipulation and visualization. A set of other packages is used: Amelia for missing data vizualisation, robCompositions to robustely impute missing values in compositional data using k-nearest neigbhors methods, and compositions to transforme compositions into compositionnal space. library(&quot;tidyverse&quot;) library(&#39;Amelia&#39;) library(&quot;robCompositions&quot;) library(&quot;compositions&quot;) 1.3 Québec potato data set Let’s load the Québec potato leaves raw compositions data set raw_leaf_df.csv available for the project in the data folder. raw_leaf_df &lt;- read_csv(&quot;data/raw_leaf_df.csv&quot;) 1.4 Selection of useful variables We create custom vectors of attributes which help select useful data columns for computations. The year of experiment is not needed instead it permits to know how long ago expériements have been monitored. Geographical coordinates are useful to map experimental sites locations later. keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) location &lt;- c(&#39;Annee&#39;, &#39;LatDD&#39;, &#39;LonDD&#39;) cultivars &lt;- c(&#39;AnalyseFoliaireStade&#39;, &#39;Cultivar&#39;, &#39;Maturity5&#39;) discard_col &lt;- c(&quot;AnalyseFoliaireC&quot;, &quot;AnalyseFoliaireS&quot;, &quot;AnalyseFoliaireB&quot;, &quot;AnalyseFoliaireCu&quot;, &quot;AnalyseFoliaireZn&quot;, &quot;AnalyseFoliaireMn&quot;, &quot;AnalyseFoliaireFe&quot;, &quot;AnalyseFoliaireAl&quot;) longNameMacro &lt;- c(&quot;AnalyseFoliaireN&quot;,&quot;AnalyseFoliaireP&quot;,&quot;AnalyseFoliaireK&quot;, &quot;AnalyseFoliaireCa&quot;,&quot;AnalyseFoliaireMg&quot;) outputs &lt;- c(&#39;RendVendable&#39;, &#39;RendPetit&#39;, &#39;RendMoy&#39;, &#39;RendGros&#39;) macroElements &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;) # for simplicity The reduced data frame becomes leaf_df which stands for the diagnostic leaves macroelements composition data frame combining corresponding cultivars names, marketable yield, year of experiment and sites geographical coordinates. leaf_df &lt;- raw_leaf_df %&gt;% select(-discard_col) colnames(leaf_df)[which(names(leaf_df) %in% longNameMacro)] &lt;- macroElements glimpse(leaf_df) ## Observations: 12,991 ## Variables: 18 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ NoBloc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## $ NoTraitement &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6,... ## $ Annee &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ LatDD &lt;dbl&gt; 46.75306, 46.75306, 46.75306, 46.75306, 4... ## $ LonDD &lt;dbl&gt; -72.33861, -72.33861, -72.33861, -72.3386... ## $ AnalyseFoliaireStade &lt;chr&gt; &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;1... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Gold... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;,... ## $ N &lt;dbl&gt; 3.805, 3.356, 3.657, 3.610, 4.983, 4.277,... ## $ P &lt;dbl&gt; 0.22, 0.20, 0.22, 0.18, 0.22, 0.21, 0.28,... ## $ K &lt;dbl&gt; 6.31, 6.85, 6.44, 5.36, 5.92, 6.73, 5.75,... ## $ Ca &lt;dbl&gt; 1.69, 1.31, 1.13, 1.30, 1.23, 1.38, 1.17,... ## $ Mg &lt;dbl&gt; 0.53, 0.21, 0.32, 0.50, 0.44, 0.45, 0.51,... ## $ RendVendable &lt;dbl&gt; 18.9442, 40.3518, 33.0379, 37.5505, 46.00... ## $ RendPetit &lt;dbl&gt; 3.9458, 4.9159, 6.3220, 3.6515, 4.6652, 4... ## $ RendMoy &lt;dbl&gt; 11.4123, 17.5599, 13.6795, 16.2410, 18.99... ## $ RendGros &lt;dbl&gt; 3.5861, 17.8760, 13.0364, 17.6580, 22.345... 1.5 Arranging the data frame These chunks set trial number NoEssai as factor, relevel categorical maturity order variable and choose cultivar Superior as reference as it has the maximum number of observations. Then, abundance of cultivars is ploted. percentage &lt;- round(with(leaf_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(leaf_df, cbind(numbOfsamples = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numbOfsamples &lt;- as.numeric(as.character(distribution$numbOfsamples)) distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) Figure 1.1: Repported cultivars abundance in the potato data frame. leaf_df$NoEssai &lt;- as.factor(leaf_df$NoEssai) leaf_df$Cultivar &lt;- relevel(factor(leaf_df$Cultivar), ref = &quot;Superior&quot;) leaf_df$Maturity5 &lt;- ordered(leaf_df$Maturity5, levels = c(&quot;early&quot;,&quot;early mid-season&quot;, &quot;mid-season&quot;,&quot;mid-season late&quot;,&quot;late&quot;)) We portray missing values for the sake of imputation. As explained in the section ??, we will retain after this processing only reasonably imputable data i.e. samples with a number of missing variable values less than half the number of studied attributes. The next cell maps the portrait of missing values. leafIonome &lt;- leaf_df[macroElements] missmap(leafIonome) Figure 1.2: Portrait of missing macroelements. This figure compiles the samples identifiers on the Y axis and macroelements on the X axis. A complete horizontal unique color band indicates wether the 5 elements are totally observed (blue band) or totally missing (red band). Only N and P have missing values that will be imputed and retained at the end. The totally missing compositions will be removed. The next cell initializes this process. # keep track of empty rows: leaf_df$leaf_allNA &lt;- apply(leaf_df[macroElements], 1, function(X) all(is.na(X))) # keep track of rows where there is any NA: leaf_df$leaf_anyNA &lt;- apply(leaf_df[macroElements], 1, function(X) any(is.na(X))) # number of NAs (missing values): leaf_df$leaf_countNA &lt;- apply(leaf_df[macroElements], 1, function(X) sum(is.na(X))) The next cell performs the imputation. Since the imputation is a time-consuming process, we saved it in a csv stored in the subfolder output and have put a switch to put on if one wants to perform the computation again. The imputation is made by KNNs in the Aitchison (compositions-friendly) metric for rows where there are 1 or 2 missing values, i.e. filter(leaf_countNA &lt;= 3). # Warning: could be a long process perform_imputation &lt;- FALSE # set to FALSE if you want to load the saved file if (perform_imputation) { set.seed(628125) leaf_imp &lt;- leaf_df %&gt;% filter(leaf_countNA &lt;= 3) %&gt;% select(macroElements) %&gt;% impKNNa(as.matrix(.), metric = &quot;Aitchison&quot;, k = 6, primitive = TRUE, normknn = TRUE, adj = &#39;median&#39;) leaf_complete &lt;- leaf_df %&gt;% select(macroElements) leaf_complete[leaf_df$leaf_countNA &lt;= 3, ] &lt;- leaf_imp$xImp names(leaf_complete) &lt;- paste0(names(leaf_complete), &quot;_imp&quot;) write_csv(leaf_complete, &quot;output/leaf_complete.csv&quot;) } else { leaf_complete &lt;- read_csv(&quot;output/leaf_complete.csv&quot;) } ## Parsed with column specification: ## cols( ## N_imp = col_double(), ## P_imp = col_double(), ## K_imp = col_double(), ## Ca_imp = col_double(), ## Mg_imp = col_double() ## ) With the next cell, imputed columnsare appended to the data frame. The nutrients diagnosis will be done with imputed compositions. leaf_df &lt;- bind_cols(leaf_df, leaf_complete) leaf_df &lt;- leaf_df %&gt;% select(-c(&quot;leaf_allNA&quot;, &quot;leaf_anyNA&quot;)) Compositional data are data where the elements of the composition are non-negative and sum to unity. I compute Fv standing for filling value, an amalgamation of all other elements closing the simplex proportions to 100%. leaf_df &lt;- leaf_df %&gt;% mutate(sum_imp = rowSums(select(., paste0(macroElements, &quot;_imp&quot;))), Fv_imp = 100 - sum_imp) %&gt;% select(-sum_imp) if (!&quot;Fv&quot; %in% macroElements) macroElements &lt;- c(macroElements, &quot;Fv&quot;) The centered log-ratio (clr) transformed compositions will be used for discriminant analysis and perturbation vector concept assessment.The next cell performs this calculation. The clr coordinates are computed in an external (intermediate) data table. leaf_composition &lt;- leaf_df %&gt;% select(paste0(macroElements, &quot;_imp&quot;)) %&gt;% acomp(.) leaf_clr &lt;- clr(leaf_composition) %&gt;% unclass() %&gt;% as_tibble() names(leaf_clr) &lt;- paste0(&quot;clr_&quot;, macroElements) write_csv(leaf_clr, &quot;output/leaf_clr.csv&quot;) The next cell binds these clr-transformed compositions to the raw composition data frame and retains useful columns. This cell also discards all the samples with too many missing compositions. leaf_df &lt;- bind_cols(leaf_df, leaf_clr) leaf_df &lt;- leaf_df %&gt;% select(keys_col, location, cultivars, outputs, &quot;leaf_countNA&quot;, starts_with(&quot;clr&quot;)) %&gt;% filter(leaf_countNA &lt;= 3) 1.6 Cultivar classes correction From a preliminary checking, we noticed that cultivars Mystere and Vivaldi have different repported maturity classes in the data set, mid-season late and late for Mystere, then early mid-season and mid-season for Vivaldi respectively. Their new maturity classes names are based on a majority vote for this study. The next cell perform this correction. We also make missing values explicit for this categorical variable. leaf_df$Maturity5[leaf_df$Cultivar == &quot;Mystere&quot;] &lt;- &quot;late&quot; leaf_df$Maturity5[leaf_df$Cultivar == &quot;Vivaldi&quot;] &lt;- &quot;early mid-season&quot; leaf_df$Cultivar &lt;- forcats::fct_explicit_na(leaf_df$Cultivar) # makes missing values explicit. 1.7 Summarise and backup Finally, we summarized the processed data frame to record the years of begining and ending of experiments, the remaining number of experiments, cultivars and maturity classes. The definitive leaves data frame is stored as leaf_clust_df.csv in output subfolder as it is an intermediate file, for cluster analysis (Chapter 2). leaf_df %&gt;% summarise(start_year = min(Annee, na.rm = TRUE), end_year = max(Annee, na.rm = TRUE), numb_trials = n_distinct(NoEssai, na.rm = TRUE), numb_cultivars = n_distinct(Cultivar, na.rm = TRUE), numb_maturityClasses = n_distinct(Maturity5, na.rm = TRUE)) ## # A tibble: 1 x 5 ## start_year end_year numb_trials numb_cultivars numb_maturityClasses ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1970 2017 4714 59 5 write_csv(leaf_df, &#39;output/leaf_clust_df.csv&#39;) "],
["Chapter-Clustering.html", "Chapter 2 Ionome analysis 2.1 Objective 2.2 Useful libraries and custom functions 2.3 Leaves processed compositions data set 2.4 The yield cut-off, low and high yielders delimiter 2.5 Centered log-ratio (clr) centroids computation 2.6 Clustering potato cultivars with leaf ionome 2.7 Axis reduction 2.8 Do clrs affect potato tuber yield?", " Chapter 2 Ionome analysis 2.1 Objective This chapter has two objectives. Firstly, we try to assign cultivars to groups based on diagnostic leaves ionomes. Although plant health is a continuous domain rather than a categorical status, yield thresholds are useful for decision-making. Because yield potential varies widely among cultivars, we split experimental data into low- and high-productivity categories using a marketable yield delimiter at the 65^th percentile for each cultivar. Hence, we use high yielders subpopulation (samples which marketable yield is larger than the yield cut-off) to look for eventual patterns discriminating groups of similar multivariate compositions (ionomics groups). Then, a principle components analysis is performed. The experimental sites locations are mapped. At the end, the output data file is called dfml_df.csv i.e. the data frame for machine learning chapter (Chapter 3). 2.2 Useful libraries and custom functions A set of packages is needed for data manipulation and visualization like tidyverse presented in previous chapter (Chapter 1), mvoutlier for multivariate outliers detection, dbscan a density-based clustering algorithm, which can be used to identify clusters of any shape in data set containing noise and outliers, factoextra needed with dbscan for clustering and visualization, vegan to perform principle components analysis, ggmap makes it easy to retrieve raster map tiles from online mapping services like Google Maps and Stamen Maps and plots them using the ggplot2 framework, cowplot can combine multiple ggplot plots to make publication-ready plots, and extrafont allows custom fonts for graphs with ggplot2. library(&quot;tidyverse&quot;) library(&quot;mvoutlier&quot;) library(&quot;dbscan&quot;) library(&quot;factoextra&quot;) library(&quot;vegan&quot;) library(&quot;ggmap&quot;) library(&quot;cowplot&quot;) library(&quot;extrafont&quot;) 2.3 Leaves processed compositions data set For this chapter, the initial data set is the outcome of the previous chapter (Chapter 1) leaf_clust_df.csv. Let’s load the data frame. leaf_clust_df &lt;- read_csv(&quot;output/leaf_clust_df.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## AnalyseFoliaireStade = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character() ## ) ## See spec(...) for full column specifications. The experimental sites locations are mapped as follows. Figure 2.1: Location of experimental sites in the Québec potato data set. 2.4 The yield cut-off, low and high yielders delimiter For cluster analysis, we keep only high yielders filtered as yield 65% quantile cutter for each cultivar. The cutQ table is used to add the variable yieldClass categorising yield potential to leaf_clust_df. HY and LY stand for high yield and low yield respectively. cutQ &lt;- leaf_clust_df %&gt;% group_by(Cultivar) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, quantile, probs = 0.65, na.rm = TRUE) #%&gt;% ## Adding missing grouping variables: `Cultivar` colnames(cutQ)[colnames(cutQ) == &quot;RendVendable&quot;] &lt;- &quot;rv_cut&quot; leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(cutQ, by = &quot;Cultivar&quot;) %&gt;% mutate(yieldClass = forcats::fct_relevel(ifelse(RendVendable &gt;= rv_cut, &quot;HY&quot;, &quot;LY&quot;), &quot;LY&quot;)) For sake of verification, let’s compute average yield per yieldClass. mean_yield &lt;- leaf_clust_df %&gt;% group_by(yieldClass) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## Adding missing grouping variables: `yieldClass` mean_yield ## # A tibble: 3 x 2 ## yieldClass RendVendable ## &lt;fct&gt; &lt;dbl&gt; ## 1 LY 25.8 ## 2 HY 40.5 ## 3 &lt;NA&gt; NaN So, the average marketable yield is 40.48 Mg \\(ha^-1\\) for high yielders and 24.78 Mg \\(ha^-1\\) for low yielders. In comparison, average potato tuber yields in 2018 in Canada and in Québec were 31.21 Mg \\(ha^-1\\) and 28.75 Mg \\(ha^-1\\) respectively. 2.5 Centered log-ratio (clr) centroids computation Compositional data transformation is done in the loaded file. We select only clr-transformed variables for high yielders, at 10 % blossom (AnalyseFoliaireStade = 10% fleur) in the next chunk. hy_df &lt;- leaf_clust_df %&gt;% mutate(isNA = apply(select(., starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable), 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; yieldClass == &quot;HY&quot; &amp; NoEssai != &quot;2&quot;) %&gt;% select(NoEssai, NoBloc, NoTraitement, starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable) %&gt;% droplevels() 1334 lines of observations (samples) will be used to find patterns in potato cultivars. The next chunks check the number of samples per cultivar in this high yielders data set. Some cultivars have been discarded from the table after the previous filter. percentage &lt;- round(with(hy_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(hy_df, cbind(numHY = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numHY &lt;- as.numeric(as.character(distribution$numHY)) # numHY is the number of samples distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) distribution &lt;- distribution %&gt;% arrange(desc(numHY)) # arrange in descending order Figure 2.2: High yielders cultivars abundance. Some cultivars are well represented, like Superior and Goldrush. Let’s compute number of cultivars and trials in the data frame. data.frame(numb_cultivars = n_distinct(hy_df$Cultivar, na.rm = TRUE), numb_trials = n_distinct(hy_df$NoEssai, na.rm = TRUE)) ## numb_cultivars numb_trials ## 1 47 151 The next chunk creates a table with cultivars, maturity classes and median clr values i.e. clr centroids for cultivars (the S… Table of the article). hy_clr &lt;- hy_df %&gt;% group_by(Cultivar, Maturity5) %&gt;% select(Cultivar, Maturity5, starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) hy_clr ## # A tibble: 47 x 8 ## # Groups: Cultivar [47] ## Cultivar Maturity5 clr_N clr_P clr_K clr_Ca clr_Mg clr_Fv ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont early 0.701 -2.02 0.664 -1.11 -1.64 3.47 ## 2 AC Chaleur early 0.696 -1.97 0.349 -1.04 -1.54 3.51 ## 3 Amandine mid-season 0.519 -2.29 0.557 -0.696 -1.62 3.53 ## 4 Ambra mid-season 0.713 -1.98 0.275 -1.08 -1.39 3.46 ## 5 Andover early mid-season 0.872 -2.17 0.738 -0.808 -2.28 3.57 ## 6 Aquilon mid-season 0.783 -1.86 0.387 -0.901 -1.83 3.48 ## 7 Argos late 0.767 -1.77 0.570 -1.28 -1.70 3.43 ## 8 Atlantic mid-season 0.784 -1.87 0.155 -1.09 -1.50 3.56 ## 9 Bijou Rouge early 0.942 -1.93 0.563 -1.12 -1.93 3.61 ## 10 Carolina early 0.598 -2.19 0.131 -0.837 -1.34 3.63 ## # ... with 37 more rows write_csv(hy_clr, &quot;output/highyielders_medianclr.csv&quot;) Multivariate outliers detection technique is used to identify outliers with a quantitle critical value of qcrit = 0.975 by cultivar only if cultivars contain at leat 20 rows. If less than 20 rows, all rows are kept. The new data frame hy_df_in will be used for patterns recognition and discriminant analysis. hy_df_IO &lt;- hy_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% do({ if (nrow(.) &lt; 20) { IO = rep(1, nrow(.)) } else { IO = sign1(.[,-1], qcrit=0.975)$wfinal01 } cbind(.,IO) }) %&gt;% ungroup() hy_df_in &lt;- hy_df_IO %&gt;% filter(IO == 1) %&gt;% select(-IO) %&gt;% droplevels() 144 outliers have been discarded. 2.6 Clustering potato cultivars with leaf ionome Patterns recognition is done with dbscan algorithm which can identify dense regions measured by the number of objects close to a given point. As explained by the author, the key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points. We use the high yielders clr centroids of cultivars in a new data frame which is the same as hy_df_in without maturity classes. hy_centroids &lt;- hy_df_in %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) %&gt;% ungroup() ## Adding missing grouping variables: `Cultivar` Two important parameters are required for dbscan: epsilon (“eps”) and minimum points (“MinPts”). The parameter eps defines the radius of neighborhood around a point x. It’s called the \\(\\epsilon\\)-neighborhood of x. The parameter MinPts is the minimum number of neighbors within “eps” radius. The optimal value of “eps” parameter can be determined as follow: set.seed(5773) hy_centroids %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% as.matrix() %&gt;% kNNdistplot(., k = 5) abline(h = 0.3, lty = 2) Figure 2.3: The optimal value of “eps” parameter. The chunk below makes the prdiction of clusters delineated by the dbscan algorithm. Zeros are not a cluster or designates the cluster of outliers. res &lt;- hy_centroids %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% as.matrix() %&gt;% dbscan(., eps = .3, minPts = 5) predict(res) ## [1] 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 1 1 1 0 1 ## [36] 0 1 1 1 0 1 1 1 0 1 1 1 This result can also be visualized graphically: Figure 2.4: Cluster plot of poato cultivars based on centered log-ratio N P K Mg Ca transformed compositions of diagnostic leaves. Black points are outliers (zeros). As shown on the plot, one cluster means there is no detectable shape between cultivars ionomes as dots are scattered differently. It may not be useful to think of possible structures between potato cultivars based on ionome. Nethertheless, one could extract scores of the first two discriminant axes and loadings of clr variables to check for correlations and elements that best discriminate axes. 2.7 Axis reduction Leaves composition data are compositionnal data (non-negative and summing to unity) so are multivariate. It’s not possible to draw such a diagram on paper with more than two or eventually three dimensions, however, even though it is a perfectly valid mathematical construct Legendre et Legendre, 2012. For the purpose of analysis, we project the multidimensional scatter diagram onto bivariate graph. The axes of this graph are chosen to represent a large fraction of the variability of the multidimensional N-P-K-Mg-Ca-Fv data matrix, in a space with reduced i.e. lower dimensionality relative to the original data set. The next chunks perform a Principle Component Analysis (PCA) to check biplots, using vegan::rda() function. leaf.pca &lt;- rda(hy_df_in %&gt;% select(starts_with(&quot;clr&quot;))) The rda result object stores samples scores in the sites table and variables loadings in species data table. scores_df &lt;- data.frame(scores(leaf.pca, choices = c(1,2))$sites) loadings_df &lt;- data.frame(scores(leaf.pca, choices = c(1,2))$species) The biplot of PCA is presented in separate plots for easy reading. Figure 2.5: Grid plot of potato ionome principle component analysis. The first principle axis or component (PC1) is formed mainly by Mg and K while the second (PC2) is driven mainly by P and Ca. 2.8 Do clrs affect potato tuber yield? We will measure the clrs effect on tuber yield by measuring their importance in machine learning models using varImp() method of random forest algorithm, in the next chapter (Chapter 3). Let’s select useful columns in a new table named dfml.csv and filter only complete cases. dfml &lt;- leaf_clust_df %&gt;% select(NoEssai, NoBloc, NoTraitement, starts_with(&quot;clr&quot;), RendVendable, rv_cut, yieldClass, AnalyseFoliaireStade, Cultivar, Maturity5) %&gt;% mutate(isNA = apply(select(., starts_with(&quot;clr&quot;), Cultivar, Maturity5, RendVendable), 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% select(-c(AnalyseFoliaireStade, isNA, is10pcf)) %&gt;% droplevels() %&gt;% filter(complete.cases(.)) write_csv(dfml, &quot;output/dfml.csv&quot;) So, the Machine learning data table contains 3382 samples. Finally, let’s check cultivars abundance in the data frame. Cultivar Goldrush overcomes the others. pc &lt;- round(with(dfml, prop.table(table(Cultivar)) * 100), 2) dist &lt;- with(dfml, cbind(freq = table(Cultivar), percentage = pc)) dist &lt;- data.frame(cbind(dist, rownames(dist))) colnames(dist)[3] &lt;- &quot;Cultivar&quot; dist$freq &lt;- as.numeric(as.character(dist$freq)) Figure 2.6: Cultivars abundance in the machine learning data frame. "],
["Chapter-Modeling.html", "Chapter 3 Predicting tuber yield category 3.1 Objective 3.2 Useful libraries 3.3 Machine learning data set 3.4 Machine learning 3.5 Variable importance estimation 3.6 Make predictions on the test set with kknn model 3.7 Comparison with non-informative classification 3.8 Nutritionally balanced compositions", " Chapter 3 Predicting tuber yield category 3.1 Objective The objective of this chapter is to develop, evaluate and compare the performance of some machine learning algorithms (k-nearest neighbors, random forest and support vector machine - package caret) in predicting tuber yield categories using clr coordinates. We use the previous chapter (chapter 2) tidded data file dfml.csv which contains clr coordinates, maturity classes and the yield two categorical variable created using the 65^th percentile for each cultivar. We use accuracy as models quality meeasure. We run a Chi-square homogenity test to compare the best model (with highest accuracy) with a random classifier consisting of an equal distribution of 50% successful and 50% unsuccessful cases. We finally compute Euclidean distance as the measure of the multivariate distance between an observation and the closest true negative. By true negative or nutritionally balanced specimens, we mean the samples correctely predicted by the best model as high yielders in the training set. The training and testing sets are stored for the next chapter (preturbation concept - Chapter 4). 3.2 Useful libraries The tidyverse package is always needed for data easy manipulation and visualization, and then extrafont to make changes in graphs as demanded for the article. The particularly useful packages are caret and kknn needed for machine leraning functions. library(&quot;tidyverse&quot;) library(&#39;extrafont&#39;) library(&#39;caret&#39;) library(&#39;kknn&#39;) library(&quot;randomForest&quot;) 3.3 Machine learning data set Let’s load the dfml.csv data set. The clr coordinates are scaled to zero mean and unity variance. dfml = read_csv(&#39;output/dfml.csv&#39;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double(), ## RendVendable = col_double(), ## rv_cut = col_double(), ## yieldClass = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character() ## ) dfml$Maturity5 = factor(dfml$Maturity5) dfml$yieldClass = factor(dfml$yieldClass) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) dfml.sc &lt;- dfml # copy dfml.sc[, clr_no] &lt;- apply(dfml.sc[, clr_no], 2, scale) # scale predictors 3.4 Machine learning 3.4.1 Data train and test splits We randomly split the data into a training set (75% of the data) used to fit the models, and a testing set (remaining 25%) used for models’ evaluation. set.seed(8539) split_index &lt;- createDataPartition(dfml.sc$yieldClass, group = &quot;Cultivar&quot;, p = 0.75, list = FALSE, times = 1) train_df &lt;- dfml.sc[split_index, ] test_df &lt;- dfml.sc[-split_index, ] ml_col &lt;- c(clr_no, &quot;yieldClass&quot;) With the kknn package, we must specify three parameters: kmax which is the number of neighbors to consider, distance a distance parameter to specify (1 for the Mahattan distance and 2 for the Euclidean distance), and a kernel which is a function to measure the distance. A best method currently used to choose the right parameters consists in creating a parameter grid. (grid &lt;- expand.grid(kmax = c(7,9,12,15), distance = 1:2, kernel = c(&quot;rectangular&quot;, &quot;gaussian&quot;, &quot;optimal&quot;))) ## kmax distance kernel ## 1 7 1 rectangular ## 2 9 1 rectangular ## 3 12 1 rectangular ## 4 15 1 rectangular ## 5 7 2 rectangular ## 6 9 2 rectangular ## 7 12 2 rectangular ## 8 15 2 rectangular ## 9 7 1 gaussian ## 10 9 1 gaussian ## 11 12 1 gaussian ## 12 15 1 gaussian ## 13 7 2 gaussian ## 14 9 2 gaussian ## 15 12 2 gaussian ## 16 15 2 gaussian ## 17 7 1 optimal ## 18 9 1 optimal ## 19 12 1 optimal ## 20 15 1 optimal ## 21 7 2 optimal ## 22 9 2 optimal ## 23 12 2 optimal ## 24 15 2 optimal The metric of “Accuracy” is used to evaluate models quality. This is the ratio of the number of correctly predicted instances divided by the total number of instances in the dataset (e.g. 95% accurate). The accuracy of the models will be estimated using a 10-fold cross-validation (cv) scheme. This will split the data set into 10 subsets of equal size. The models are built 10 times, each time leaving out one of the subsets from training and use it as the test set. control &lt;- trainControl(method = &quot;cv&quot;, number = 10) metric &lt;- &quot;Accuracy&quot; 3.4.2 Building the Models We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable Jason Brownlee. # a) Non-linear algorithm ## kNN set.seed(7) kknn_ &lt;- train(yieldClass ~., data = train_df %&gt;% select(ml_col), method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) # b) Advanced algorithms ## SVM set.seed(7) svm_ &lt;- train(yieldClass ~., data = train_df %&gt;% select(ml_col), method = &quot;svmRadial&quot;, metric = metric, trControl = control) ## Random Forest set.seed(7) rf_ &lt;- train(yieldClass ~., data = train_df %&gt;% select(ml_col), method = &quot;rf&quot;, metric = metric, trControl = control) 3.4.3 Goodness of fit on training set We assess the accuracy metric also during modeling (with train set) but the target metric is for the evaluation set. This chart sorts models. # Summary results results &lt;- resamples(list(kknn_model = kknn_, svm_model = svm_, rf_model = rf_)) #summary(results) with dotplot() dotplot(results) Figure 3.1: Comparison of models accuracies at training. This chunk also sorts models in a descending order using accuracies only, in a table. models_acc &lt;- data.frame(Model = summary(results)$models, Accuracy = c(confusionMatrix(train_df$yieldClass, predict(kknn_))$overall[1], confusionMatrix(train_df$yieldClass, predict(svm_))$overall[1], confusionMatrix(train_df$yieldClass, predict(rf_))$overall[1])) models_acc[order(models_acc[,&quot;Accuracy&quot;], decreasing = TRUE), ] ## Model Accuracy ## 3 rf_model 0.9810800 ## 1 kknn_model 0.8557351 ## 2 svm_model 0.6748128 The next one prints the best tuning parameters that maximizes model accuracy. data.frame(Model = summary(results)$models, param = c(kknn_$bestTune, svm_$bestTune, rf_$bestTune)) ## Model param.kmax param.distance param.kernel param.sigma param.C ## 1 kknn_model 12 1 optimal 0.1840573 1 ## 2 svm_model 12 1 optimal 0.1840573 1 ## 3 rf_model 12 1 optimal 0.1840573 1 ## param.mtry ## 1 2 ## 2 2 ## 3 2 3.4.4 Models’ evaluation (test set) Model evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. The next chunk performs this computations and gives the sorted accuracy metrics. predicted_kknn_ &lt;- predict(kknn_, test_df %&gt;% select(ml_col)) predicted_svm_ &lt;- predict(svm_, test_df %&gt;% select(ml_col)) predicted_rf_ &lt;- predict(rf_, test_df %&gt;% select(ml_col)) #The best model tests_acc &lt;- data.frame(Model = summary(results)$models, Accuracy_on_test = c( confusionMatrix(test_df$yieldClass, predicted_kknn_)$overall[1], confusionMatrix(test_df$yieldClass, predicted_svm_)$overall[1], confusionMatrix(test_df$yieldClass, predicted_rf_)$overall[1])) tests_acc[order(tests_acc[,&quot;Accuracy_on_test&quot;], decreasing = TRUE), ] ## Model Accuracy_on_test ## 3 rf_model 0.7076923 ## 1 kknn_model 0.7065089 ## 2 svm_model 0.6402367 The k-nearest neighbours, the random forest and the support vector machine models returned similar predictive accuracies (although slightly higher for the former). 3.5 Variable importance estimation The varImp() method is then used to estimate the variable importance, which is printed (summarized) and plotted. varImp() ranks features by importance. importance &lt;- varImp(rf_, scale = FALSE) # scale between 1 to 100 print(importance) ## rf variable importance ## ## Overall ## clr_N 220.4 ## clr_Mg 201.3 ## clr_Fv 193.3 ## clr_K 190.8 ## clr_Ca 185.5 ## clr_P 184.4 #tiff(&#39;images/var-imp-plot.tiff&#39;) plot(importance, cex = 1.2, cex.lab = 2, cex.axis = 2, ylab = &quot;variable&quot;, col = &quot;black&quot;) Figure 3.2: Importance of clr variables (effect) in the model. #dev.off() 3.6 Make predictions on the test set with kknn model We sort the predictive quality metrics by cultivar with random forest algorithm: model rf_clr, and tide the table. test_df$ypred = predicted_kknn_ # adds predictions to test set cultivar_acc &lt;- test_df %&gt;% group_by(Cultivar) %&gt;% do(Accuracy = as.numeric(confusionMatrix(.$yieldClass, .$ypred)$overall[1]), numb_samples = as.numeric(nrow(.))) cultivar_acc$Accuracy &lt;- round(unlist(cultivar_acc$Accuracy), 2) cultivar_acc$numb_samples &lt;- unlist(cultivar_acc$numb_samples) data = data.frame(subset(cultivar_acc, Accuracy&gt;0)) data[order(data[,&quot;Accuracy&quot;], decreasing=T), ] ## Cultivar Accuracy numb_samples ## 4 Ambra 1.00 1 ## 10 Carolina 1.00 3 ## 13 Dark Red Chieftain 1.00 4 ## 19 Harmony 1.00 1 ## 25 Peribonka 1.00 1 ## 38 Viking 1.00 1 ## 22 Mystere 0.94 18 ## 2 AC Chaleur 0.86 7 ## 29 Reba 0.86 7 ## 5 Andover 0.80 10 ## 24 Norland 0.80 15 ## 28 Prospect 0.80 5 ## 33 Russet Burbank 0.80 5 ## 40 W 1386 0.80 5 ## 36 Snowden 0.79 43 ## 18 Goldrush 0.76 140 ## 11 Chieftain 0.75 57 ## 26 Pike 0.75 4 ## 37 Superior 0.75 105 ## 20 Kennebec 0.73 48 ## 16 FL 1533 0.72 39 ## 15 FL 1207 0.71 91 ## 6 Aquilon 0.67 15 ## 21 Lanorma 0.67 3 ## 34 Russet Norkota 0.67 3 ## 42 Yukon Gold 0.63 19 ## 8 Atlantic 0.62 42 ## 12 Coastal Russet 0.62 55 ## 7 Argos 0.60 5 ## 1 AC Belmont 0.58 12 ## 17 Frontier Russet 0.57 7 ## 35 Shepody 0.52 25 ## 3 Amandine 0.50 2 ## 9 Bijou Rouge 0.50 8 ## 30 Red Cloud 0.50 4 ## 27 Pommerelle 0.43 7 ## 39 Vivaldi 0.40 5 ## 23 Nordonna 0.33 3 ## 31 Red Maria 0.33 6 ## 32 Roko 0.33 3 ## 14 Estima 0.25 4 ## 41 Waneta 0.25 4 The predictive accuracy is very high for some cultivars, but this result must be taken with care due to small size of samples often available. We plot accuracies for cultivars using ggplot2 functions. Figure 3.3: Predictive accuracy for cultivars. 3.7 Comparison with non-informative classification The non-informative classification consists of an equal distribution of 50% successful and 50% unsuccessful classification cases (Swets J. A., 1988). We run the Chi-square homogenity test to compare predictive accuracy of the ..kknn.. model to this non-informative classification model. cm &lt;- confusionMatrix(predicted_kknn_, test_df$yieldClass) cm$table # confusion matrix ## Reference ## Prediction HY LY ## HY 192 107 ## LY 141 405 # rf_clr_and_ionomicgroup model&#39;s classification good_class &lt;- cm$table[1,1]+cm$table[2,2] # HY or LY and correctly predicted misclass &lt;- cm$table[1,2]+cm$table[2,1] # wrong classification ml_class &lt;- c(good_class, misclass) # Non-informative model (nim) total &lt;- sum(cm$table) # total number of samples good_nim &lt;- 0.50 * total misclass_nim &lt;- 0.50 * total non_inf_model &lt;- c(good_nim, misclass_nim) # Matrix for chisquare test m &lt;- rbind(ml_class, non_inf_model) m ## [,1] [,2] ## ml_class 597.0 248.0 ## non_inf_model 422.5 422.5 # chisq.test khi2_test &lt;- chisq.test(m) khi2_test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: m ## X-squared = 74.422, df = 1, p-value &lt; 2.2e-16 The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test, the too small p-value denotes important difference between the models. 3.8 Nutritionally balanced compositions We consider as True Negatives (TN) specimens for this study, observations of the training data set having a high yield (HY) and correctly predicted with the __k nearest neighbors__ model. Let’s save prediction for training set tr_pred &lt;- predict(kknn_)#, train_df %&gt;% select(ml_col)) train_df &lt;- bind_cols(dfml[split_index, ], pred_yield = tr_pred) TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; tr_pred == &#39;HY&#39;, ] Then, we compute clr centroids (means) for cultivars using True Negatives original (not scaled) clr values. These centroids (S3 Table of the article) could be used as provisional clr norms for cultivars. The standard deviations could also be computed. TNmeanNorms &lt;- TNs %&gt;% group_by(Cultivar) %&gt;% select(clr_no) %&gt;% summarise_all(list(mean)) ## Adding missing grouping variables: `Cultivar` TNmeanNorms ## # A tibble: 45 x 7 ## Cultivar clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont 0.703 -2.12 0.622 -1.62 -1.07 3.49 ## 2 AC Chaleur 0.704 -1.93 0.360 -1.55 -1.07 3.50 ## 3 Amandine 0.519 -2.29 0.557 -1.62 -0.696 3.53 ## 4 Ambra 0.713 -1.98 0.275 -1.39 -1.08 3.46 ## 5 Andover 0.904 -2.20 0.743 -2.19 -0.850 3.60 ## 6 Aquilon 0.810 -1.94 0.404 -1.83 -0.894 3.45 ## 7 Argos 0.819 -1.75 0.402 -1.70 -1.13 3.36 ## 8 Atlantic 0.775 -1.90 0.132 -1.50 -1.07 3.57 ## 9 Bijou Rouge 0.962 -2.06 0.598 -2.16 -1.01 3.68 ## 10 Carolina 0.595 -2.18 0.114 -1.32 -0.833 3.63 ## # ... with 35 more rows write_csv(TNmeanNorms, &quot;output/provisional_norms.csv&quot;) We also save prediction for testing set. te_pred &lt;- predict(kknn_, test_df %&gt;% select(ml_col)) test_df &lt;- bind_cols(dfml[-split_index, ], pred_yield = te_pred) And finally backup for next chapter. write_csv(train_df, &quot;output/train_df.csv&quot;) write_csv(test_df, &quot;output/test_df.csv&quot;) "],
["Chapter-Perturbation.html", "Chapter 4 Ionome perturbation concept 4.1 Objective 4.2 Data set and useful libraries 4.3 Euclidean distance from nutritionally balanced compositions 4.4 Perturbation effect of some elements on the whole 4.5 Rebalancing a misbalanced sample by perturbation", " Chapter 4 Ionome perturbation concept 4.1 Objective The objective of this chapter is to show the user a visual example of perturbation effect in a Simplex, and to develop a mathematical workflow useful to adjust the ionome of potato crops for diagnostic purpose. Perturbation in compositional space plays the same role as translation plays in real space. The assumption is that some natural processes in nature can be interpreted as a change from one composition C1 to another C2 through the application of a perturbation: p ⊕ C1 ===&gt; C2. The difference between a new observation and a closest healthy composition (closest true negative - TN) or reference composition can be back-transformed to the compositional space. The resulted vector is the perturbation vector. Theoretically, a misbalanced composition could be balanced (translated into a healthy zone) using a perturbation operation. Using this concept, ionome of a new cultivar could be assigned to the cultivar sharing similar leaf composition, and where nutrient requirements have been already documented by fertilizer trials. We used the testing set to display the effect of a perturbation on the whole simplex. We selected two elements (N and P) and simulated an increase of their initial (observed) clr values by 20% (theoretically). The observed (observation) and new clr vector (perturbation) were back transformed to N, P, K, Ca, Mg and Fv compositional space for comparison. Secondly, the procedure used to rebalance a misbalanced composition is decribed. As explained at the end of the Chapter 3, we consider as True Negatives (TN) specimens (or healthy points) for this study, observations of the training set having a high yield (HY) and correctly predicted by the k nearest neighbors model. 4.2 Data set and useful libraries We need package compositions for further clr back-transformation to compositional space. The package reshape will be used to melt an intermediate data frame. library(&quot;tidyverse&quot;) library(&quot;extrafont&quot;) library(&#39;compositions&#39;) library(&quot;reshape&quot;) The previous train_df and test_df are loaded. train_df = read_csv(&quot;output/train_df.csv&quot;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double(), ## RendVendable = col_double(), ## rv_cut = col_double(), ## yieldClass = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## pred_yield = col_character() ## ) test_df = read_csv(&quot;output/test_df.csv&quot;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double(), ## RendVendable = col_double(), ## rv_cut = col_double(), ## yieldClass = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## pred_yield = col_character() ## ) TNs = train_df %&gt;% filter(yieldClass == &#39;HY&#39; &amp; pred_yield == &#39;HY&#39;) clr_no = c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Ca&quot;, &quot;clr_Mg&quot;, &quot;clr_Fv&quot;) Filtrer train_df et test_df pour ne conserver que les observations ayant les cultivars correspondant dans les vrais négatifs, et seulement les déséquilibrés. train_df &lt;- train_df %&gt;% filter(Cultivar %in% unique(TNs$Cultivar)) test_df &lt;- test_df %&gt;% filter(Cultivar %in% unique(TNs$Cultivar)) 4.3 Euclidean distance from nutritionally balanced compositions The chunk below activates the custom function used to compute Euclidean distance. eucl_dist_f &lt;- function(x, y) { sqrt(sum((x-y)^2)) } For each imbalanced composition, we use the next loop to compute all the euclidean distances between all the compositions in “TNs” of the corresponding cultivar. The computation is possible even if the cultivar is unknown, the loop must just be updated. Here, the loop returns the smallest Euclidean distance stored in debal vector. For train_df: debal &lt;- c() debal_index &lt;- c() for (i in 1:nrow(train_df)) { clr_i &lt;- as.numeric(train_df[i, clr_no]) TNs_target &lt;- TNs %&gt;% filter(Cultivar == train_df$Cultivar[i]) %&gt;% select(clr_no) eucl_dist &lt;- apply(TNs_target, 1, function(x) eucl_dist_f(x = x, y = clr_i)) debal_index[i] &lt;- which.min(eucl_dist) debal[i] &lt;- eucl_dist[debal_index[i]] } train_df$debal &lt;- debal train_df &lt;- train_df %&gt;% filter(debal != 0) train_df %&gt;% glimpse() ## Observations: 1,724 ## Variables: 16 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4... ## $ NoBloc &lt;dbl&gt; 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 1, 2, 2, 2, 2, 3, 3... ## $ NoTraitement &lt;dbl&gt; 1, 2, 3, 6, 1, 2, 8, 4, 5, 7, 1, 1, 2, 3, 4, 2, 3... ## $ clr_N &lt;dbl&gt; 0.3321186, 0.4252302, 0.4453417, 0.4885647, 0.473... ## $ clr_P &lt;dbl&gt; -2.518325, -2.394957, -2.365429, -2.525335, -2.28... ## $ clr_K &lt;dbl&gt; 0.8379383, 1.1387291, 1.0112272, 0.9418880, 0.718... ## $ clr_Ca &lt;dbl&gt; -0.4794688, -0.5154924, -0.7290838, -0.6426036, -... ## $ clr_Mg &lt;dbl&gt; -1.639076, -2.346167, -1.990736, -1.763195, -1.69... ## $ clr_Fv &lt;dbl&gt; 3.466813, 3.692658, 3.628680, 3.500681, 3.337752,... ## $ RendVendable &lt;dbl&gt; 18.944200, 40.351800, 33.037900, 41.016700, 15.11... ## $ rv_cut &lt;dbl&gt; 41.33053, 41.33053, 41.33053, 41.33053, 41.33053,... ## $ yieldClass &lt;chr&gt; &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-se... ## $ pred_yield &lt;chr&gt; &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;... ## $ debal &lt;dbl&gt; 0.4859422, 0.3245021, 0.2126167, 0.2987338, 0.384... For test_df: debal &lt;- c() debal_index &lt;- c() for (i in 1:nrow(test_df)) { clr_i &lt;- as.numeric(test_df[i, clr_no]) TNs_target &lt;- TNs %&gt;% filter(Cultivar == test_df$Cultivar[i]) %&gt;% select(clr_no) eucl_dist &lt;- apply(TNs_target, 1, function(x) eucl_dist_f(x = x, y = clr_i)) debal_index[i] &lt;- which.min(eucl_dist) debal[i] &lt;- eucl_dist[debal_index[i]] } test_df$debal &lt;- debal test_df &lt;- test_df %&gt;% filter(debal != 0) test_df %&gt;% glimpse() ## Observations: 803 ## Variables: 16 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 5, 5, 5, 5... ## $ NoBloc &lt;dbl&gt; 1, 1, 2, 2, 2, 3, 3, 3, 1, 1, 1, 1, 3, 1, 1, 1, 2... ## $ NoTraitement &lt;dbl&gt; 4, 5, 5, 6, 7, 1, 2, 8, 2, 3, 5, 6, 1, 2, 4, 5, 5... ## $ clr_N &lt;dbl&gt; 0.3993264, 0.6519389, 0.5692523, 0.7251908, 0.507... ## $ clr_P &lt;dbl&gt; -2.599180, -2.468221, -2.423432, -2.327678, -2.50... ## $ clr_K &lt;dbl&gt; 0.7945826, 0.8242433, 0.7214698, 0.7597218, 0.676... ## $ clr_Ca &lt;dbl&gt; -0.6220171, -0.7470790, -0.6610863, -0.6945233, -... ## $ clr_Mg &lt;dbl&gt; -1.577529, -1.775074, -1.730285, -1.991205, -1.51... ## $ clr_Fv &lt;dbl&gt; 3.604817, 3.514191, 3.524081, 3.528494, 3.430364,... ## $ RendVendable &lt;dbl&gt; 37.55050, 46.00890, 42.95690, 49.24620, 41.86690,... ## $ rv_cut &lt;dbl&gt; 41.33053, 41.33053, 41.33053, 41.33053, 41.33053,... ## $ yieldClass &lt;chr&gt; &quot;LY&quot;, &quot;HY&quot;, &quot;HY&quot;, &quot;HY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-se... ## $ pred_yield &lt;chr&gt; &quot;HY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;... ## $ debal &lt;dbl&gt; 0.48988609, 0.16712719, 0.23425051, 0.21198871, 0... 4.4 Perturbation effect of some elements on the whole This subsection illustrates the principle that strictly positive data constrained to some whole are inherently related to each other. Changing a proportion (so, perturbation on some proportion(s)) inherently affects at least another proportion, because such data convey only relative information (Aitchison, 1982). leaf_clr_o stands for original clr values of the tidded test set test_df. # Compute (or select here) the clrs leaf_clr_o &lt;- test_df %&gt;% select(clr_no) #leaf_clr_o &lt;- test_df %&gt;% # filter(debal &gt;= quantile(train_df$debal, p = .75)) %&gt;% # select(clr_no) summary(leaf_clr_o) ## clr_N clr_P clr_K clr_Ca ## Min. :0.0000 Min. :-3.159 Min. :-0.3560 Min. :-1.98953 ## 1st Qu.:0.6253 1st Qu.:-2.171 1st Qu.: 0.2671 1st Qu.:-1.16304 ## Median :0.7282 Median :-2.005 Median : 0.4315 Median :-1.00005 ## Mean :0.7227 Mean :-2.004 Mean : 0.4567 Mean :-1.00711 ## 3rd Qu.:0.8349 3rd Qu.:-1.838 3rd Qu.: 0.6078 3rd Qu.:-0.84266 ## Max. :1.2106 Max. :-1.186 Max. : 1.8047 Max. : 0.06082 ## clr_Mg clr_Fv ## Min. :-2.7207 Min. :2.969 ## 1st Qu.:-1.9545 1st Qu.:3.472 ## Median :-1.7108 Median :3.547 ## Mean :-1.7294 Mean :3.561 ## 3rd Qu.:-1.5006 3rd Qu.:3.640 ## Max. :-0.6952 Max. :4.167 Let’s perturb the original clr values for N and P. # Perturb the original clrs pert_col &lt;- c(1, 2) # the column indices which is perturbed: clr_N and clr_K respectively perturbation &lt;- c(0.2, 0.2) # the amount added to the clr of the pert_col, same lenght as pert_col leaf_clr_f &lt;- leaf_clr_o for (i in seq_along(pert_col)) { leaf_clr_f[, pert_col[i]] &lt;- leaf_clr_f[, pert_col[i]] * (1 + perturbation[i]) } summary(leaf_clr_f) ## clr_N clr_P clr_K clr_Ca ## Min. :0.0000 Min. :-3.791 Min. :-0.3560 Min. :-1.98953 ## 1st Qu.:0.7503 1st Qu.:-2.605 1st Qu.: 0.2671 1st Qu.:-1.16304 ## Median :0.8739 Median :-2.406 Median : 0.4315 Median :-1.00005 ## Mean :0.8672 Mean :-2.404 Mean : 0.4567 Mean :-1.00711 ## 3rd Qu.:1.0019 3rd Qu.:-2.206 3rd Qu.: 0.6078 3rd Qu.:-0.84266 ## Max. :1.4528 Max. :-1.423 Max. : 1.8047 Max. : 0.06082 ## clr_Mg clr_Fv ## Min. :-2.7207 Min. :2.969 ## 1st Qu.:-1.9545 1st Qu.:3.472 ## Median :-1.7108 Median :3.547 ## Mean :-1.7294 Mean :3.561 ## 3rd Qu.:-1.5006 3rd Qu.:3.640 ## Max. :-0.6952 Max. :4.167 The next one transforms clrs (original and perturbed clrs) back to compositions. # From clrs to compositions leaf_o &lt;- apply(leaf_clr_o, 1, function(x) exp(x) / sum(exp(x))) %&gt;% t() leaf_f &lt;- apply(leaf_clr_f, 1, function(x) exp(x) / sum(exp(x))) %&gt;% t() Then, we plot the original and perturbed ionomes to check a general tendency. Observation column plots the original “N”, “P”, “K”, “Ca”, “Mg” and “Fv” compositions, Perturbation represents new compositions after perturbation and Difference column stands for perturbation occured in the Observation to yied new compositions. Data are tidded before. rshleaf_o &lt;- melt(data.frame(leaf_o)) %&gt;% mutate(vector = rep(&quot;Observation&quot;, nrow(.))) ## Using as id variables rshleaf_f &lt;- melt(data.frame(leaf_f)) %&gt;% mutate(vector = rep(&quot;Perturbation&quot;, nrow(.))) ## Using as id variables rshdf &lt;- bind_rows(rshleaf_o, rshleaf_f) rshdf$is_perturbed &lt;- ifelse(rshdf$variable %in% colnames(leaf_o[, pert_col]), &quot;Perturbed&quot;, &quot;Not perturbed&quot;) rshdf$variable &lt;- sub(pattern = &quot;clr_&quot;, replacement = &quot;&quot;, x = rshdf$variable, fixed = TRUE) %&gt;% fct_relevel(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;, &quot;Fv&quot;) Figure 4.1: Perturbation effect of some elements on the whole. All the components change when the clr of a single component is offset. The components whose clr has been perturbed obviously change the most (2-Perturbation). The component whose clr is the highest (generally Fv) compensate most of the perturbation. Although P clr values have been increased, P proportion decreased globally for the new equilibrium of the simplex. 4.5 Rebalancing a misbalanced sample by perturbation Let’s suppose that we got this point selected at random in imbalanced or misbalanced specimens. set.seed(92559) imbalanced &lt;- train_df[sample(nrow(train_df), 1), ] t(imbalanced) ## [,1] ## NoEssai &quot;412&quot; ## NoBloc &quot;1&quot; ## NoTraitement &quot;9&quot; ## clr_N &quot;0.7735603&quot; ## clr_P &quot;-2.464426&quot; ## clr_K &quot;0.5688289&quot; ## clr_Ca &quot;-1.034959&quot; ## clr_Mg &quot;-1.660053&quot; ## clr_Fv &quot;3.817049&quot; ## RendVendable &quot;34.74147&quot; ## rv_cut &quot;41.33053&quot; ## yieldClass &quot;LY&quot; ## Cultivar &quot;Goldrush&quot; ## Maturity5 &quot;mid-season&quot; ## pred_yield &quot;LY&quot; ## debal &quot;0.2208642&quot; Or even, we could rather use the most imbalanced occurrence, why not ! imbalanced &lt;- train_df[which.max(train_df$debal), ] misbalanced &lt;- imbalanced # copy t(misbalanced) ## [,1] ## NoEssai &quot;200&quot; ## NoBloc &quot;1&quot; ## NoTraitement &quot;2&quot; ## clr_N &quot;0.957698&quot; ## clr_P &quot;-1.83199&quot; ## clr_K &quot;-0.04637332&quot; ## clr_Ca &quot;0.4678508&quot; ## clr_Mg &quot;-2.247862&quot; ## clr_Fv &quot;2.700677&quot; ## RendVendable &quot;30.37194&quot; ## rv_cut &quot;32.6&quot; ## yieldClass &quot;LY&quot; ## Cultivar &quot;Superior&quot; ## Maturity5 &quot;early mid-season&quot; ## pred_yield &quot;LY&quot; ## debal &quot;1.345815&quot; How could we rebalance it? The first step is to find the closest balanced point in the TNs of the corresponding cultivar. Let’s re-compute its Euclidean distances from TNs and identify the TNs’ sample from which the distance is minimum. misbalanced &lt;- misbalanced[clr_no] eucl_dist_misbal &lt;- apply(TNs %&gt;% filter(Cultivar == imbalanced$Cultivar) %&gt;% select(clr_no), 1, function(x) eucl_dist_f(x = x, y = misbalanced)) index_misbal &lt;- which.min(t(data.frame(eucl_dist_misbal))) index_misbal # return the index of the sample ## [1] 50 The closest healthy sample is the one which index is 50 in TNs charing the same cultivar with the new sample. Using this index we could refind the minimum imbalance index value computed. (misbal &lt;- eucl_dist_misbal[index_misbal]) ## [1] 1.345815 The Euclidean distance matches with the corresponding debal value: imbalanced$debal[1] = 1.3458148. The closest point in the TNs subset is this one: target_TNs &lt;- TNs %&gt;% filter(Cultivar == imbalanced$Cultivar) closest &lt;- target_TNs[index_misbal, ] t(closest) ## [,1] ## NoEssai &quot;71&quot; ## NoBloc &quot;3&quot; ## NoTraitement &quot;5&quot; ## clr_N &quot;0.5576154&quot; ## clr_P &quot;-2.078376&quot; ## clr_K &quot;0.4710692&quot; ## clr_Ca &quot;-0.3762287&quot; ## clr_Mg &quot;-2.022806&quot; ## clr_Fv &quot;3.448726&quot; ## RendVendable &quot;37.56831&quot; ## rv_cut &quot;32.6&quot; ## yieldClass &quot;HY&quot; ## Cultivar &quot;Superior&quot; ## Maturity5 &quot;early mid-season&quot; ## pred_yield &quot;HY&quot; Note that Cultivar of the misbalanced and the closest healthy composition are the same. We compute the clr difference between the closest and the misbalanced points. closest = closest[clr_no] clr_diff = closest - misbalanced t(clr_diff) ## [,1] ## clr_N -0.4000826 ## clr_P -0.2463863 ## clr_K 0.5174425 ## clr_Ca -0.8440794 ## clr_Mg 0.2250562 ## clr_Fv 0.7480496 The perturbation vector is that clr difference back-transformed to leaf compositional space. comp_names &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;, &quot;Fv&quot;) perturbation_vector &lt;- clrInv(clr_diff) names(perturbation_vector) &lt;- comp_names t(perturbation_vector) ## [,1] ## N 0.09679140 ## P 0.11287200 ## K 0.24227737 ## Ca 0.06208853 ## Mg 0.18085523 ## Fv 0.30511547 ## attr(,&quot;class&quot;) ## [1] acomp Next, we should compute the corresponding compositions of the clr coordinates of the misbalanced point, as well as the closest TN point. The vectors could be gathered in a table made up of perturbation vector, misbalanced composition and the closest reference sample (pmc). misbal_comp &lt;- clrInv(misbalanced) names(misbal_comp) &lt;- comp_names closest_comp &lt;- clrInv(closest) names(closest_comp) &lt;- comp_names pmc = rbind(perturbation_vector, misbal_comp, closest_comp) rownames(pmc) = c(&quot;perturbation_vector&quot;,&quot;misbal_comp&quot;,&quot;closest_comp&quot;) pmc ## N P K Ca Mg ## perturbation_vector 0.0967914 0.112872002 0.2422774 0.06208853 0.1808552 ## misbal_comp 0.1282805 0.007881602 0.0470000 0.07860000 0.0052000 ## closest_comp 0.0488500 0.003500000 0.0448000 0.01920000 0.0037000 ## Fv ## perturbation_vector 0.3051155 ## misbal_comp 0.7330379 ## closest_comp 0.8799500 We could even check that the simplex is closed to 1 for each vector. sum(perturbation_vector); sum(misbal_comp); sum(closest_comp) ## [1] 1 ## [1] 1 ## [1] 1 The closest composition minus the misbalanced composition should return the perturbation vector. print(closest_comp - misbal_comp) ## N P K Ca Mg Fv ## [1,] 0.0967914 0.112872 0.2422774 0.06208853 0.1808552 0.3051155 ## attr(,&quot;class&quot;) ## [1] acomp print(perturbation_vector) # for comparison ## N P K Ca Mg Fv ## [1,] 0.0967914 0.112872 0.2422774 0.06208853 0.1808552 0.3051155 ## attr(,&quot;class&quot;) ## [1] acomp Or even, perturb the misbalanced point by the perturbation vector, you should obtain the closest TN point: print(misbal_comp + perturbation_vector) # perturbation ## N P K Ca Mg Fv ## [1,] 0.04885 0.0035 0.0448 0.0192 0.0037 0.87995 ## attr(,&quot;class&quot;) ## [1] acomp print(closest_comp) # for comparison ## N P K Ca Mg Fv ## [1,] 0.04885 0.0035 0.0448 0.0192 0.0037 0.87995 ## attr(,&quot;class&quot;) ## [1] acomp So, the assumption is true. The next codes show the concept using dots plots and histograms for each vector. A data frame is tidded for ggplot. Visualization is better with histograms. df &lt;- data.frame(rbind(misbalanced, closest, clr_diff), vectors = factor(c(&quot;Observation&quot;, &quot;Reference&quot;, &quot;Perturbation&quot;))) df$vectors &lt;- df$vectors %&gt;% fct_relevel(&quot;Observation&quot;, &quot;Reference&quot;, &quot;Perturbation&quot;) dfreshape &lt;- melt(df) ## Using vectors as id variables dfreshape$variable &lt;- sub(pattern = &quot;clr_&quot;, replacement = &quot;&quot;, x = dfreshape$variable, fixed = TRUE) %&gt;% fct_relevel(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;, &quot;Fv&quot;) ggplot(data = dfreshape, aes(x = value, y = vectors, colour = vectors)) + geom_point() + facet_wrap(~ variable, scales = &quot;free_x&quot;) + labs(x=&#39;Nutrient clr coordinate&#39;, y =&#39;&#39;) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) + theme_bw() Figure 4.2: Perturbation vector computation example dotplot using the most imbalanced foliar sample. ggsave(&quot;images/perturb_dotplot.tiff&quot;, width = 7, height = 3) g1 &lt;- ggplot(data = dfreshape, aes(x = variable, y = value, fill = vectors)) + geom_bar(aes(fill = vectors), stat = &quot;identity&quot;, position = position_dodge()) + coord_flip() + theme_bw() + ylab(&quot;Nutrients clr coordinates&quot;) + xlab(&quot;Diagnostic nutrients&quot;) + theme(legend.title = element_blank()) + theme(text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) g1 + scale_fill_discrete(breaks = c(&quot;Observation&quot;,&quot;Reference&quot;,&quot;Perturbation&quot;)) + scale_fill_manual(values=c(&quot;grey50&quot;, &quot;black&quot;, &quot;grey80&quot;)) Figure 4.3: Perturbation vector computation example barplot using the most imbalanced foliar sample. ggsave(&quot;images/perturb_barplot.tiff&quot;, width = 6, height = 4) "],
["references.html", "References", " References "]
]
