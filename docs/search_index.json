[
["index.html", "Nutrient Diagnosis Of Potato Chapter 1 Data processing 1.1 Ojective 1.2 Useful libraries for data handling 1.3 Québec potato data set 1.4 Selection of useful variables 1.5 Arranging the data frame 1.6 Cultivars classes correction. 1.7 Summarise and backup", " Nutrient Diagnosis Of Potato zcoulibali 2019-06-10 Chapter 1 Data processing 1.1 Ojective This chapter is the first of a series of R markdown codes aiming to describe computations methodology used to derive the results and conclusions of potato nutrient diagnosis article. The data set is a collection of potato surveys and nitrogen (N), phosphorous (P) and potassium (K) fertilizer trials conducted in Quebec from 1970 to 2017 between the US border at the 45^th parallel and the Northern limit of cultivation at the 49^th parallel. The useful variables are the first mature leaf (4^th from top, sampled at the beginning of blossom stage) nitrogen, phosphorus, potassium, calcium and magnesium compositions, cultivars used in experiments, cultivar maturity classes and tuber marketable yield. These variables are extracted from the Québec potato raw data table (raw_potato_df) and handled to obtain useful variables for cluster analysis and tuber yield class prediction. A previous exploration has shown that oligoelements contained too many missing values, for this reason these elements were excluded from analysis. The chapter ends with the backup of a processed data frame useful for next chapters. *** 1.2 Useful libraries for data handling I need package tidyverse which loads a set of packages for data handling and vizualisation. I used Amelia for missing data vizualisation, mice for missing data imputation and compositions to transforme compositions into compositionnal space. library(&quot;tidyverse&quot;) library(&#39;Amelia&#39;) library(&quot;mice&quot;) library(&quot;compositions&quot;) 1.3 Québec potato data set I load the Québec potato raw data set raw_potato_df.csv available for the project. raw_potato_df &lt;- read_csv(&quot;data/raw_potato_df.csv&quot;) 1.4 Selection of useful variables I create custom vectors of attributes which help select useful data columns for computations. The year of experiment is not needed instead it permits to know how long ago expériements have been monitored. Geographical coordinates will be needed to map experimental sites locations later. keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) longNameMacro &lt;- c(&quot;AnalyseFoliaireN&quot;,&quot;AnalyseFoliaireP&quot;,&quot;AnalyseFoliaireK&quot;, &quot;AnalyseFoliaireCa&quot;,&quot;AnalyseFoliaireMg&quot;) extra_col &lt;- c(&#39;Annee&#39;, &#39;LatDD&#39;, &#39;LonDD&#39;, &#39;AnalyseFoliaireStade&#39;) cultivarAndyield &lt;- c(&#39;Cultivar&#39;, &#39;Maturity5&#39;, &#39;RendVendable&#39;) useful_col &lt;- c(keys_col, extra_col, cultivarAndyield, longNameMacro) macroElements &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;) # for simplicity The reduced data frame becomes leaf_df which stands for the diagnostic leaves macroelements composition data frame combining corresponding cultivars data, marketable yield, year of experiment and sites geographical coordinates. leaf_df &lt;- raw_potato_df %&gt;% select(useful_col) colnames(leaf_df)[which(names(leaf_df) %in% longNameMacro)] &lt;- macroElements glimpse(leaf_df) ## Observations: 12,991 ## Variables: 15 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ NoBloc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## $ NoTraitement &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6,... ## $ Annee &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ LatDD &lt;dbl&gt; 46.75306, 46.75306, 46.75306, 46.75306, 4... ## $ LonDD &lt;dbl&gt; -72.33861, -72.33861, -72.33861, -72.3386... ## $ AnalyseFoliaireStade &lt;chr&gt; &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;1... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Gold... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;,... ## $ RendVendable &lt;dbl&gt; 18.9442, 40.3518, 33.0379, 37.5505, 46.00... ## $ N &lt;dbl&gt; 3.805, 3.356, 3.657, 3.610, 4.983, 4.277,... ## $ P &lt;dbl&gt; 0.22, 0.20, 0.22, 0.18, 0.22, 0.21, 0.28,... ## $ K &lt;dbl&gt; 6.31, 6.85, 6.44, 5.36, 5.92, 6.73, 5.75,... ## $ Ca &lt;dbl&gt; 1.69, 1.31, 1.13, 1.30, 1.23, 1.38, 1.17,... ## $ Mg &lt;dbl&gt; 0.53, 0.21, 0.32, 0.50, 0.44, 0.45, 0.51,... 1.5 Arranging the data frame I set trial number NoEssai as factor, relevel categorical maturity order variable and choose cultivar Superior as reference as it has the maximum number of observations in the data frame. percentage &lt;- round(with(leaf_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(leaf_df, cbind(numbOfsamples = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numbOfsamples &lt;- as.numeric(as.character(distribution$numbOfsamples)) distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) distribution %&gt;% arrange(desc(numbOfsamples)) %&gt;% head(5) ## numbOfsamples percentage Cultivar ## 1 3108 24.03 Superior ## 2 1544 11.94 Kennebec ## 3 1420 10.98 Goldrush ## 4 1145 8.85 Shepody ## 5 989 7.65 Chieftain leaf_df$NoEssai &lt;- as.factor(leaf_df$NoEssai) leaf_df$Cultivar &lt;- relevel(factor(leaf_df$Cultivar), ref = &quot;Superior&quot;) leaf_df$Maturity5 &lt;- ordered(leaf_df$Maturity5, levels = c(&quot;early&quot;,&quot;early mid-season&quot;, &quot;mid-season&quot;,&quot;mid-season late&quot;,&quot;late&quot;)) I portrait missing values for the sake of imputation. As explained in the 1.1 section, I will retain after this processing only reasonably imputable data i.e., missing data of a sample less than half the number of studied attributes. The next cell maps the portrait of missing values. leafIonome &lt;- leaf_df[macroElements] missmap(leafIonome) Figure 1.1: Portrait of missing macroelements. This figure compiles the samples identifiers on the Y axis and macroelements on the X axis. A complete horizontal unique color band indicates wether the 5 elements are totally observed (blue band) or totally missing (red band). Only N and P have missing values that will be imputed and retained at the end. The totally missing compositions will be removed. The next cell initializes this process. # keep track of empty rows: leaf_df$leaf_allNA &lt;- apply(leaf_df[macroElements], 1, function(X) all(is.na(X))) # keep track of rows where there is any NA: leaf_df$leaf_anyNA &lt;- apply(leaf_df[macroElements], 1, function(X) any(is.na(X))) # number of NAs (missing values): leaf_df$leaf_countNA &lt;- apply(leaf_df[macroElements], 1, function(X) sum(is.na(X))) The next cell performs three times imputation and compiles the mean (average) for each sample and for each element in a new table stored in a subfolder output. At this step imputation is made for all the missing values of the complete table. Too many missing data rows will be discarded later. # Warning: could be a long process perform_imputation &lt;- TRUE if (perform_imputation) { leaf_mice &lt;- leaf_df %&gt;% select(macroElements) %&gt;% mice(data = ., m = 3, method = &quot;rf&quot;) leaf_complete &lt;- Reduce(&quot;+&quot;, complete(leaf_mice, action = &quot;all&quot;))/ leaf_mice$m names(leaf_complete) &lt;- paste0(names(leaf_complete), &quot;_imp&quot;) write_csv(leaf_complete, &quot;output/leaf_complete.csv&quot;) } else { leaf_complete &lt;- read_csv(&quot;output/leaf_complete.csv&quot;) } ## ## iter imp variable ## 1 1 N P K Ca Mg ## 1 2 N P K Ca Mg ## 1 3 N P K Ca Mg ## 2 1 N P K Ca Mg ## 2 2 N P K Ca Mg ## 2 3 N P K Ca Mg ## 3 1 N P K Ca Mg ## 3 2 N P K Ca Mg ## 3 3 N P K Ca Mg ## 4 1 N P K Ca Mg ## 4 2 N P K Ca Mg ## 4 3 N P K Ca Mg ## 5 1 N P K Ca Mg ## 5 2 N P K Ca Mg ## 5 3 N P K Ca Mg With the next cell I append imputed columns to the data frame. The nutrients diagnosis will be done with imputed compositions. leaf_df &lt;- bind_cols(leaf_df, leaf_complete) leaf_df &lt;- leaf_df %&gt;% select(-c(&quot;leaf_allNA&quot;, &quot;leaf_anyNA&quot;)) Compositional data are data where the elements of the composition are non-negative and sum to unity. I compute Fv standing for filling value, an amalgamation of all other elements closing the simplex to 100%. leaf_df &lt;- leaf_df %&gt;% mutate(sum_imp = rowSums(select(., paste0(macroElements, &quot;_imp&quot;))), Fv_imp = 100 - sum_imp) %&gt;% select(-sum_imp) if (!&quot;Fv&quot; %in% macroElements) macroElements &lt;- c(macroElements, &quot;Fv&quot;) I use centered log-ratio (clr) transformation for discriminant analysis and perturbation vector concept assessment.The next cell perform this calculation. The clr coordinates are computed in an external (intermediate) data table. leaf_composition &lt;- leaf_df %&gt;% select(paste0(macroElements, &quot;_imp&quot;)) %&gt;% acomp(.) leaf_clr &lt;- clr(leaf_composition) %&gt;% unclass() %&gt;% as_tibble() names(leaf_clr) &lt;- paste0(&quot;clr_&quot;, macroElements) write_csv(leaf_clr, &quot;output/leaf_clr.csv&quot;) The next cell binds these clr-transformed compositions to the raw composition data frame and retains useful columns. I also used this cell to discard all the samples with too many missing composition. leaf_df &lt;- bind_cols(leaf_df, leaf_clr) leaf_df &lt;- leaf_df %&gt;% select(keys_col, extra_col, cultivarAndyield, &quot;leaf_countNA&quot;, starts_with(&quot;clr&quot;)) %&gt;% filter(leaf_countNA &lt;= 3) 1.6 Cultivars classes correction. From a previous checking, I noticed that cultivars Mystere and Vivaldi have different repported maturity classes in the data set, mid-season late and late for Mystere, then early mid-season and mid-season for Vivaldi respectively. Their new maturity classes names are based on a majority vote for this study. The next cell perform this correction. I also make missing values explicit for this categorical variable. leaf_df$Maturity5[leaf_df$Cultivar == &quot;Mystere&quot;] &lt;- &quot;late&quot; leaf_df$Maturity5[leaf_df$Cultivar == &quot;Vivaldi&quot;] &lt;- &quot;early mid-season&quot; leaf_df$Cultivar &lt;- forcats::fct_explicit_na(leaf_df$Cultivar) # makes missing values explicit. 1.7 Summarise and backup Finally, I summarized the processed data frame to record the years of begining and ending of experiment, the remaining number of experiments, cultivars and maturity classes. The definitive leaves data frame is stored as leaf_clust_df.csv in output subfolder as it is an intermediate file, for cluster analysis (Chapter 2). leaf_df %&gt;% summarise(start_year = min(Annee, na.rm = TRUE), end_year = max(Annee, na.rm = TRUE), numb_trials = n_distinct(NoEssai, na.rm = TRUE), numb_cultivars = n_distinct(Cultivar, na.rm = TRUE), numb_maturityClasses = n_distinct(Maturity5, na.rm = TRUE) ) ## # A tibble: 1 x 5 ## start_year end_year numb_trials numb_cultivars numb_maturityClasses ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1970 2017 4714 59 5 write_csv(leaf_df, &#39;output/leaf_clust_df.csv&#39;) "],
["Chapter-Clustering.html", "Chapter 2 Cluster analysis of potato cultivars 2.1 Objective 2.2 Useful libraries and custom functions 2.3 Leaves processed compositions data set 2.4 High yielders delimiter 2.5 clr centroids computation 2.6 Axis reduction 2.7 Cascade K Means clustering 2.8 Arranging data for Machine Learning 2.9 clr X ionomics groups interactions effects", " Chapter 2 Cluster analysis of potato cultivars 2.1 Objective Although plant health is a continuous domain rather than a categorical status, yield thresholds are useful for decision-making. Because yield potential varies widely among cultivars, I use this chapter to split experimental data into low- and high-productivity categories using a marketable yield delimiter at the 65th percentile for each cultivar. Hence, I use high yielders subpopulation (samples which marketable yield is larger than the yield cut-off) to perform a cascade k-means clustering aiming to discriminate Groups of similar multivariate compositions (ionomics groups). I combine a discriminant analysis to check the macroelements that best discriminate cultivars. I also check the performence of Groups discrimination by computing the Groups X clr-coordinates interaction effect coefficients in a linear mixed effect model. I finally append the new variable ionomicGroupto the leaves clr data frame and discard all the rows with any missing data. I map the experimental sites locations in the third sudsection of this chapter. The output data file is called leaf_ml_df.csv i.e., the data frame for machine learning chapter 3. *** 2.2 Useful libraries and custom functions We need a set of packages for data handling and visualization like the tidyverse package presented in previous chapter 1, ellipse offers functions for drawing ellipses and ellipse-Like confidence regions (maybe I will set this option to FLASE for graph simplicity), mvoutlier for multivariate outliers detection, ade4 gathers tools for multivariate data analysis (I use it for discriminant analysis), vegan performs cluster analysis by k-means or cascade k-means clustering, ggmap makes it easy to retrieve raster map tiles from online mapping services like Google Maps and Stamen Maps and plot them using the ggplot2 framework, extrafont for custom fonts for graphs with ggplot2, ggrepel provides text and label geoms for ‘ggplot2’ that help to avoid overlapping text labels, plotly …, nlme for linear and non-linear mixed effect modeling. Some custom functions are also loaded mainly for Pseudo r-square caculation with linear mixed effect model and discriminant biplot. library(&quot;tidyverse&quot;) library(&#39;ellipse&#39;) library(&quot;mvoutlier&quot;) library(&quot;ade4&quot;) library(&quot;vegan&quot;) library(&quot;extrafont&quot;) library(&quot;ggrepel&quot;) library(&quot;ggmap&quot;) #library(&quot;plotly&quot;) library(&quot;nlme&quot;) source(&quot;data/functions.R&quot;) source(&#39;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_trad.R&#39;) 2.3 Leaves processed compositions data set For this chapter, the initial data set is the outcome of the prvious chapter (1) leaf_clust_df. I load the data frame and create vectors of columns I will use in this chapter. leaf_clust_df &lt;- read_csv(&quot;output/leaf_clust_df.csv&quot;) ## Parsed with column specification: ## cols( ## NoEssai = col_double(), ## NoBloc = col_double(), ## NoTraitement = col_double(), ## Annee = col_double(), ## LatDD = col_double(), ## LonDD = col_double(), ## AnalyseFoliaireStade = col_character(), ## Cultivar = col_character(), ## Maturity5 = col_character(), ## RendVendable = col_double(), ## leaf_countNA = col_double(), ## clr_N = col_double(), ## clr_P = col_double(), ## clr_K = col_double(), ## clr_Ca = col_double(), ## clr_Mg = col_double(), ## clr_Fv = col_double() ## ) keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) cultivarAndyield &lt;- c(&#39;Cultivar&#39;, &#39;Maturity5&#39;, &#39;RendVendable&#39;) extra_col &lt;- c(&#39;LatDD&#39;, &#39;LonDD&#39;, &#39;AnalyseFoliaireStade&#39;) The next cell maps experimental sites locations. #library(&quot;ggmap&quot;) qc_leaf &lt;- get_stamenmap(bbox = c(left=-76, right=-68, bottom=45, top=50), zoom=7, maptype = &#39;toner-lite&#39;) ggmap(qc_leaf) + geom_point(data = leaf_clust_df %&gt;% select(LonDD, LatDD) %&gt;% unique(), aes(x = LonDD, y = LatDD), size = 2, shape = 1) + coord_map(&quot;mercator&quot;) + theme_bw() + theme(text = element_text(family = &quot;Arial&quot;, face = &quot;bold&quot;, size = 12)) (#fig:leaf_df-sites-locations)Location of experimental sites (green dots) in the Québec potato data set. #ggsave(&quot;images/leaf_clust_df-sites-locations.png&quot;, width=10, height=8) 2.4 High yielders delimiter For cluster analysis, I keep only high yielders i.e. yield 65% quantile cutter for each cultivar. The cutQ table contains the yield delimiter for each cultivar. Then, this table is used to add the variable yieldClass to leaf_clust_df. HY and LY stand for high yield and low yield respectively. cutQ &lt;- leaf_clust_df %&gt;% group_by(Cultivar) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, quantile, probs=0.65, na.rm = TRUE) %&gt;% rename(rv_cut = RendVendable) ## Adding missing grouping variables: `Cultivar` leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(cutQ, by = &quot;Cultivar&quot;) %&gt;% mutate(yieldClass = forcats::fct_relevel(ifelse(RendVendable &gt;= rv_cut, &quot;HY&quot;, &quot;LY&quot;), &quot;LY&quot;)) glimpse(leaf_clust_df) ## Observations: 8,131 ## Variables: 19 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ NoBloc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## $ NoTraitement &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6,... ## $ Annee &lt;dbl&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ LatDD &lt;dbl&gt; 46.75306, 46.75306, 46.75306, 46.75306, 4... ## $ LonDD &lt;dbl&gt; -72.33861, -72.33861, -72.33861, -72.3386... ## $ AnalyseFoliaireStade &lt;chr&gt; &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;1... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Gold... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;,... ## $ RendVendable &lt;dbl&gt; 18.9442, 40.3518, 33.0379, 37.5505, 46.00... ## $ leaf_countNA &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,... ## $ clr_N &lt;dbl&gt; 0.3321186, 0.4252302, 0.4453417, 0.399326... ## $ clr_P &lt;dbl&gt; -2.518325, -2.394957, -2.365429, -2.59918... ## $ clr_K &lt;dbl&gt; 0.83793831, 1.13872910, 1.01122715, 0.794... ## $ clr_Ca &lt;dbl&gt; -0.4794688, -0.5154924, -0.7290838, -0.62... ## $ clr_Mg &lt;dbl&gt; -1.639076, -2.346167, -1.990736, -1.57752... ## $ clr_Fv &lt;dbl&gt; 3.466813, 3.692658, 3.628680, 3.604817, 3... ## $ rv_cut &lt;dbl&gt; 41.33053, 41.33053, 41.33053, 41.33053, 4... ## $ yieldClass &lt;fct&gt; LY, LY, LY, LY, HY, LY, HY, HY, LY, LY, H... For sake of verification, I compute average yield per yieldClass. meanYield = leaf_clust_df %&gt;% group_by(yieldClass) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## Adding missing grouping variables: `yieldClass` meanYield ## # A tibble: 3 x 2 ## yieldClass RendVendable ## &lt;fct&gt; &lt;dbl&gt; ## 1 LY 25.8 ## 2 HY 40.5 ## 3 &lt;NA&gt; NaN Average marketable yield is 40.48 Mg ha-1 for high yielders and 24.78 Mg ha^-1 for low yielders. In comparison, average potato tuber yields in 2017 in Canada and in Québec were respectively 40 Mg ha^-1 and 38.4 Mg ha^-1. 2.5 clr centroids computation Compositional data transformation is done in the loaded file. I keep only clr-transformed coordinates for high yielders, at 10 % blossom (AnalyseFoliaireStade = 10% fleur) in the next chunk. highYielders_df &lt;- leaf_clust_df %&gt;% mutate(isNA = apply(.[c(clr_no, cultivarAndyield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; yieldClass == &quot;HY&quot; &amp; NoEssai != &quot;2&quot;) %&gt;% select(one_of(keys_col, clr_no, cultivarAndyield)) %&gt;% droplevels() nrow(highYielders_df) ## [1] 1334 So, 1334 lines of observations (samples) will be used for potato cultivars clustering. I Check how many rows of data are they for each cultivar. percentage &lt;- round(with(highYielders_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(highYielders_df, cbind(numHY = table(Cultivar), percentage = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$numHY &lt;- as.numeric(as.character(distribution$numHY)) # numHY = number of samples distribution$percentage &lt;- as.numeric(as.character(distribution$percentage)) distribution %&gt;% arrange(desc(numHY)) %&gt;% head(5) # arrange in descending order ## numHY percentage Cultivar ## 1 199 14.92 Superior ## 2 197 14.77 Goldrush ## 3 127 9.52 FL 1207 ## 4 119 8.92 Chieftain ## 5 88 6.60 Atlantic Some cultivars are well represented, like Superior and Goldrush. Let’s compute number of cultivars and trials in the data frame. data.frame(numb_cultivars = n_distinct(highYielders_df$Cultivar, na.rm = TRUE), numb_trials = n_distinct(highYielders_df$NoEssai, na.rm = TRUE)) ## numb_cultivars numb_trials ## 1 47 151 I create a table with cultivars, maturity classes and computed median clr values i.e., clr centroids. highYielders_clr &lt;- highYielders_df %&gt;% group_by(Cultivar, Maturity5) %&gt;% select(Cultivar, Maturity5, starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) highYielders_clr ## # A tibble: 47 x 8 ## # Groups: Cultivar [47] ## Cultivar Maturity5 clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont early 0.701 -2.02 0.664 -1.64 -1.11 3.47 ## 2 AC Chaleur early 0.696 -1.97 0.349 -1.54 -1.04 3.51 ## 3 Amandine mid-season 0.519 -2.29 0.557 -1.62 -0.696 3.53 ## 4 Ambra mid-season 0.713 -1.98 0.275 -1.39 -1.08 3.46 ## 5 Andover early mid-season 0.872 -2.17 0.738 -2.28 -0.808 3.57 ## 6 Aquilon mid-season 0.783 -1.86 0.387 -1.83 -0.901 3.48 ## 7 Argos late 0.767 -1.77 0.570 -1.70 -1.28 3.43 ## 8 Atlantic mid-season 0.784 -1.87 0.155 -1.50 -1.09 3.56 ## 9 Bijou Rouge early 0.942 -1.93 0.563 -1.93 -1.12 3.61 ## 10 Carolina early 0.598 -2.19 0.131 -1.34 -0.837 3.63 ## # ... with 37 more rows I use multivariate outliers detection technic to identify outliers with a criterion of detection limit of 0.975 by cultivar only if cultivars contain at leat 20 rows. If less than 20 rows, all rows are kept. The new data frame will be used for discriminant analysis. I call it lda_df. highYielders_df_IO &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% do({ if (nrow(.) &lt; 20) { IO = rep(1, nrow(.)) } else { IO = sign1(.[,-1], qcrit=0.975)$wfinal01 } cbind(.,IO) }) lda_df &lt;- highYielders_df_IO %&gt;% filter(IO == 1) %&gt;% droplevels() nrow(lda_df) ## [1] 1196 Hence, \\r nrow(highYielders_df)-nrow(lda_df) outliers have been discarded from the analysis. The remaining data frame contains \\r n_distinct(lda_df$Cultivar, na.rm = TRUE) cultivars from \\r n_distinct(lda_df$NoEssai, na.rm = TRUE) trials. 2.6 Axis reduction Leaves composition data are compositionnal data (non-negative and summing to unity) so are multivariate. As argued by Legendre et Legendre, 2012 It is not possible to draw such a diagram on paper with more than two or eventually three dimensions, however, even though it is a perfectly valid mathematical construct. For the purpose of analysis, I will project the multidimensional scatter diagram onto bivariate graphs whose axes are known to be of particular interest. The axes of these graphs are chosen to represent a large fraction of the variability of the multidimensional N-P-K-Mg-Ca-Fv data matrix, in a space with reduced (i.e. lower) dimensionality relative to the original data set. With the next chunks, I perform a duality diagram (or dudi) PCA (priciple component analysis) and store scores and loadings. I also filter the data frame so that cultivars with less than 5 occurences will be masked on the resulting biplot. pca_leaf &lt;- dudi.pca(lda_df[clr_no], scannf = FALSE, scale = FALSE) lda_leaf &lt;- discrimin(dudi = pca_leaf, fac = factor(lda_df$Cultivar), scannf = FALSE) lda_leaf_score = lda_leaf$li lda_leaf_loading = lda_leaf$fa lda_df$Cultivar &lt;- factor(lda_df$Cultivar) lda_leaf_group &lt;- lda_df$Cultivar n_cultivar &lt;- table(lda_df$Cultivar) # Do not schow Cultivars whose number of occurrences is &lt; 5 n_filter &lt;- lda_leaf_group %in% names(n_cultivar[n_cultivar &gt;= 5]) filter_cultivars &lt;- names(n_cultivar[n_cultivar &gt;= 5]) lda_df_filter &lt;- lda_df[n_filter, ] I can plot the tThe distance biplot as result of discriminant analysis. I use a custom function. options(repr.plot.width = 6, repr.plot.height = 6) plot_lda(score = lda_leaf_score[n_filter, ], loading = lda_leaf_loading[n_filter, ], group = lda_leaf_group[n_filter], ell_dev = FALSE, ell_err = FALSE, #TRUE, scale_load = 0.5, level = 0.95, legend = FALSE, label = TRUE, transparency = 0.3, xlim = c(-2, 2), ylim = c(-3.5, 3), points = FALSE) Figure 2.1: Discriminant distance biplot of potato cultivars. The first discriminant axis (DS1) is formed mainly by Mg, K and N. The second discriminant axis (DS2) is driven mainly by the Fv, K and weakely by Ca. I perform cluster analysis in next subsection. 2.7 Cascade K Means clustering What is cascade K Means Clustering? It is an an unsupervised learning algorithm inspired by its similar K Means clustering both trying to cluster data based on their similarity. There is no outcome to be predicted. The algorithm just tries to find patterns in the data. In k means clustering, we have to specify the number of clusters we want the data to be grouped into. In cascade K means method, we give a minimum and a maximum number of cluster wanted. The algorithm randomly assigns each observation to a cluster, and finds the centroid of each cluster. Then, the algorithm iterates through two steps: reassign data points to the cluster whose centroid is closest, Calculate new centroid of each cluster. These two steps are repeated till the within cluster variation cannot be reduced any further. The within cluster variation is calculated as the sum of the euclidean distance between the data points and their respective cluster centroids. I use the high yielders clr centroids (medians) for cultivars in a next data frame which is the same as highYielders_clr without maturity classes, and Calinski-Harabasz (1974) criterion (package vegan) for clustering.The chunk also plots the process results. highYieldersCentroids &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) set.seed(5773) highYieldersKmeans &lt;- cascadeKM(highYieldersCentroids[, -1], inf.gr = 3, sup.gr = 8, criterion = &quot;ssi&quot;) options(repr.plot.width = 6, repr.plot.height = 4) plot(highYieldersKmeans) Figure 2.2: K-means partitions comparison (calinski criterion). The red dot of the right hand side graph shows 3 optimal clustering partitions which is the inferior limit I gave to the process. I check the partitions data frame. highYieldersKmeans$partition %&gt;% head() ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups ## 1 1 3 1 1 4 3 ## 2 3 2 3 4 6 4 ## 3 2 1 5 5 7 5 ## 4 3 2 3 4 6 8 ## 5 1 4 4 6 5 2 ## 6 1 4 1 1 4 3 I consider 6 groups established from clustering (...) corresponding to the second column. I can use the reference of the column or its name 6 groups directly. I add it up to the centroids data frame. highYieldersCentroids$kgroup &lt;- highYieldersKmeans$partition[, &quot;6 groups&quot;] I compute discriminant scores centroïdes for cultivars. lda_centroids &lt;- lda_leaf_score %&gt;% mutate(group = lda_leaf_group) %&gt;% group_by(group) %&gt;% summarise_all(list(mean)) Plot Cultivar groups in LDA with a custom function. options(repr.plot.width = 6, repr.plot.height = 6) plot_lda(score=lda_leaf_score[n_filter, ], loading=lda_leaf_loading[n_filter, ], group = lda_leaf_group[n_filter], ell_dev=FALSE, ell_err= FALSE, #TRUE, scale_load = 0.4, level=0.95, legend=FALSE, label=TRUE, transparency=0.4, xlim = c(-2.5, 2), ylim = c(-3, 4), points=F) # colour dots for groups (or clusters) col = factor(highYieldersCentroids[highYieldersCentroids$Cultivar %in% filter_cultivars, &#39;kgroup&#39;][[1]]) # remove the cultivar column points(lda_centroids[lda_centroids$group %in% filter_cultivars, c(&#39;DS1&#39;, &#39;DS2&#39;)], pch = 19, col = col, cex = 0.9) legend(1.3, 4, legend = paste(rep(&#39;cluster&#39;, nlevels(col)), as.numeric(levels(col))), pch = 19, col = unique(col), cex = 0.9) Figure 2.3: Discriminant distance biplot of potato cultivars showing ionomics groups. It’s a bit difficult to colour cultivar name in the plot with the custom function. I use functions from packages ggplot2, ggrepel and plotly instead. New intermediate data frames are created for this purpose. cultivars_filtre &lt;- data.frame(Cultivar = filter_cultivars, i_group=col) df &lt;- data.frame( score = lda_leaf_score[n_filter, ], loadings = lda_leaf_loading[n_filter,], Cultivar = lda_leaf_group[n_filter] ) df &lt;- df %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) ## Warning: Column `Cultivar` joining factors with different levels, coercing ## to character vector df %&gt;% head() ## score.DS1 score.DS2 loadings.DS1 loadings.DS2 Cultivar i_group ## 1 1.0429585 -1.0222524 2.0083978 0.1877953 AC Belmont 1 ## 2 0.6075317 -1.5138835 -0.1476953 -0.7188404 AC Belmont 1 ## 3 0.7296306 -1.3385042 2.0326780 -2.3681983 AC Belmont 1 ## 4 0.5341272 -0.7657073 -2.3129315 -1.3651320 AC Belmont 1 ## 5 0.4269388 -1.1073114 0.3006409 -1.3922328 AC Belmont 1 ## 6 0.3528173 -1.1509198 -1.8810900 5.6566082 AC Belmont 1 centroids = lda_centroids[lda_centroids$group %in% filter_cultivars, ] names(centroids)[match(&quot;group&quot;, names(centroids))] &lt;- &quot;Cultivar&quot; centroids &lt;- centroids %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) centroids %&gt;% head() ## # A tibble: 6 x 4 ## Cultivar DS1 DS2 i_group ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 AC Belmont 0.130 -1.01 1 ## 2 AC Chaleur -0.540 -0.115 4 ## 3 Andover 1.93 0.0467 6 ## 4 Aquilon 0.249 -0.245 1 ## 5 Argos 0.171 -0.822 2 ## 6 Atlantic -1.23 0.405 4 The chunk below plots discriminant biplot with colours corresponding to ionomics groups. options(repr.plot.width = 9, repr.plot.height = 9) g &lt;- ggplot(centroids, aes(DS1, DS2, label = Cultivar, col=i_group)) + geom_text_repel() + geom_point(alpha = 0.5) + theme_classic(base_size = 12) + #scale_color_manual(values=c(&quot;red&quot;, &quot;magenta&quot;, &quot;blue&quot;, &quot;black&quot;)) + theme(axis.text=element_text(size=12)) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) # Add discriminant loadings using geom_segment() and arrow() x=0; y=0; labels = c(clr_no, rep(NA, nrow(df)-length(clr_no))) g + geom_segment(data=df, mapping=aes(x=x, y=y, xend=x+loadings.DS1, yend=y+loadings.DS2), arrow=arrow(), size = 1, color=&quot;grey80&quot;) + geom_text(data=df, mapping=aes(x=loadings.DS1, y=loadings.DS2, label=labels), size=5, color=&quot;black&quot;) + geom_hline(yintercept=0, color=&quot;black&quot;, linetype=2) + geom_vline(mapping=aes(xintercept=0), color=&quot;black&quot;, linetype=2) + theme(axis.line=element_blank()) Figure 2.4: Discriminant biplot and cluster analysis result of potato cultivars. #ggsave(&quot;images/cultivar_clust.png&quot;, width=10, height=8, dpi = 300) I push cultivars yield cut-off and ionomics groups in the initial data frame. ionomicGroup &lt;- data.frame(lda_centroids[, 1], ionomicGroup = factor(highYieldersKmeans$partition[, &quot;6 groups&quot;])) colnames(ionomicGroup)[colnames(ionomicGroup)==&quot;group&quot;] &lt;- &quot;Cultivar&quot; cutQ &lt;- cutQ[-1, ] # to discard missing cultivar names colnames(cutQ)[which(names(cutQ) == &quot;rv_cut&quot;)] &lt;- &quot;yieldCutoff&quot; cutQ_ig &lt;- cutQ %&gt;% left_join(ionomicGroup, by = &quot;Cultivar&quot;) leaf_clust_df &lt;- leaf_clust_df %&gt;% left_join(y = cutQ_ig, by = &#39;Cultivar&#39;) %&gt;% select(-rv_cut) 2.8 Arranging data for Machine Learning I select useful columns for the next chapter (3) and save the new table as dfml.csv. I conserve only complete cases. new_col &lt;- c(&#39;yieldCutoff&#39;, &#39;yieldClass&#39;, &#39;ionomicGroup&#39;) dfml &lt;- leaf_clust_df %&gt;% select(one_of(c(keys_col, clr_no, cultivarAndyield, new_col, &#39;AnalyseFoliaireStade&#39;))) %&gt;% mutate(isNA = apply(.[c(clr_no, cultivarAndyield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% select(-c(AnalyseFoliaireStade, isNA, is10pcf)) %&gt;% droplevels() %&gt;% filter(complete.cases(.)) nrow(dfml) ## [1] 3382 write_csv(dfml, &quot;output/dfml.csv&quot;) Hence, the data table contains 3382 for the next chapter (Machine learning). 2.9 clr X ionomics groups interactions effects I use a linear mixed effect model to assess the effect of clr coordinates on tubers marketable yield between ionomics groups. This last subsection perfom this analysis by extracting and ploting the interaction coefficients of the model. dfml$Cultivar &lt;- factor(dfml$Cultivar) #dfml$Maturity5 &lt;- relevel(dfml$Maturity5, ref = &quot;late&quot;) dfml$NoEssai &lt;- factor(dfml$NoEssai) colnames(dfml)[colnames(dfml)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; dfml$group_i &lt;- factor(dfml$group_i) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Ca&quot;, &quot;clr_Mg&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;, &quot;clrFv&quot;) # for plot colnames(dfml)[which(names(dfml) %in% clr_no)] &lt;- clrNo I scale clr coordinates before ajusting linear mixed model. Discard the filling value to deal with singularity problem. dfml.sc &lt;- dfml # copy dfml.sc[, clrNo] &lt;- apply(dfml.sc[, clrNo], 2, scale) used_clr = c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;) # without &quot;clr_Fv&quot; lmm &lt;- lme(RendVendable ~ (clrN + clrP + clrK + clrCa + clrMg):group_i, data=dfml.sc, random= ~1|NoEssai) pseudoR2 = rsq(dfml.sc$RendVendable, predict(lmm)) pseudoR2 ## [1] 0.7364863 The next code extracts the interactions coefficients of the model and their p-values (pv) matrix. pv &lt;- summary(lmm)$tTable[-1,] pv ## Value Std.Error DF t-value p-value ## clrN:group_i1 5.3933833 0.5389478 3151 10.00724555 3.135945e-23 ## clrN:group_i2 4.2021244 0.7172997 3151 5.85825466 5.159835e-09 ## clrN:group_i3 1.2119832 5.5667558 3151 0.21771805 8.276629e-01 ## clrN:group_i4 8.3461345 0.6718082 3151 12.42338882 1.246209e-34 ## clrN:group_i5 6.1861820 2.2442539 3151 2.75645378 5.876828e-03 ## clrN:group_i6 -0.1294606 0.9959042 3151 -0.12999303 8.965802e-01 ## clrP:group_i1 4.1971990 0.5815802 3151 7.21688810 6.634669e-13 ## clrP:group_i2 4.5098052 1.1683933 3151 3.85983475 1.157399e-04 ## clrP:group_i3 1.1852349 8.6400878 3151 0.13717857 8.908984e-01 ## clrP:group_i4 8.6397679 0.9105687 3151 9.48832116 4.475315e-21 ## clrP:group_i5 7.2422511 3.0573772 3151 2.36877908 1.790689e-02 ## clrP:group_i6 -0.2334311 1.5708848 3151 -0.14859846 8.818800e-01 ## clrK:group_i1 4.6342810 0.7945301 3151 5.83273222 6.004987e-09 ## clrK:group_i2 3.1938560 1.2446162 3151 2.56613737 1.032964e-02 ## clrK:group_i3 5.1043849 8.1914423 3151 0.62313628 5.332401e-01 ## clrK:group_i4 8.7525831 0.9499339 3151 9.21388679 5.589392e-20 ## clrK:group_i5 7.9748672 3.3775008 3151 2.36117406 1.827772e-02 ## clrK:group_i6 -1.7072640 2.0562420 3151 -0.83028359 4.064414e-01 ## clrCa:group_i1 6.2723620 0.7259704 3151 8.63996880 8.791023e-18 ## clrCa:group_i2 4.0265051 1.1985246 3151 3.35955145 7.899779e-04 ## clrCa:group_i3 4.1851995 14.6818169 3151 0.28506006 7.756169e-01 ## clrCa:group_i4 9.8682808 0.9569556 3151 10.31216141 1.518061e-24 ## clrCa:group_i5 9.2844210 2.7933048 3151 3.32381241 8.981094e-04 ## clrCa:group_i6 -0.1379925 1.8419373 3151 -0.07491704 9.402855e-01 ## clrMg:group_i1 3.8278647 0.5819178 3151 6.57801625 5.561189e-11 ## clrMg:group_i2 1.7023925 1.0971841 3151 1.55160147 1.208580e-01 ## clrMg:group_i3 -1.6469431 7.2522446 3151 -0.22709425 8.203652e-01 ## clrMg:group_i4 7.5107056 0.8956168 3151 8.38607073 7.480994e-17 ## clrMg:group_i5 7.9422630 3.0104999 3151 2.63818747 8.376047e-03 ## clrMg:group_i6 -0.5952666 1.6845395 3151 -0.35337051 7.238343e-01 Then, I extract their confident intervals, and process data for the plot. interval &lt;- tibble(Estimate = intervals(lmm)$fixed[-1, 2], LL = intervals(lmm)$fixed[-1, 1], UL = intervals(lmm)$fixed[-1, 3]) interval$variable &lt;- rep(&#39;NA&#39;, nrow(interval)) interval$variable &lt;- rownames(intervals(lmm)$fixed)[-1] interval$ionomic_group &lt;- rep(paste(&quot;group&quot;, 1:nlevels(dfml$group_i)), length(clrNo)-1) interval$used_clr &lt;- rep(used_clr, each = nlevels(dfml$group_i)) interval$pvalue &lt;- pv[,&quot;p-value&quot;] interval$is_significant = ifelse(interval$pvalue &lt;= 0.05, &#39;P &lt; 0.05&#39;, &#39;P &gt; 0.05&#39;) interval ## # A tibble: 30 x 8 ## Estimate LL UL variable ionomic_group used_clr pvalue ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 5.39 4.34 6.45 clrN:gr~ group 1 clrN 3.14e-23 ## 2 4.20 2.80 5.61 clrN:gr~ group 2 clrN 5.16e- 9 ## 3 1.21 -9.70 12.1 clrN:gr~ group 3 clrN 8.28e- 1 ## 4 8.35 7.03 9.66 clrN:gr~ group 4 clrN 1.25e-34 ## 5 6.19 1.79 10.6 clrN:gr~ group 5 clrN 5.88e- 3 ## 6 -0.129 -2.08 1.82 clrN:gr~ group 6 clrN 8.97e- 1 ## 7 4.20 3.06 5.34 clrP:gr~ group 1 clrP 6.63e-13 ## 8 4.51 2.22 6.80 clrP:gr~ group 2 clrP 1.16e- 4 ## 9 1.19 -15.8 18.1 clrP:gr~ group 3 clrP 8.91e- 1 ## 10 8.64 6.85 10.4 clrP:gr~ group 4 clrP 4.48e-21 ## # ... with 20 more rows, and 1 more variable: is_significant &lt;chr&gt; This cell plots the interaction coefficients and their confident intervals using ggplot2. options(repr.plot.width = 4, repr.plot.height = 8) gg &lt;- ggplot(data = interval, mapping = aes(x = Estimate, y = used_clr, color=is_significant)) + facet_grid(ionomic_group ~ .) + #, scales = &#39;free&#39;, space = &#39;free&#39;) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = used_clr, yend = used_clr)) + geom_point() + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) + theme_bw() + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) gg + theme(legend.title = element_blank())#, legend.position = &#39;bottom&#39;) Figure 2.5: Effect of ionome perturbation on marketable yield as illustrated by a linear mixed effect model. #ggsave(&quot;images/coef_lmm.tiff&quot;, width = 5, height = 5, dpi = 300) "],
["Chapter-Modeling.html", "Chapter 3 Predicting marketable tuber yield 3.1 Load data file 3.2 Machine learning 3.3 Assessing goodness of fit", " Chapter 3 Predicting marketable tuber yield 3.1 Load data file Load data file data_ionome.csv saved from previous cluster analysis. library(&quot;tidyverse&quot;) # &#39;diplyr&#39; and &#39;ggplot2&#39; library(&#39;extrafont&#39;) # Changing Fonts for Graphs df = read_csv(&#39;output/dfml.csv&#39;) colnames(df)[colnames(df)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; # make simplier df$group_i = factor(df$group_i) df$yieldClass = factor(df$yieldClass) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrMg&quot;, &quot;clrCa&quot;, &quot;clrFv&quot;) colnames(df)[which(names(df) %in% clr_no)] &lt;- clrNo df.sc = df # copy df.sc[, clrNo] &lt;- apply(df.sc[, clrNo], 2, scale) # scale clr coordinates Dummy code maturity order and ionomic group. if(&quot;Maturity5&quot; %in% colnames(df.sc)) { df.sc$Maturity5 &lt;- model.matrix(~ordered(factor(df.sc$Maturity5)))[, 2] } if(&quot;group_i&quot; %in% colnames(df.sc)) { df.sc$group_i &lt;- model.matrix(~ordered(factor(df.sc$group_i)))[, 2] } Check the data frame structure. pc &lt;- round(with(df.sc, prop.table(table(Cultivar)) * 100), 2) dist &lt;- with(df.sc, cbind(freq = table(Cultivar), percentage = pc)) dist &lt;- data.frame(cbind(dist, rownames(dist))) colnames(dist)[3] &lt;- &quot;Cultivar&quot; dist$freq &lt;- as.numeric(as.character(dist$freq)) dist %&gt;% arrange(desc(freq)) %&gt;% head(10) # or discard the last part to see all. ## freq percentage Cultivar ## 1 560 16.56 Goldrush ## 2 367 10.85 Superior ## 3 346 10.23 FL 1207 ## 4 258 7.63 Chieftain ## 5 189 5.59 Kennebec ## 6 188 5.56 Snowden ## 7 184 5.44 Atlantic ## 8 183 5.41 FL 1533 ## 9 165 4.88 Coastal Russet ## 10 112 3.31 Shepody 3.2 Machine learning Partioning data in Train and Test (evaluation) sets. Load libraries for machine learning functions. library(&#39;caret&#39;) library(&#39;kknn&#39;) set.seed(853739) # random.org split_index &lt;- createDataPartition(df.sc$yieldClass, group = &quot;Cultivar&quot;, p = 0.75, list = FALSE, times = 1) train &lt;- df.sc[split_index, ] test &lt;- df.sc[-split_index, ] ## With clr coordinates ml_clr &lt;- c(clrNo, &#39;yieldClass&#39;) train_clr = train[, ml_clr] test_clr = test[, ml_clr] ## With clr and maturity classes ml_mc &lt;- c(clrNo, &#39;Maturity5&#39;, &#39;yieldClass&#39;) train_mc = train[, ml_mc] test_mc = test[, ml_mc] ## With clr and ionomic groups ml_grp &lt;- c(clrNo, &#39;group_i&#39;, &#39;yieldClass&#39;) train_grp = train[, ml_grp] test_grp = test[, ml_grp] The knn model will be trained on a grid. grid &lt;- expand.grid(kmax = c(7,9,12,15), # neighborhood distance = 1:2, # 1 for euclidean distance, 2 for Mahalannobis kernel = &quot;optimal&quot;) grid ## kmax distance kernel ## 1 7 1 optimal ## 2 9 1 optimal ## 3 12 1 optimal ## 4 15 1 optimal ## 5 7 2 optimal ## 6 9 2 optimal ## 7 12 2 optimal ## 8 15 2 optimal The models will be train with a 10-fold cross-validation (cv) based on accuracy as loss function. control &lt;- trainControl(method = &quot;cv&quot;, number = 10) metric &lt;- &quot;Accuracy&quot; Train Models: # a) Non-linear algorithm ## kNN set.seed(7) kknn_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) # b) Advanced algorithms ## SVM set.seed(7) svm_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;svmRadial&quot;, metric = metric, trControl = control) ## Random Forest set.seed(7) rf_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;rf&quot;, metric = metric, trControl = control) rf_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;rf&quot;, metric = metric, trControl = control) rf_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;rf&quot;, metric = metric, trControl = control) 3.3 Assessing goodness of fit Models results, to select the best model. # Summary results results &lt;- resamples(list(kknn_clr_solely = kknn_clr, kknn_clr_and_ionomicgroup = kknn_grp, kknn_clr_and_maturityclass = kknn_mc, svm_clr_solely = svm_clr, svm_clr_and_ionomicgroup = svm_grp, svm_clr_and_maturityclass = svm_mc, rf_clr_solely = rf_clr, rf_clr_and_ionomicgroup = rf_grp, rf_clr_and_maturityclass = rf_mc)) #summary(results) with dotplot() options(repr.plot.width = 5, repr.plot.height = 4) dotplot(results) Figure 3.1: Comparison of models accuracies at training. Check the best model at training, but only accuracies with test sets are used as models quality metric. models_acc &lt;- data.frame(Model = summary(results)$models, Accuracy = c(confusionMatrix(train_clr$yieldClass, predict(kknn_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(kknn_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(kknn_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(svm_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(svm_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(svm_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(rf_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(rf_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(rf_mc))$overall[1])) models_acc[order(models_acc[,&quot;Accuracy&quot;], decreasing = TRUE), ] ## Model Accuracy ## 7 rf_clr_solely 0.9795034 ## 8 rf_clr_and_ionomicgroup 0.9795034 ## 9 rf_clr_and_maturityclass 0.9795034 ## 2 kknn_clr_and_ionomicgroup 0.8876626 ## 3 kknn_clr_and_maturityclass 0.8683484 ## 1 kknn_clr_solely 0.8588885 ## 6 svm_clr_and_maturityclass 0.6980686 ## 5 svm_clr_and_ionomicgroup 0.6838786 ## 4 svm_clr_solely 0.6637761 Quality metrics with test (evaluation) set. predicted_kknn_clr &lt;- predict(kknn_clr, test_clr) predicted_kknn_mc &lt;- predict(kknn_mc, test_mc) predicted_kknn_grp &lt;- predict(kknn_grp, test_grp) predicted_svm_clr &lt;- predict(svm_clr, test_clr) predicted_svm_mc &lt;- predict(svm_mc, test_mc) predicted_svm_grp &lt;- predict(svm_grp, test_grp) predicted_rf_clr &lt;- predict(rf_clr, test_clr) predicted_rf_mc &lt;- predict(rf_mc, test_mc) predicted_rf_grp &lt;- predict(rf_grp, test_grp) #The best model tests_acc &lt;- data.frame(Model = summary(results)$models, Accuracy_on_test = c( confusionMatrix(test_clr$yieldClass, predicted_kknn_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_kknn_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_kknn_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_svm_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_svm_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_svm_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_rf_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_rf_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_rf_mc)$overall[1])) tests_acc[order(tests_acc[,&quot;Accuracy_on_test&quot;], decreasing = TRUE), ] ## Model Accuracy_on_test ## 9 rf_clr_and_maturityclass 0.7254438 ## 8 rf_clr_and_ionomicgroup 0.7183432 ## 3 kknn_clr_and_maturityclass 0.7053254 ## 7 rf_clr_solely 0.7029586 ## 1 kknn_clr_solely 0.6828402 ## 2 kknn_clr_and_ionomicgroup 0.6816568 ## 6 svm_clr_and_maturityclass 0.6721893 ## 5 svm_clr_and_ionomicgroup 0.6639053 ## 4 svm_clr_solely 0.6568047 Yield Class Prediction with rf algorithm on test set Order prediction quality metric by cultivar with rf combining clr values and new clustered variable (rf_clr_and_ionomicgroup). test$ypred = predicted_rf_grp # adds predictions to test set cultivar_acc &lt;- test %&gt;% group_by(Cultivar) %&gt;% do(Accuracy = as.numeric(confusionMatrix(.$yieldClass, .$ypred)$overall[1]), N_obs = as.numeric(nrow(.))) cultivar_acc$Accuracy &lt;- unlist(cultivar_acc$Accuracy) cultivar_acc$N_obs &lt;- unlist(cultivar_acc$N_obs) data = data.frame(subset(cultivar_acc, Accuracy&gt;0)) data[order(data[,&quot;Accuracy&quot;], decreasing=T), ] ## Cultivar Accuracy N_obs ## 12 Dark Red Chieftain 1.0000000 2 ## 18 Harmony 1.0000000 1 ## 19 Kanona 1.0000000 1 ## 21 Keuka Gold 1.0000000 1 ## 22 Lamoka 1.0000000 2 ## 25 Norland 1.0000000 12 ## 26 Peribonka 1.0000000 2 ## 29 Red Cloud 1.0000000 4 ## 35 Sifra 1.0000000 2 ## 40 W 1386 1.0000000 4 ## 41 Waneta 1.0000000 3 ## 10 Chieftain 0.8260870 69 ## 34 Shepody 0.8214286 28 ## 24 Mystere 0.8125000 16 ## 6 Argos 0.8000000 5 ## 33 Russet Norkota 0.8000000 5 ## 14 FL 1207 0.7872340 94 ## 17 Goldrush 0.7746479 142 ## 20 Kennebec 0.7727273 44 ## 2 AC Chaleur 0.7500000 4 ## 4 Andover 0.7500000 8 ## 31 Roko 0.7500000 4 ## 32 Russet Burbank 0.7500000 8 ## 11 Coastal Russet 0.7352941 34 ## 27 Pommerelle 0.7142857 7 ## 36 Snowden 0.6923077 39 ## 9 Carolina 0.6666667 3 ## 30 Red Maria 0.6666667 3 ## 15 FL 1533 0.6607143 56 ## 37 Superior 0.6288660 97 ## 1 AC Belmont 0.6250000 16 ## 39 Vivaldi 0.6000000 5 ## 7 Atlantic 0.5769231 52 ## 42 Yukon Gold 0.5555556 18 ## 5 Aquilon 0.5263158 19 ## 3 Ambra 0.5000000 2 ## 8 Bijou Rouge 0.5000000 2 ## 13 Estima 0.5000000 12 ## 16 Frontier Russet 0.5000000 4 ## 28 Reba 0.5000000 4 ## 23 Lanorma 0.3333333 3 ## 38 Viking 0.3333333 3 Check it with geom_segment(): options(repr.plot.width = 8, repr.plot.height = 4) ggplot(data, aes(reorder(Cultivar, Accuracy), Accuracy)) + geom_point(aes(color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1)))) + geom_segment(aes(x=Cultivar, xend=Cultivar, y=0, yend=Accuracy, color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1))), size=1) + xlab(&quot;Cultivar&quot;) + theme_bw() + theme(legend.title=element_blank(), axis.text.x=element_text(angle=90, hjust=1))+ theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) (#fig:accuracy_cultivar)Predictive accuracy for cultivars. #ggsave(&quot;images/cultivAcc.tiff&quot;, width=8, height=3) Run the Chi-square homogenity test to compare prediction with a non-informative classification consisting of an equal distribution of 50% successful and 50% unsuccessful cases, using kknn. cm &lt;- confusionMatrix(predicted_kknn_grp, test_grp$yieldClass) # confusion matrix cm$table ## Reference ## Prediction HY LY ## HY 206 142 ## LY 127 370 # Model&#39;s classification good_class &lt;- cm$table[1,1]+cm$table[2,2] # HY or LY and correctly predicted misclass &lt;- cm$table[1,2]+cm$table[2,1] # wrong classification ml_class &lt;- c(good_class, misclass) # Non-informative model total &lt;- sum(cm$table) # number of samples good_nim &lt;- 0.50 * total misclass_nim &lt;- 0.50 * total non_inf_model &lt;- c(good_nim, misclass_nim) # Matrix for chisquare test m &lt;- rbind(ml_class, non_inf_model) m ## [,1] [,2] ## ml_class 576.0 269.0 ## non_inf_model 422.5 422.5 # chisq.test khi2_test &lt;- chisq.test(m) khi2_test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: m ## X-squared = 56.923, df = 1, p-value = 4.533e-14 The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test. The train data set with predicted yield classes train_df backup will be used to test perturbation vector theory in the next file. pred_yield &lt;- predict(rf_grp, train_grp) train_df = data.frame(cbind(df[split_index, ], pred_yield)) write_csv(train_df, &quot;output/train_df.csv&quot;) nrow(train_df) ## [1] 2537 True Negatives (TN) specimens in this study are observations of the training data set with high yield (HY) and correctly predicted. Compute clr centroids for True Negatives with original clr values; use original data frame df. TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; pred_yield == &#39;HY&#39;, ] nrow(TNs) ## [1] 988 The next cell computes clr balances Centroids for ionomic groups. TNmedianNorms &lt;- TNs %&gt;% group_by(group_i) %&gt;% select(clrNo) %&gt;% summarise_all(list(median)) ## Adding missing grouping variables: `group_i` TNmedianNorms ## # A tibble: 6 x 7 ## group_i clrN clrP clrK clrMg clrCa clrFv ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.727 -1.98 0.512 -1.82 -0.979 3.56 ## 2 2 0.910 -1.93 0.532 -1.92 -1.14 3.57 ## 3 3 0.894 -2.22 0.359 -1.60 -1.20 3.83 ## 4 4 0.747 -1.97 0.219 -1.48 -1.04 3.50 ## 5 5 0.601 -2.20 0.507 -1.60 -0.823 3.40 ## 6 6 0.712 -2.02 0.640 -2.13 -0.837 3.61 "],
["Chapter-Perturbation-vector.html", "Chapter 4 Perturbation vector theory 4.1 Dissimilarity index between compositions 4.2 Rebalancing a misbalanced sample by perturbation", " Chapter 4 Perturbation vector theory The first step is to compute a dissimilarity index according to the distance from the closest healthy point (the closest TN). Let’s load data set. library(&quot;tidyverse&quot;) # dplyr and ggplot2 library(extrafont) # Changing Fonts for Graphs train_df = read_csv(&quot;output/train_df.csv&quot;) train_df$group_i = factor(train_df$group_i) As considered in previous 03_machine-learning.ipynb file, True Negatives (TN) specimens in this study are observations of the training data set with high yield (HY) and correctly predicted. TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; train_df$pred_yield == &#39;HY&#39;, ] clrNo = c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;, &quot;clrFv&quot;) # for simplicity 4.1 Dissimilarity index between compositions Euclidean distance as dissimilarity index For each composition (line) in the “unbalanced specimens”, calculate all the euclidean distances between all the compositions in “TNs” of the corresponding group Return the smallest euclidean distance as the unbalanced index of the composition. Function to compute euclidean distance eucl_dist_f &lt;- function(x, y) { sqrt(sum((x-y)^2)) } Compute debalance (or imbalance) index (debal) using this loop. debal &lt;- c() debal_index &lt;- c() for (i in 1:nrow(train_df)) { clr_i &lt;- as.numeric(train_df[i, clrNo]) eucl_dist &lt;- apply(TNs %&gt;% filter(group_i == train_df$group_i[i]) %&gt;% select(clrNo), 1, function(x) eucl_dist_f(x=x, y=clr_i)) debal_index[i] &lt;- which.min(eucl_dist) debal[i] &lt;- eucl_dist[debal_index[i]] } train_df$debal &lt;- debal train_df %&gt;% glimpse() ## Observations: 2,537 ## Variables: 17 ## $ NoEssai &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ NoBloc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3... ## $ NoTraitement &lt;dbl&gt; 1, 2, 4, 6, 7, 8, 2, 3, 4, 6, 7, 1, 2, 4, 6, 7, 8... ## $ clrN &lt;dbl&gt; 0.3321186, 0.4252302, 0.3993264, 0.4885647, 0.720... ## $ clrP &lt;dbl&gt; -2.518325, -2.394957, -2.599180, -2.525335, -2.30... ## $ clrK &lt;dbl&gt; 0.8379383, 1.1387291, 0.7945826, 0.9418880, 0.720... ## $ clrMg &lt;dbl&gt; -1.639076, -2.346167, -1.577529, -1.763195, -1.70... ## $ clrCa &lt;dbl&gt; -0.4794688, -0.5154924, -0.6220171, -0.6426036, -... ## $ clrFv &lt;dbl&gt; 3.466813, 3.692658, 3.604817, 3.500681, 3.432323,... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-se... ## $ RendVendable &lt;dbl&gt; 18.94420, 40.35180, 37.55050, 41.01670, 46.79370,... ## $ yieldCutoff &lt;dbl&gt; 41.33053, 41.33053, 41.33053, 41.33053, 41.33053,... ## $ yieldClass &lt;chr&gt; &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;... ## $ group_i &lt;fct&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2... ## $ pred_yield &lt;chr&gt; &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;HY&quot;, &quot;LY&quot;, &quot;HY&quot;, &quot;... ## $ debal &lt;dbl&gt; 0.2915435, 0.3247296, 0.2628167, 0.2987338, 0.000... 4.2 Rebalancing a misbalanced sample by perturbation Suppose we got this point selected at random in unbalanced specimens. #set.seed(932559) # random.org unbalanced &lt;- subset(train_df, debal !=0) misbalanced &lt;- unbalanced[sample(nrow(unbalanced), 1), ] misbalanced ## # A tibble: 1 x 17 ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg clrCa clrFv Cultivar ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 15004 2 1 0.759 -1.99 0.208 -1.61 -0.855 3.49 Goldrush ## # ... with 7 more variables: Maturity5 &lt;chr&gt;, RendVendable &lt;dbl&gt;, ## # yieldCutoff &lt;dbl&gt;, yieldClass &lt;chr&gt;, group_i &lt;fct&gt;, pred_yield &lt;chr&gt;, ## # debal &lt;dbl&gt; Or even, you can use the most unbalanced occurrence, … why not ! (misbalanced = unbalanced[which.max(unbalanced$debal), ]) ## # A tibble: 1 x 17 ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg clrCa clrFv Cultivar ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 275 3 3 0.886 -1.69 0.0354 -1.26 -1.45 3.48 Andover ## # ... with 7 more variables: Maturity5 &lt;chr&gt;, RendVendable &lt;dbl&gt;, ## # yieldCutoff &lt;dbl&gt;, yieldClass &lt;chr&gt;, group_i &lt;fct&gt;, pred_yield &lt;chr&gt;, ## # debal &lt;dbl&gt; How to rebalance it? The first step is to find in TNs of the corresponding ionomic group the closest balanced point. Let’s re-compute the euclidean distance, like for a new (an unknown) point: misbalanced &lt;- misbalanced[clrNo] eucl_dist_misbal &lt;- apply(TNs[, clrNo], 1, function(x) eucl_dist_f(x=x, y=misbalanced)) index_misbal &lt;- which.min(t(data.frame(eucl_dist_misbal))) # return the index of the sample index_misbal ## [1] 401 Euclidean distance matched with the corresponding debal value: misbal = eucl_dist_misbal[index_misbal] # to compare to corresponding debal value misbal ## [1] 0.2411328 The closest point in the TNs data set is this one: closest &lt;- TNs[index_misbal, ] closest ## # A tibble: 1 x 16 ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg clrCa clrFv Cultivar ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 89 3 19 0.746 -1.61 0.0654 -1.22 -1.33 3.36 FL 1207 ## # ... with 6 more variables: Maturity5 &lt;chr&gt;, RendVendable &lt;dbl&gt;, ## # yieldCutoff &lt;dbl&gt;, yieldClass &lt;chr&gt;, group_i &lt;fct&gt;, pred_yield &lt;chr&gt; Note that Ionomics groups of the misbalanced and the closest composition are the same … most of the times ! You need package compositions for further clr back-transformation de compositional space. library(&#39;compositions&#39;) Perturbation in compositional space plays the same role as translation plays in real space. Some natural processes in nature can be interpreted as a change from one composition C1 to another C2 through the application of a perturbation: p ⊕ C1 ====&gt; C2. The difference between the new observation and the closest TN composition can be back-transformed to the compositional space. The obtained vector is the perturbation vector. closest = closest[clrNo] clr_diff = closest - misbalanced clr_diff ## clrN clrP clrK clrCa clrMg clrFv ## 1 -0.139849 0.08075188 0.02995412 0.1174671 0.03790527 -0.1262294 comp_names &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;, &quot;Fv&quot;) perturbation_vector &lt;- clrInv(clr_diff) names(perturbation_vector) &lt;- comp_names Compute the compositions of the ilr coordinates of the misbalanced point, as well as the closest TN point: misbal_comp &lt;- clrInv(misbalanced) names(misbal_comp) &lt;- comp_names closest_comp &lt;- clrInv(closest) names(closest_comp) &lt;- comp_names pmc = rbind(perturbation_vector, misbal_comp, closest_comp) rownames(pmc) = c(&quot;perturbation_vector&quot;,&quot;misbal_comp&quot;,&quot;closest_comp&quot;) pmc # data frame made up of perturbation vector, misbalanced composition and the closest reference sample ## N P K Ca Mg ## perturbation_vector 0.1442253 0.1798238 0.1709173 0.1865488 0.1722817 ## misbal_comp 0.0660000 0.0050000 0.0282000 0.0064000 0.0077000 ## closest_comp 0.0645800 0.0061000 0.0327000 0.0081000 0.0090000 ## Fv ## perturbation_vector 0.146203 ## misbal_comp 0.886700 ## closest_comp 0.879520 For each vector, check that the simplex is closed to 1. sum(perturbation_vector); sum(misbal_comp); sum(closest_comp) ## [1] 1 ## [1] 1 ## [1] 1 The closest composition minus the misbalanced composition should return the perturbation vector. print(closest_comp - misbal_comp) # soustraction ## N P K Ca Mg Fv ## [1,] 0.1442253 0.1798238 0.1709173 0.1865488 0.1722817 0.146203 ## attr(,&quot;class&quot;) ## [1] acomp print(perturbation_vector) # for comparison ## N P K Ca Mg Fv ## [1,] 0.1442253 0.1798238 0.1709173 0.1865488 0.1722817 0.146203 ## attr(,&quot;class&quot;) ## [1] acomp Or even, perturb the misbalanced point by the perturbation vector, you should obtain the closest TN point: print(misbal_comp + perturbation_vector) # perturbation ## N P K Ca Mg Fv ## [1,] 0.06458 0.0061 0.0327 0.0081 0.009 0.87952 ## attr(,&quot;class&quot;) ## [1] acomp print(closest_comp) # for comparison ## N P K Ca Mg Fv ## [1,] 0.06458 0.0061 0.0327 0.0081 0.009 0.87952 ## attr(,&quot;class&quot;) ## [1] acomp So, the assumption is true. The next codes try to show the concept using plots using clr coordinates. d = data.frame(rbind(misbalanced, closest, clr_diff)) # mis, cl, per vectors = c(&quot;observation&quot;, &quot;reference&quot;, &quot;perturbation&quot;) d$vectors = factor(vectors) d ## clrN clrP clrK clrCa clrMg clrFv ## 1 0.8857481 -1.69446870 0.03541537 -1.4476086 -1.26268628 3.4836001 ## 2 0.7458992 -1.61371682 0.06536948 -1.3301415 -1.22478102 3.3573707 ## 3 -0.1398490 0.08075188 0.02995412 0.1174671 0.03790527 -0.1262294 ## vectors ## 1 observation ## 2 reference ## 3 perturbation This data frame must be reshape for ggplot. library(&quot;reshape&quot;) # function melt() dreshape = melt(d) dreshape ## vectors variable value ## 1 observation clrN 0.88574813 ## 2 reference clrN 0.74589917 ## 3 perturbation clrN -0.13984896 ## 4 observation clrP -1.69446870 ## 5 reference clrP -1.61371682 ## 6 perturbation clrP 0.08075188 ## 7 observation clrK 0.03541537 ## 8 reference clrK 0.06536948 ## 9 perturbation clrK 0.02995412 ## 10 observation clrCa -1.44760862 ## 11 reference clrCa -1.33014153 ## 12 perturbation clrCa 0.11746709 ## 13 observation clrMg -1.26268628 ## 14 reference clrMg -1.22478102 ## 15 perturbation clrMg 0.03790527 ## 16 observation clrFv 3.48360010 ## 17 reference clrFv 3.35737071 ## 18 perturbation clrFv -0.12622939 Plot with dots for each vector. options(repr.plot.width = 6, repr.plot.height = 3) ggplot(data = dreshape, aes(x = value, y = vectors)) + geom_point() + facet_wrap(~ variable, scales = &quot;free_x&quot;) + labs(x=&#39;clr coordinate&#39;, y =&#39;&#39;) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) Figure 4.1: Perturbation vector computation example dotplot using the most imbalanced foliar sample. #ggsave(&quot;images/perturb_dotplot.tiff&quot;, width = 6, height = 4) options(repr.plot.width = 5, repr.plot.height = 3) ggplot(data=dreshape, aes(x=variable, y=value, fill=vectors)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + coord_flip() + theme_bw() + theme(legend.title=element_blank()) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) Figure 4.2: Perturbation vector computation example barplot using the most imbalanced foliar sample. #ggsave(&quot;images/perturb_barplot.tiff&quot;, width = 6, height = 4) "],
["references.html", "References", " References "]
]
