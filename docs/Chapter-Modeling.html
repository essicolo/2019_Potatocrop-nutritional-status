<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Predicting tuber yield category | Balancing the nutritional status of potato crops.</title>
  <meta name="description" content="We use the bookdown package to write a book that describs the statistical computations of my PhD Project, for publication on Github." />
  <meta name="generator" content="bookdown 0.10 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Predicting tuber yield category | Balancing the nutritional status of potato crops." />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="We use the bookdown package to write a book that describs the statistical computations of my PhD Project, for publication on Github." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Predicting tuber yield category | Balancing the nutritional status of potato crops." />
  
  <meta name="twitter:description" content="We use the bookdown package to write a book that describs the statistical computations of my PhD Project, for publication on Github." />
  

<meta name="author" content="Zonlehoua Coulibali and Serge-Étienne Parent" />


<meta name="date" content="2019-07-30" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="Chapter-Clustering.html">
<link rel="next" href="Chapter-Perturbation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Data processing</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#objective"><i class="fa fa-check"></i><b>1.1</b> Objective</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#useful-libraries-for-data-handling"><i class="fa fa-check"></i><b>1.2</b> Useful libraries for data handling</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#quebec-potato-data-set"><i class="fa fa-check"></i><b>1.3</b> Québec potato data set</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#selection-of-useful-variables"><i class="fa fa-check"></i><b>1.4</b> Selection of useful variables</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#arranging-the-data-frame"><i class="fa fa-check"></i><b>1.5</b> Arranging the data frame</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#cultivar-classes-correction"><i class="fa fa-check"></i><b>1.6</b> Cultivar classes correction</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#summarise-and-backup"><i class="fa fa-check"></i><b>1.7</b> Summarise and backup</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html"><i class="fa fa-check"></i><b>2</b> Ionome analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#objective-1"><i class="fa fa-check"></i><b>2.1</b> Objective</a></li>
<li class="chapter" data-level="2.2" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#useful-libraries-and-custom-functions"><i class="fa fa-check"></i><b>2.2</b> Useful libraries and custom functions</a></li>
<li class="chapter" data-level="2.3" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#leaves-processed-compositions-data-set"><i class="fa fa-check"></i><b>2.3</b> Leaves processed compositions data set</a></li>
<li class="chapter" data-level="2.4" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#the-yield-cut-off-low-and-high-yielders-delimiter"><i class="fa fa-check"></i><b>2.4</b> The yield cut-off, low and high yielders delimiter</a></li>
<li class="chapter" data-level="2.5" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#centered-log-ratio-clr-centroids-computation"><i class="fa fa-check"></i><b>2.5</b> Centered log-ratio (clr) centroids computation</a></li>
<li class="chapter" data-level="2.6" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#clustering-potato-cultivars-with-leaf-ionome"><i class="fa fa-check"></i><b>2.6</b> Clustering potato cultivars with leaf ionome</a></li>
<li class="chapter" data-level="2.7" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#axis-reduction"><i class="fa fa-check"></i><b>2.7</b> Axis reduction</a></li>
<li class="chapter" data-level="2.8" data-path="Chapter-Clustering.html"><a href="Chapter-Clustering.html#do-clrs-affect-potato-tuber-yield"><i class="fa fa-check"></i><b>2.8</b> Do clrs affect potato tuber yield?</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html"><i class="fa fa-check"></i><b>3</b> Predicting tuber yield category</a><ul>
<li class="chapter" data-level="3.1" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#objective-2"><i class="fa fa-check"></i><b>3.1</b> Objective</a></li>
<li class="chapter" data-level="3.2" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#useful-libraries"><i class="fa fa-check"></i><b>3.2</b> Useful libraries</a></li>
<li class="chapter" data-level="3.3" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#machine-learning-data-set"><i class="fa fa-check"></i><b>3.3</b> Machine learning data set</a></li>
<li class="chapter" data-level="3.4" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#machine-learning"><i class="fa fa-check"></i><b>3.4</b> Machine learning</a><ul>
<li class="chapter" data-level="3.4.1" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#data-train-and-test-splits"><i class="fa fa-check"></i><b>3.4.1</b> Data train and test splits</a></li>
<li class="chapter" data-level="3.4.2" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#building-the-models"><i class="fa fa-check"></i><b>3.4.2</b> Building the Models</a></li>
<li class="chapter" data-level="3.4.3" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#goodness-of-fit-on-training-set"><i class="fa fa-check"></i><b>3.4.3</b> Goodness of fit on training set</a></li>
<li class="chapter" data-level="3.4.4" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#models-evaluation-test-set"><i class="fa fa-check"></i><b>3.4.4</b> Models’ evaluation (test set)</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#variable-importance-estimation"><i class="fa fa-check"></i><b>3.5</b> Variable importance estimation</a></li>
<li class="chapter" data-level="3.6" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#make-predictions-on-the-test-set-with-kknn-model"><i class="fa fa-check"></i><b>3.6</b> Make predictions on the test set with <code>kknn</code> model</a></li>
<li class="chapter" data-level="3.7" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#comparison-with-non-informative-classification"><i class="fa fa-check"></i><b>3.7</b> Comparison with non-informative classification</a></li>
<li class="chapter" data-level="3.8" data-path="Chapter-Modeling.html"><a href="Chapter-Modeling.html#nutritionally-balanced-compositions"><i class="fa fa-check"></i><b>3.8</b> Nutritionally balanced compositions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html"><i class="fa fa-check"></i><b>4</b> Ionome perturbation concept</a><ul>
<li class="chapter" data-level="4.1" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html#objective-3"><i class="fa fa-check"></i><b>4.1</b> Objective</a></li>
<li class="chapter" data-level="4.2" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html#data-set-and-useful-libraries"><i class="fa fa-check"></i><b>4.2</b> Data set and useful libraries</a></li>
<li class="chapter" data-level="4.3" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html#euclidean-distance-from-nutritionally-balanced-compositions"><i class="fa fa-check"></i><b>4.3</b> Euclidean distance from nutritionally balanced compositions</a></li>
<li class="chapter" data-level="4.4" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html#perturbation-effect-of-some-elements-on-the-whole"><i class="fa fa-check"></i><b>4.4</b> Perturbation effect of some elements on the whole</a></li>
<li class="chapter" data-level="4.5" data-path="Chapter-Perturbation.html"><a href="Chapter-Perturbation.html#rebalancing-a-misbalanced-sample-by-perturbation"><i class="fa fa-check"></i><b>4.5</b> Rebalancing a misbalanced sample by perturbation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Balancing the nutritional status of potato crops.</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="Chapter-Modeling" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Predicting tuber yield category</h1>
<div id="objective-2" class="section level2">
<h2><span class="header-section-number">3.1</span> Objective</h2>
<hr />
<p>The objective of this chapter is to develop, evaluate and compare the performance of some machine learning algorithms (k-nearest neighbors, random forest and support vector machine - <a href="https://topepo.github.io/caret/index.html">package caret</a>) in predicting tuber yield categories using clr coordinates. We use the previous chapter (chapter <a href="Chapter-Clustering.html#Chapter-Clustering">2</a>) tidded data file <code>dfml.csv</code> which contains clr coordinates, maturity classes and the yield two categorical variable created using the 65^th percentile for each cultivar. We use <code>accuracy</code> as models quality meeasure. We run a <em>Chi-square homogenity test</em> to compare the best model (with highest accuracy) with a <em>random classifier</em> consisting of an equal distribution of 50% successful and 50% unsuccessful cases.</p>
<p>We finally compute Euclidean distance as the measure of the multivariate distance between an observation and the closest true negative. By true negative or nutritionally balanced specimens, we mean the samples correctely predicted by the best model as high yielders in the training set. The training and testing sets are stored for the next chapter (preturbation concept - Chapter <a href="Chapter-Perturbation.html#Chapter-Perturbation">4</a>).</p>
<hr />
</div>
<div id="useful-libraries" class="section level2">
<h2><span class="header-section-number">3.2</span> Useful libraries</h2>
<p>The <code>tidyverse</code> package is always needed for data easy manipulation and visualization, and then <code>extrafont</code> to make changes in graphs as demanded for the article. The particularly useful packages are <code>caret</code> and <a href="https://www.rdocumentation.org/packages/kknn/versions/1.3.1">kknn</a> needed for machine leraning functions.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;tidyverse&quot;</span>)
<span class="kw">library</span>(<span class="st">&#39;extrafont&#39;</span>)
<span class="kw">library</span>(<span class="st">&#39;caret&#39;</span>)
<span class="kw">library</span>(<span class="st">&#39;kknn&#39;</span>)</code></pre></div>
</div>
<div id="machine-learning-data-set" class="section level2">
<h2><span class="header-section-number">3.3</span> Machine learning data set</h2>
<p>Let’s load the <code>dfml.csv</code> data set. The clr coordinates are scaled to zero mean and unity variance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dfml =<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&#39;output/dfml.csv&#39;</span>)</code></pre></div>
<pre><code>## Parsed with column specification:
## cols(
##   NoEssai = col_double(),
##   NoBloc = col_double(),
##   NoTraitement = col_double(),
##   clr_N = col_double(),
##   clr_P = col_double(),
##   clr_K = col_double(),
##   clr_Ca = col_double(),
##   clr_Mg = col_double(),
##   clr_Fv = col_double(),
##   RendVendable = col_double(),
##   rv_cut = col_double(),
##   yieldClass = col_character(),
##   Cultivar = col_character(),
##   Maturity5 = col_character()
## )</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dfml<span class="op">$</span>Maturity5 =<span class="st"> </span><span class="kw">factor</span>(dfml<span class="op">$</span>Maturity5)
dfml<span class="op">$</span>yieldClass =<span class="st"> </span><span class="kw">factor</span>(dfml<span class="op">$</span>yieldClass)
clr_no &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;clr_N&quot;</span>, <span class="st">&quot;clr_P&quot;</span>, <span class="st">&quot;clr_K&quot;</span>, <span class="st">&quot;clr_Mg&quot;</span>, <span class="st">&quot;clr_Ca&quot;</span>, <span class="st">&quot;clr_Fv&quot;</span>)
dfml.sc &lt;-<span class="st"> </span>dfml <span class="co"># copy</span>
dfml.sc[, clr_no] &lt;-<span class="st"> </span><span class="kw">apply</span>(dfml.sc[, clr_no], <span class="dv">2</span>, scale) <span class="co"># scale predictors</span></code></pre></div>
</div>
<div id="machine-learning" class="section level2">
<h2><span class="header-section-number">3.4</span> Machine learning</h2>
<div id="data-train-and-test-splits" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Data train and test splits</h3>
<p>We randomly split the data into a training set (75% of the data) used to fit the models, and a testing set (remaining 25%) used for models’ evaluation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">8539</span>)
split_index &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(dfml.sc<span class="op">$</span>yieldClass, <span class="dt">group =</span> <span class="st">&quot;Cultivar&quot;</span>,
                                   <span class="dt">p =</span> <span class="fl">0.75</span>, <span class="dt">list =</span> <span class="ot">FALSE</span>, <span class="dt">times =</span> <span class="dv">1</span>)
train_df &lt;-<span class="st"> </span>dfml.sc[split_index, ]
test_df &lt;-<span class="st"> </span>dfml.sc[<span class="op">-</span>split_index, ]
ml_col &lt;-<span class="st"> </span><span class="kw">c</span>(clr_no, <span class="st">&quot;yieldClass&quot;</span>)</code></pre></div>
<p>With the <code>kknn</code> package, we must specify three parameters: <code>kmax</code> which is the number of neighbors to consider, <code>distance</code> a distance parameter to specify (1 for the Mahattan distance and 2 for the Euclidean distance), and a <code>kernel</code> which is a function to measure the distance. A best method currently used to choose the right parameters consists in creating a parameter grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(grid &lt;-<span class="st">  </span><span class="kw">expand.grid</span>(<span class="dt">kmax =</span> <span class="kw">c</span>(<span class="dv">7</span>,<span class="dv">9</span>,<span class="dv">12</span>,<span class="dv">15</span>), <span class="dt">distance =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,
                      <span class="dt">kernel =</span> <span class="kw">c</span>(<span class="st">&quot;rectangular&quot;</span>, <span class="st">&quot;gaussian&quot;</span>, <span class="st">&quot;optimal&quot;</span>)))</code></pre></div>
<pre><code>##    kmax distance      kernel
## 1     7        1 rectangular
## 2     9        1 rectangular
## 3    12        1 rectangular
## 4    15        1 rectangular
## 5     7        2 rectangular
## 6     9        2 rectangular
## 7    12        2 rectangular
## 8    15        2 rectangular
## 9     7        1    gaussian
## 10    9        1    gaussian
## 11   12        1    gaussian
## 12   15        1    gaussian
## 13    7        2    gaussian
## 14    9        2    gaussian
## 15   12        2    gaussian
## 16   15        2    gaussian
## 17    7        1     optimal
## 18    9        1     optimal
## 19   12        1     optimal
## 20   15        1     optimal
## 21    7        2     optimal
## 22    9        2     optimal
## 23   12        2     optimal
## 24   15        2     optimal</code></pre>
<p>The metric of “Accuracy” is used to evaluate models quality. This is the ratio of the number of correctly predicted instances divided by the total number of instances in the dataset (e.g. 95% accurate).</p>
<p>The accuracy of the models will be estimated using a <code>10-fold cross-validation (cv)</code> scheme. This will split the data set into 10 subsets of equal size. The models are built 10 times, each time leaving out one of the subsets from training and use it as the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">control &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">10</span>)
metric &lt;-<span class="st"> &quot;Accuracy&quot;</span></code></pre></div>
</div>
<div id="building-the-models" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Building the Models</h3>
<p>We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable <a href="https://machinelearningmastery.com/machine-learning-in-r-step-by-step/">Jason Brownlee</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># a) Non-linear algorithm</span>
## kNN
<span class="kw">set.seed</span>(<span class="dv">7</span>)
kknn_ &lt;-<span class="st"> </span><span class="kw">train</span>(yieldClass <span class="op">~</span>., <span class="dt">data =</span> train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col), <span class="dt">method =</span> <span class="st">&quot;kknn&quot;</span>, 
                  <span class="dt">metric =</span> metric, <span class="dt">trControl =</span> control, <span class="dt">tuneGrid =</span> grid)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># b) Advanced algorithms</span>
## SVM
<span class="kw">set.seed</span>(<span class="dv">7</span>)
svm_ &lt;-<span class="st"> </span><span class="kw">train</span>(yieldClass <span class="op">~</span>., <span class="dt">data =</span> train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col), <span class="dt">method =</span> <span class="st">&quot;svmRadial&quot;</span>, 
                 <span class="dt">metric =</span> metric, <span class="dt">trControl =</span> control)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Random Forest
<span class="kw">set.seed</span>(<span class="dv">7</span>)
rf_ &lt;-<span class="st"> </span><span class="kw">train</span>(yieldClass <span class="op">~</span>., <span class="dt">data =</span> train_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col), <span class="dt">method =</span> <span class="st">&quot;rf&quot;</span>, 
                <span class="dt">metric =</span> metric, <span class="dt">trControl =</span> control)</code></pre></div>
</div>
<div id="goodness-of-fit-on-training-set" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Goodness of fit on training set</h3>
<p>We assess the accuracy metric also during modeling (with train set) but the target metric is for the evaluation set. This chart sorts models.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Summary results</span>
results &lt;-<span class="st"> </span><span class="kw">resamples</span>(<span class="kw">list</span>(<span class="dt">kknn_model =</span> kknn_, 
                          <span class="dt">svm_model =</span> svm_, 
                          <span class="dt">rf_model =</span> rf_))
<span class="co">#summary(results) with dotplot()</span>
<span class="kw">dotplot</span>(results)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:ml-traindotplot"></span>
<img src="2019_Bookdown_files/figure-html/ml-traindotplot-1.png" alt="Comparison of models accuracies at training." width="100%" />
<p class="caption">
Figure 3.1: Comparison of models accuracies at training.
</p>
</div>
<p>This chunk also sorts models in a descending order using accuracies only, in a table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">models_acc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="kw">summary</span>(results)<span class="op">$</span>models,
                       <span class="dt">Accuracy =</span> <span class="kw">c</span>(<span class="kw">confusionMatrix</span>(train_df<span class="op">$</span>yieldClass, <span class="kw">predict</span>(kknn_))<span class="op">$</span>overall[<span class="dv">1</span>],
                                    <span class="kw">confusionMatrix</span>(train_df<span class="op">$</span>yieldClass, <span class="kw">predict</span>(svm_))<span class="op">$</span>overall[<span class="dv">1</span>],
                                    <span class="kw">confusionMatrix</span>(train_df<span class="op">$</span>yieldClass, <span class="kw">predict</span>(rf_))<span class="op">$</span>overall[<span class="dv">1</span>]))
models_acc[<span class="kw">order</span>(models_acc[,<span class="st">&quot;Accuracy&quot;</span>], <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]</code></pre></div>
<pre><code>##        Model  Accuracy
## 3   rf_model 0.9834450
## 1 kknn_model 0.8561293
## 2  svm_model 0.6614111</code></pre>
<p>The next one prints the best tuning parameters that maximizes model accuracy.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="kw">summary</span>(results)<span class="op">$</span>models,
           <span class="dt">param =</span> <span class="kw">c</span>(kknn_<span class="op">$</span>bestTune, 
                     svm_<span class="op">$</span>bestTune, 
                     rf_<span class="op">$</span>bestTune))</code></pre></div>
<pre><code>##        Model param.kmax param.distance param.kernel param.sigma param.C
## 1 kknn_model         12              1     gaussian   0.1953406     0.5
## 2  svm_model         12              1     gaussian   0.1953406     0.5
## 3   rf_model         12              1     gaussian   0.1953406     0.5
##   param.mtry
## 1          6
## 2          6
## 3          6</code></pre>
</div>
<div id="models-evaluation-test-set" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Models’ evaluation (test set)</h3>
<p>Model evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. The next chunk performs this computations and gives the sorted accuracy metrics.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">predicted_kknn_ &lt;-<span class="st"> </span><span class="kw">predict</span>(kknn_, test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col))
predicted_svm_ &lt;-<span class="st"> </span><span class="kw">predict</span>(svm_, test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col))
predicted_rf_ &lt;-<span class="st"> </span><span class="kw">predict</span>(rf_, test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col))

<span class="co">#The best model</span>
tests_acc &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Model =</span> <span class="kw">summary</span>(results)<span class="op">$</span>models,
                        <span class="dt">Accuracy_on_test =</span> <span class="kw">c</span>(
                          <span class="kw">confusionMatrix</span>(test_df<span class="op">$</span>yieldClass, predicted_kknn_)<span class="op">$</span>overall[<span class="dv">1</span>],
                          <span class="kw">confusionMatrix</span>(test_df<span class="op">$</span>yieldClass, predicted_svm_)<span class="op">$</span>overall[<span class="dv">1</span>],
                          <span class="kw">confusionMatrix</span>(test_df<span class="op">$</span>yieldClass, predicted_rf_)<span class="op">$</span>overall[<span class="dv">1</span>]))
tests_acc[<span class="kw">order</span>(tests_acc[,<span class="st">&quot;Accuracy_on_test&quot;</span>], <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), ]</code></pre></div>
<pre><code>##        Model Accuracy_on_test
## 1 kknn_model        0.6994083
## 3   rf_model        0.6923077
## 2  svm_model        0.6437870</code></pre>
<p>The k-nearest neighbours, the random forest and the support vector machine models returned similar predictive accuracies (<em>although slightly higher for the former</em>).</p>
</div>
</div>
<div id="variable-importance-estimation" class="section level2">
<h2><span class="header-section-number">3.5</span> Variable importance estimation</h2>
<p>The <code>varImp()</code> method is then used to estimate the variable importance, which is printed (summarized) and plotted. <code>varImp()</code> ranks features by importance.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">importance &lt;-<span class="st"> </span><span class="kw">varImp</span>(rf_, <span class="dt">scale =</span> <span class="ot">FALSE</span>) <span class="co"># scale between 1 to 100</span>
<span class="kw">print</span>(importance)</code></pre></div>
<pre><code>## rf variable importance
## 
##        Overall
## clr_N    241.7
## clr_Fv   195.3
## clr_Mg   194.6
## clr_Ca   189.5
## clr_K    183.5
## clr_P    177.1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#tiff(&#39;images/var-imp-plot.tiff&#39;)</span>
<span class="kw">plot</span>(importance, <span class="dt">cex =</span> <span class="fl">1.2</span>, <span class="dt">cex.lab =</span> <span class="dv">2</span>, <span class="dt">cex.axis =</span> <span class="dv">2</span>, <span class="dt">ylab =</span> <span class="st">&quot;variable&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;black&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:var-imp-plot"></span>
<img src="2019_Bookdown_files/figure-html/var-imp-plot-1.png" alt="Importance of clr variables (effect) in the model." width="100%" />
<p class="caption">
Figure 3.2: Importance of clr variables (effect) in the model.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#dev.off()</span></code></pre></div>
</div>
<div id="make-predictions-on-the-test-set-with-kknn-model" class="section level2">
<h2><span class="header-section-number">3.6</span> Make predictions on the test set with <code>kknn</code> model</h2>
<p>We sort the predictive quality metrics by cultivar with <code>random forest</code> algorithm: model <code>rf_clr</code>, and tide the table.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_df<span class="op">$</span>ypred =<span class="st"> </span>predicted_kknn_ <span class="co"># adds predictions to test set</span>

cultivar_acc &lt;-<span class="st"> </span>test_df <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(Cultivar) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">do</span>(<span class="dt">Accuracy =</span> <span class="kw">as.numeric</span>(<span class="kw">confusionMatrix</span>(.<span class="op">$</span>yieldClass, .<span class="op">$</span>ypred)<span class="op">$</span>overall[<span class="dv">1</span>]),
       <span class="dt">numb_samples =</span> <span class="kw">as.numeric</span>(<span class="kw">nrow</span>(.)))

cultivar_acc<span class="op">$</span>Accuracy &lt;-<span class="st"> </span><span class="kw">round</span>(<span class="kw">unlist</span>(cultivar_acc<span class="op">$</span>Accuracy), <span class="dv">2</span>)
cultivar_acc<span class="op">$</span>numb_samples &lt;-<span class="st"> </span><span class="kw">unlist</span>(cultivar_acc<span class="op">$</span>numb_samples)

data =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">subset</span>(cultivar_acc, Accuracy<span class="op">&gt;</span><span class="dv">0</span>))
data[<span class="kw">order</span>(data[,<span class="st">&quot;Accuracy&quot;</span>], <span class="dt">decreasing=</span>T), ]</code></pre></div>
<pre><code>##              Cultivar Accuracy numb_samples
## 3               Ambra     1.00            1
## 9            Carolina     1.00            1
## 12 Dark Red Chieftain     1.00            6
## 20             Lamoka     1.00            1
## 25          Peribonka     1.00            2
## 40             W 1386     1.00            5
## 41             Waneta     1.00            3
## 24            Norland     0.94           18
## 19           Kennebec     0.83           42
## 29               Reba     0.83            6
## 4             Andover     0.80            5
## 17           Goldrush     0.80          143
## 35            Shepody     0.77           30
## 37            Snowden     0.77           39
## 11     Coastal Russet     0.76           41
## 2          AC Chaleur     0.75            8
## 10          Chieftain     0.75           71
## 34     Russet Norkota     0.75            4
## 22            Mystere     0.73           11
## 33     Russet Burbank     0.71            7
## 8         Bijou Rouge     0.67           12
## 18            Harmony     0.67            3
## 21            Lanorma     0.67            3
## 28           Prospect     0.67            6
## 31          Red Maria     0.67            6
## 36              Sifra     0.67            3
## 38           Superior     0.67           86
## 15            FL 1533     0.66           47
## 6               Argos     0.60            5
## 14            FL 1207     0.60           81
## 16    Frontier Russet     0.60            5
## 39            Vivaldi     0.60            5
## 42         Yukon Gold     0.59           17
## 5             Aquilon     0.58           12
## 13             Estima     0.57           14
## 7            Atlantic     0.53           47
## 1          AC Belmont     0.50           16
## 27         Pommerelle     0.45           11
## 30          Red Cloud     0.40            5
## 23           Nordonna     0.25            4
## 32               Roko     0.25            4
## 26               Pike     0.17            6</code></pre>
<p>The predictive accuracy is very high for some cultivars, but this result must be taken with care due to small size of samples often available. We plot accuracies for cultivars using ggplot2 functions.</p>
<div class="figure" style="text-align: center"><span id="fig:accuracy-cultivar"></span>
<img src="2019_Bookdown_files/figure-html/accuracy-cultivar-1.png" alt="Predictive accuracy for cultivars." width="100%" />
<p class="caption">
Figure 3.3: Predictive accuracy for cultivars.
</p>
</div>
</div>
<div id="comparison-with-non-informative-classification" class="section level2">
<h2><span class="header-section-number">3.7</span> Comparison with non-informative classification</h2>
<p>The non-informative classification consists of an equal distribution of 50% successful and 50% unsuccessful classification cases <a href="https://science.sciencemag.org/content/240/4857/1285">(Swets J. A., 1988)</a>. We run the Chi-square homogenity test to compare predictive accuracy of the <strong>..kknn..</strong> model to this non-informative classification model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cm &lt;-<span class="st"> </span><span class="kw">confusionMatrix</span>(predicted_kknn_, test_df<span class="op">$</span>yieldClass) 
cm<span class="op">$</span>table <span class="co"># confusion matrix</span></code></pre></div>
<pre><code>##           Reference
## Prediction  HY  LY
##         HY 194 115
##         LY 139 397</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># rf_clr_and_ionomicgroup model&#39;s classification</span>
good_class &lt;-<span class="st"> </span>cm<span class="op">$</span>table[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">+</span>cm<span class="op">$</span>table[<span class="dv">2</span>,<span class="dv">2</span>] <span class="co"># HY or LY and correctly predicted</span>
misclass &lt;-<span class="st"> </span>cm<span class="op">$</span>table[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">+</span>cm<span class="op">$</span>table[<span class="dv">2</span>,<span class="dv">1</span>]   <span class="co"># wrong classification</span>
ml_class &lt;-<span class="st"> </span><span class="kw">c</span>(good_class, misclass)

<span class="co"># Non-informative model (nim)</span>
total &lt;-<span class="st"> </span><span class="kw">sum</span>(cm<span class="op">$</span>table) <span class="co"># total number of samples</span>
good_nim &lt;-<span class="st"> </span><span class="fl">0.50</span> <span class="op">*</span><span class="st"> </span>total
misclass_nim &lt;-<span class="st"> </span><span class="fl">0.50</span> <span class="op">*</span><span class="st"> </span>total
non_inf_model &lt;-<span class="st"> </span><span class="kw">c</span>(good_nim, misclass_nim)

<span class="co"># Matrix for chisquare test</span>
m &lt;-<span class="st"> </span><span class="kw">rbind</span>(ml_class, non_inf_model)
m</code></pre></div>
<pre><code>##                [,1]  [,2]
## ml_class      591.0 254.0
## non_inf_model 422.5 422.5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># chisq.test</span>
khi2_test &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(m)
khi2_test</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  m
## X-squared = 69.155, df = 1, p-value &lt; 2.2e-16</code></pre>
<p>The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test, the too small p-value denotes important difference between the models.</p>
</div>
<div id="nutritionally-balanced-compositions" class="section level2">
<h2><span class="header-section-number">3.8</span> Nutritionally balanced compositions</h2>
<p>We consider as True Negatives (TN) specimens for this study, observations of the training data set having a high yield (HY) and correctly predicted with the <code>__k nearest neighbors__</code> model.</p>
<p>Let’s save prediction for training set</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tr_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(kknn_)<span class="co">#, train_df %&gt;% select(ml_col))</span>
train_df &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(dfml[split_index, ], <span class="dt">pred_yield =</span> tr_pred)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TNs =<span class="st"> </span>train_df[train_df<span class="op">$</span>yieldClass <span class="op">==</span><span class="st"> &#39;HY&#39;</span> <span class="op">&amp;</span><span class="st"> </span>tr_pred <span class="op">==</span><span class="st"> &#39;HY&#39;</span>, ]</code></pre></div>
<p>Then, we compute <strong>clr centroids (means)</strong> for cultivars using True Negatives original (not scaled) clr values. These centroids (<strong>S3 Table of the article</strong>) could be used as provisional <code>clr norms</code> for cultivars. The standard deviations could also be computed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TNmeanNorms &lt;-<span class="st"> </span>TNs <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(Cultivar) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(clr_no) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise_all</span>(<span class="kw">list</span>(mean))</code></pre></div>
<pre><code>## Adding missing grouping variables: `Cultivar`</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TNmeanNorms</code></pre></div>
<pre><code>## # A tibble: 43 x 7
##    Cultivar    clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv
##    &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 AC Belmont  0.708 -2.11 0.610  -1.63 -1.06    3.49
##  2 AC Chaleur  0.710 -1.97 0.319  -1.52 -1.03    3.49
##  3 Ambra       0.710 -2.01 0.296  -1.39 -1.05    3.44
##  4 Andover     0.878 -2.18 0.723  -2.21 -0.819   3.61
##  5 Aquilon     0.813 -1.93 0.388  -1.84 -0.898   3.46
##  6 Argos       0.751 -1.79 0.550  -1.69 -1.23    3.42
##  7 Atlantic    0.783 -1.86 0.105  -1.50 -1.08    3.56
##  8 Bijou Rouge 0.941 -1.96 0.526  -2.07 -1.07    3.64
##  9 Carolina    0.595 -2.18 0.114  -1.32 -0.833   3.63
## 10 Chieftain   0.767 -1.91 0.407  -1.83 -1.04    3.60
## # ... with 33 more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">write_csv</span>(TNmeanNorms, <span class="st">&quot;output/provisional_norms.csv&quot;</span>)</code></pre></div>
<p>We also save prediction for testing set, not scaled clr test data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">te_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(kknn_, test_df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(ml_col))
test_df &lt;-<span class="st"> </span><span class="kw">bind_cols</span>(dfml[<span class="op">-</span>split_index, ], <span class="dt">pred_yield =</span> te_pred)</code></pre></div>
<p>And finally backup for next chapter.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">write_csv</span>(train_df, <span class="st">&quot;output/train_df.csv&quot;</span>)
<span class="kw">write_csv</span>(test_df, <span class="st">&quot;output/test_df.csv&quot;</span>)</code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="Chapter-Clustering.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="Chapter-Perturbation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["2019_Bookdown.pdf", "2019_Bookdown.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
