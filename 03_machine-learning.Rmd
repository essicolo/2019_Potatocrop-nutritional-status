# Predicting tuber yield category {#Chapter-Modeling}

## Objective

***
The objective of this chapter is to develop, evaluate and compare the performance of some machine learning algorithms (k-nearest neighbors, random forest and support vector machine - [package caret](https://topepo.github.io/caret/index.html)) in predicting yield categories using clr coordinates. I use the previous chapter (chapter \@ref(Chapter-Clustering)) handeled data file `dfml.csv` which contains clr coordinates, maturity classes and the yield two categorical variable created using the 65^th percentile for each cultivar. I use `accuracy` as models quality metric. I run the Chi-square homogenity test to compare prediction with a non-informative classification consisting of an equal distribution of 50% successful and 50% unsuccessful cases, using the best model (highest accuracy's model). Then, I filter only true negative specimens i.e., correctely predicted high yielders of training data set, in a new data frame for leaf health index analysis purpose (Chapter \@ref(Chapter-Perturbation-vector)).

***

## Useful libraries

The `tidyverse` package is always needed for data manipulation and visualization, and then `extrafont` to make changes in graphs as demanded for the article. The particularly useful packages are `caret` and [kknn](https://www.rdocumentation.org/packages/kknn/versions/1.3.1) needed for machine leraning functions.

```{r, warning=FALSE, message=FALSE}
library("tidyverse")
library('extrafont')
library('caret')
library('kknn')
```

## Machine learning data set

I load the `dfml.csv` data set and named it `df`. The clr coordinates are scaled to zero mean and unity variance.

```{r}
df = read_csv('output/dfml.csv')
df$Maturity5 = factor(df$Maturity5)
df$yieldClass = factor(df$yieldClass)

clr_no <- c("clr_N", "clr_P", "clr_K", "clr_Mg", "clr_Ca", "clr_Fv")
clrNo <- c("clrN", "clrP", "clrK", "clrMg", "clrCa", "clrFv")
colnames(df)[which(names(df) %in% clr_no)] <- clrNo

df.sc <- df # copy
df.sc[, clrNo] <- apply(df.sc[, clrNo], 2, scale) # scale clr coordinates
```

I check the data frame to see cultivars abundance. Cultivar `Goldrush` overcomes the others.

```{r}
pc <- round(with(df.sc, prop.table(table(Cultivar)) * 100), 2)
dist <- with(df.sc, cbind(freq = table(Cultivar), percentage = pc))
dist <- data.frame(cbind(dist, rownames(dist)))
colnames(dist)[3] <- "Cultivar"
dist$freq <- as.numeric(as.character(dist$freq))
```

```{r, ml-cultivar-abundance, fig.height = 10, fig.width = 5, out.width = "100%", fig.align = "center", warning = FALSE, message = FALSE, fig.cap = "Cultivars abundance in the machine learning data frame.", echo = FALSE}
dist %>%
  ggplot(aes(x = reorder(Cultivar, freq), y = freq)) +
  geom_col() +
  geom_text(aes(label = freq), hjust = -0.1) +
  ylim(c(0, 650)) +
  labs(x = 'Cultivar', y = 'Number of samples') +
  coord_flip()
```

## Machine learning

### Train and Test splits

I randomly split the data into a training set (75 % of the data) used to fit the models, and a testing set (remaining 25 %) I use for models' evaluation. The next chunk splits data and groups predictors using three schemes.

```{r}
set.seed(853739)
split_index <- createDataPartition(df.sc$yieldClass, group = "Cultivar",
                                   p = 0.75, list = FALSE, times = 1)
train_df <- df.sc[split_index, ]
test_df <- df.sc[-split_index, ]
ml_col <- c(clrNo, "yieldClass")
```

With the `kknn` package, we must specify three parameters: `kmax` which is the number of neighbors to consider, `distance` a distance parameter to specify (1 for the Mahattan distance and 2 for the Euclidean distance), and a `kernel` which is a function to measure the distance. A best method currently used to choose the right parameters consists in creating a parameter grid.

```{r}
grid <-  expand.grid(kmax = c(7,9,12,15), distance = 1:2,
                     kernel = c("rectangular", "gaussian", "optimal"))
grid
```

I use the metric of “Accuracy” to evaluate models. This is a ratio of the number of correctly predicted instances divided by the total number of instances in the dataset (e.g. 95% accurate). 

The accuracy of the models will be estimated using a `10-fold cross-validation (cv)` scheme. This will split the data set into 10 subsets of equal size. The models are built 10 times, each time leaving out one of the subsets from training and use it as the test set.

```{r}
control <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"
```

### Building the Models

I reset the random number seed before reach run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable as explained by [Jason Brownlee](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/).

```{r}
# a) Non-linear algorithm
## kNN
set.seed(7)
kknn_clr <- train(yieldClass ~., data = train_df %>% select(ml_col), method = "kknn", 
                  metric = metric, trControl = control, tuneGrid = grid)
```

```{r}
# b) Advanced algorithms
## SVM
set.seed(7)
svm_clr <- train(yieldClass ~., data = train_df %>% select(ml_col), method = "svmRadial", 
                 metric = metric, trControl = control)
```

```{r}
## Random Forest
set.seed(7)
rf_clr <- train(yieldClass ~., data = train_df %>% select(ml_col), method = "rf", 
                metric = metric, trControl = control)
```

### Goodness of fit on training set

I assess the accuracy metric also when building the models (training set) but the target metric is for the evaluation set. This chart sorts models.

```{r ml-traindotplot, out.width="100%", fig.align="center", warning=FALSE, message=FALSE, fig.cap="Comparison of models accuracies at training."}
# Summary results
results <- resamples(list(kknn_model = kknn_clr, 
                          svm_model = svm_clr, 
                          rf_model = rf_clr)
                     )

#summary(results) with dotplot()
dotplot(results)
```

This chunk also sorts models in a descending order using accuracies only, in a table.

```{r}
models_acc <- data.frame(Model = summary(results)$models,
                       Accuracy = c(confusionMatrix(train_df$yieldClass, predict(kknn_clr))$overall[1],
                                    confusionMatrix(train_df$yieldClass, predict(svm_clr))$overall[1],
                                    confusionMatrix(train_df$yieldClass, predict(rf_clr))$overall[1])
                       )

models_acc[order(models_acc[,"Accuracy"], decreasing = TRUE), ]
```

### Models' evaluation (on testing set)

Model evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. The next chunk performs this computations and gives the sorted accuracy metrics.

```{r}
predicted_kknn_clr <- predict(kknn_clr, test_df %>% select(ml_col))
predicted_svm_clr <- predict(svm_clr, test_df %>% select(ml_col))
predicted_rf_clr <- predict(rf_clr, test_df %>% select(ml_col))

#The best model
tests_acc <- data.frame(Model = summary(results)$models,
                        Accuracy_on_test = c(
                          confusionMatrix(test_df$yieldClass, predicted_kknn_clr)$overall[1],
                          confusionMatrix(test_df$yieldClass, predicted_svm_clr)$overall[1],
                          confusionMatrix(test_df$yieldClass, predicted_rf_clr)$overall[1])
                        )
tests_acc[order(tests_acc[,"Accuracy_on_test"], decreasing = TRUE), ]
```

The k-nearest neighbours, the random forest and the support vector machine models returned similar predictive accuracies although slightly higher for the random forest. 

## Yield class prediction with rf algorithm

I sort the predictive quality metrics by cultivar with `random forest` algorithm: model `rf_clr`, and tide the table.

```{r}
test_df$ypred = predicted_rf_clr # adds predictions to test set

cultivar_acc <- test_df %>%
    group_by(Cultivar) %>%
    do(Accuracy = as.numeric(confusionMatrix(.$yieldClass, .$ypred)$overall[1]),
       numb_samples = as.numeric(nrow(.)))

cultivar_acc$Accuracy <- round(unlist(cultivar_acc$Accuracy), 2)
cultivar_acc$numb_samples <- unlist(cultivar_acc$numb_samples)

data = data.frame(subset(cultivar_acc, Accuracy>0))
data[order(data[,"Accuracy"], decreasing=T), ]
```

The predictive accuracy is very high for some cultivars, but this result must be taken with care due to small size of samples often available. The next chunk plots accuracies for cultivars using ggplot2 functions.

```{r accuracy_cultivar, fig.height = 8, fig.width = 5, out.width="100%", fig.align="center", warning=FALSE, message=FALSE, fig.cap="Predictive accuracy for cultivars."}
ggplot(data, aes(x = reorder(Cultivar, Accuracy), y = Accuracy, fill = Accuracy)) +
  geom_col() +
  scale_fill_gradient(low = "lightgrey", high = "black") + 
  geom_text(aes(label = Accuracy), hjust = -0.1) +
  ylim(c(0, 1.1)) +
  labs(x = 'Cultivar', y = 'Accuracy') +
  coord_flip() +
  theme(text=element_text(family = "Arial", face = "bold", size = 12))
ggsave("images/cultivar-accuracy.tiff", width=5, height=8)
```

## Comparison with non-informative classification

The non-informative classification consists of an equal distribution of 50% successful and 50% unsuccessful classification cases [(Swets J. A., 1988)](https://science.sciencemag.org/content/240/4857/1285). I run the Chi-square homogenity test to compare predictive accuracy of the random forest model with this non-informative classification model. 

```{r}
cm <- confusionMatrix(predicted_rf_clr, test_df$yieldClass) 
cm$table # confusion matrix
```

```{r}
# rf_clr_and_ionomicgroup model's classification
good_class <- cm$table[1,1]+cm$table[2,2] # HY or LY and correctly predicted
misclass <- cm$table[1,2]+cm$table[2,1]   # wrong classification
ml_class <- c(good_class, misclass)

# Non-informative model (nim)
total <- sum(cm$table) # total number of samples
good_nim <- 0.50 * total
misclass_nim <- 0.50 * total
non_inf_model <- c(good_nim, misclass_nim)

# Matrix for chisquare test
m <- rbind(ml_class, non_inf_model)
m

# chisq.test
khi2_test <- chisq.test(m)
khi2_test
```

The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test, the too small p-value denotes important difference between the models.

## Train data set backup for TN specimens selection

The train data set with predicted yield classes `train_df` backup will be used to test perturbation vector theory in the next chapter.

```{r}
pred_yield <- predict(rf_clr, train_df %>% select(ml_col))
train_df = data.frame(cbind(df[split_index, ], pred_yield))
write_csv(train_df, "output/train_df.csv")
nrow(train_df)
```

I consider as True Negatives (TN) specimens for this study, observations of the training data set having a high yield (HY) and correctly predicted with the `random forest` model. Then, I compute clr centroids for cultivars using True Negatives original (not scaled) clr values. These centroids could be used as `clr norms` for cultivars.

```{r}
TNs = train_df[train_df$yieldClass == 'HY' & pred_yield == 'HY', ]
TNmedianNorms <- TNs %>%
  group_by(Cultivar) %>%
  select(clrNo) %>%
  summarise_all(list(median))
TNmedianNorms
```

